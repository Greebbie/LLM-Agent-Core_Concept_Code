{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "47a3504d",
                              "metadata": {},
                              "source": [
                                        "# Ch11 | 推理优化：KV Cache 工程实战\n",
                                        "\n",
                                        "> 阅读顺序：建议在完成 Ch10 DPO 后进入本章（推理优化阶段）。\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "**目标：** 理解并实现 KV Cache 加速推理\n",
                                        "\n",
                                        "**核心问题：** 为什么模型生成越长越慢？如何优化？\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## 本章内容\n",
                                        "\n",
                                        "1. **问题分析**：O(n^2) 的计算复杂度\n",
                                        "2. **KV Cache 原理**：缓存 K 和 V 避免重复计算\n",
                                        "3. **代码实现**：修改 Forward 函数\n",
                                        "4. **性能对比**：加速效果演示"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "prereq_ch8_001",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 前置知识：为什么生成越长越慢？\n",
                                        "\n",
                                        "### 自回归生成的过程\n",
                                        "\n",
                                        "LLM 生成文本是**一个词一个词**生成的：\n",
                                        "\n",
                                        "```\n",
                                        "输入: \"今天天气\"\n",
                                        "第1步: \"今天天气\" → 模型 → \"很\"\n",
                                        "第2步: \"今天天气很\" → 模型 → \"好\"\n",
                                        "第3步: \"今天天气很好\" → 模型 → \"！\"\n",
                                        "...\n",
                                        "```\n",
                                        "\n",
                                        "### 朴素实现的问题\n",
                                        "\n",
                                        "每生成一个新词，都要重新计算**整个序列**的 Attention：\n",
                                        "\n",
                                        "```\n",
                                        "生成第1个词: 计算 1 个词的 Attention\n",
                                        "生成第2个词: 计算 2 个词的 Attention (第1个词重复计算)\n",
                                        "生成第3个词: 计算 3 个词的 Attention (第1、2个词重复计算)\n",
                                        "...\n",
                                        "生成第n个词: 计算 n 个词的 Attention\n",
                                        "\n",
                                        "总计算量: 1 + 2 + 3 + ... + n = O(n^2)\n",
                                        "```\n",
                                        "\n",
                                        "**生成 100 个词需要 5050 次计算，效率极低。**\n",
                                        "\n",
                                        "### 解决方案：KV Cache\n",
                                        "\n",
                                        "观察：在 Attention 中，**之前词的 K 和 V 不会改变**。\n",
                                        "\n",
                                        "```\n",
                                        "Attention(Q, K, V):\n",
                                        "  - Q: 当前词的查询\n",
                                        "  - K: 所有词的键（之前的不变）\n",
                                        "  - V: 所有词的值（之前的不变）\n",
                                        "```\n",
                                        "\n",
                                        "**KV Cache**：缓存之前计算的 K 和 V，每次只计算新词的 Q, K, V。\n",
                                        "\n",
                                        "```\n",
                                        "生成第n个词:\n",
                                        "  - 从缓存读取前 n-1 个词的 K, V\n",
                                        "  - 只计算新词的 Q, K, V\n",
                                        "  - 计算量: O(1)\n",
                                        "\n",
                                        "总计算量: O(n)，不是 O(n^2)\n",
                                        "```\n",
                                        "\n",
                                        "### 本章目标\n",
                                        "\n",
                                        "- 深入理解 KV Cache 的原理\n",
                                        "- 实现带 Cache 的 Attention\n",
                                        "- 对比有无 Cache 的性能差异"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "7065f7e8",
                              "metadata": {},
                              "source": [
                                        "## 0. 环境准备"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "7519d915",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "import time\n",
                                        "plt.rcParams[\"font.sans-serif\"] = [\"Microsoft YaHei\", \"SimHei\", \"Noto Sans CJK SC\", \"Arial Unicode MS\"]\n",
                                        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
                                        "torch.manual_seed(42)\n",
                                        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                                        "print(f\"Using device: {device}\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "e891b109",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 1. 问题：为什么生成越长越慢？\n",
                                        "\n",
                                        "### 朴素实现的问题\n",
                                        "\n",
                                        "每生成一个新 token，都要：\n",
                                        "1. 重新计算整个序列的 Q, K, V\n",
                                        "2. 做完整的 Attention 运算\n",
                                        "\n",
                                        "```\n",
                                        "生成第 1 个 token: 计算 1 次 Attention\n",
                                        "生成第 2 个 token: 计算 2 次 Attention (重复！)\n",
                                        "生成第 3 个 token: 计算 3 次 Attention (重复！)\n",
                                        "...\n",
                                        "生成第 n 个 token: 计算 n 次 Attention\n",
                                        "\n",
                                        "总计: 1 + 2 + 3 + ... + n = O(n^2) 次计算！\n",
                                        "```"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "76a16666",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 可视化 O(n^2) 的问题\n",
                                        "n = np.arange(1, 101)\n",
                                        "naive_ops = n * (n + 1) / 2  # 1+2+...+n\n",
                                        "cached_ops = n  # 每步只算1次\n",
                                        "\n",
                                        "plt.figure(figsize=(12, 5))\n",
                                        "\n",
                                        "plt.subplot(1, 2, 1)\n",
                                        "plt.plot(n, naive_ops, 'r-', linewidth=2, label='Naive O(n^2)')\n",
                                        "plt.plot(n, cached_ops, 'g-', linewidth=2, label='With KV Cache O(n)')\n",
                                        "plt.xlabel('Sequence Length')\n",
                                        "plt.ylabel('Total Operations')\n",
                                        "plt.title('Computational Cost Comparison')\n",
                                        "plt.legend()\n",
                                        "plt.grid(True, alpha=0.3)\n",
                                        "\n",
                                        "plt.subplot(1, 2, 2)\n",
                                        "speedup = naive_ops / cached_ops\n",
                                        "plt.plot(n, speedup, 'b-', linewidth=2)\n",
                                        "plt.xlabel('Sequence Length')\n",
                                        "plt.ylabel('Speedup Factor')\n",
                                        "plt.title('KV Cache Speedup')\n",
                                        "plt.grid(True, alpha=0.3)\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(f\"当序列长度为 100 时:\")\n",
                                        "print(f\"  朴素方法: {naive_ops[-1]:.0f} 次操作\")\n",
                                        "print(f\"  KV Cache: {cached_ops[-1]:.0f} 次操作\")\n",
                                        "print(f\"  加速: {speedup[-1]:.1f}x\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "87e30697",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 2. KV Cache 原理\n",
                                        "\n",
                                        "### 关键洞察\n",
                                        "\n",
                                        "在自回归生成中：\n",
                                        "- **我们只需要当前 token 的输出**，因此只需要当前 token 的 Query（Q）\n",
                                        "- **历史 token 的 Key/Value（K/V）不会改变**，可以直接复用\n",
                                        "\n",
                                        "### 一步一步看“只算最后一个 Query”\n",
                                        "\n",
                                        "| 步骤 | 输入序列 | 需要计算 | 复用缓存 |\n",
                                        "|---|---|---|---|\n",
                                        "| 1 | [A] | Q1, K1, V1 | 无 |\n",
                                        "| 2 | [A, B] | Q2, K2, V2 | K1, V1 |\n",
                                        "| 3 | [A, B, C] | Q3, K3, V3 | K1, V1, K2, V2 |\n",
                                        "\n",
                                        "**核心点：** 生成阶段只关心“下一个 token”，所以只需要**最后一个 token 的 Query**。\n",
                                        "\n",
                                        "### 训练 vs 推理（为何训练时不用 KV Cache）\n",
                                        "\n",
                                        "- **训练**：一次性喂入全序列，需要所有 token 的 Q/K/V\n",
                                        "- **推理**：逐 token 生成，只需要当前 token 的 Q，并复用历史 K/V\n",
                                        "\n",
                                        "接下来用更直观的矩阵和图示来对比两种计算方式。"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "c668dc38",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 直观对比：Attention 矩阵需要算哪些部分\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "import numpy as np\n",
                                        "\n",
                                        "T = 6  # 序列长度\n",
                                        "\n",
                                        "# 朴素方法：每步重新计算完整 T x T 注意力矩阵\n",
                                        "naive = np.tril(np.ones((T, T)))\n",
                                        "\n",
                                        "# KV Cache：只需要最后一个 Query，对应 1 x T 的注意力\n",
                                        "cache = np.ones((1, T))\n",
                                        "\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
                                        "\n",
                                        "axes[0].imshow(naive, cmap='Blues', vmin=0, vmax=1)\n",
                                        "axes[0].set_title('朴素方法：重算完整 T x T')\n",
                                        "axes[0].set_xlabel('Key 位置')\n",
                                        "axes[0].set_ylabel('Query 位置')\n",
                                        "axes[0].set_xticks(range(T))\n",
                                        "axes[0].set_yticks(range(T))\n",
                                        "\n",
                                        "axes[1].imshow(cache, cmap='Greens', vmin=0, vmax=1)\n",
                                        "axes[1].set_title('KV Cache：只算最后 1 x T')\n",
                                        "axes[1].set_xlabel('Key 位置')\n",
                                        "axes[1].set_yticks([0])\n",
                                        "axes[1].set_yticklabels(['当前 Query'])\n",
                                        "axes[1].set_xticks(range(T))\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"解释：生成时只需要最后一个 token 的输出，所以只算最后一行的注意力。\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "qfnx3fcu44",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# KV Cache 工作原理可视化（更直观版）\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "from matplotlib.patches import FancyBboxPatch\n",
                                        "import matplotlib as mpl\n",
                                        "\n",
                                        "# 设置中文字体，避免乱码\n",
                                        "mpl.rcParams[\"font.sans-serif\"] = [\"Microsoft YaHei\", \"SimHei\", \"Arial Unicode MS\"]\n",
                                        "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
                                        "\n",
                                        "\n",
                                        "def _box(ax, x, y, w, h, text, fc, ec, ls=\"-\"):\n",
                                        "    patch = FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.08\",\n",
                                        "                           facecolor=fc, edgecolor=ec, linewidth=1.5, linestyle=ls)\n",
                                        "    ax.add_patch(patch)\n",
                                        "    ax.text(x + w / 2, y + h / 2, text, ha=\"center\", va=\"center\", fontsize=9)\n",
                                        "\n",
                                        "\n",
                                        "def draw_step(ax, tokens, step, mode):\n",
                                        "    ax.set_xlim(0, 10)\n",
                                        "    ax.set_ylim(0, 8)\n",
                                        "    ax.axis(\"off\")\n",
                                        "\n",
                                        "    if mode == \"naive\":\n",
                                        "        title = f\"朴素方法 Step {step + 1}: 重算所有 Q/K/V\"\n",
                                        "        color = \"#E74C3C\"\n",
                                        "    else:\n",
                                        "        title = f\"KV Cache Step {step + 1}: 只算新 token 的 Q/K/V\"\n",
                                        "        color = \"#27AE60\"\n",
                                        "\n",
                                        "    ax.set_title(title, fontsize=11, fontweight=\"bold\", color=color)\n",
                                        "\n",
                                        "    for i, tok in enumerate(tokens):\n",
                                        "        x = 0.8 + i * 2.2\n",
                                        "        # token\n",
                                        "        tok_color = \"#AED6F1\" if mode == \"naive\" else (\"#D5F5E3\" if i == step else \"#E8E8E8\")\n",
                                        "        _box(ax, x, 6.2, 1.4, 0.8, tok, tok_color, \"#2980B9\" if mode == \"naive\" else \"#27AE60\")\n",
                                        "\n",
                                        "        if mode == \"naive\":\n",
                                        "            # 所有 token 都重算 Q/K/V\n",
                                        "            repeat = i < step\n",
                                        "            fc = \"#FFB3B3\" if repeat else \"#B3FFB3\"\n",
                                        "            _box(ax, x, 4.6, 0.6, 0.7, \"Q\", fc, \"#333\")\n",
                                        "            _box(ax, x, 3.5, 0.6, 0.7, \"K\", fc, \"#333\")\n",
                                        "            _box(ax, x, 2.4, 0.6, 0.7, \"V\", fc, \"#333\")\n",
                                        "        else:\n",
                                        "            # 只算当前 token 的 Q/K/V；旧 token 的 K/V 从缓存读取\n",
                                        "            if i == step:\n",
                                        "                _box(ax, x, 4.6, 0.6, 0.7, \"Q\", \"#B3FFB3\", \"#333\")\n",
                                        "                _box(ax, x, 3.5, 0.6, 0.7, \"K\", \"#B3FFB3\", \"#333\")\n",
                                        "                _box(ax, x, 2.4, 0.6, 0.7, \"V\", \"#B3FFB3\", \"#333\")\n",
                                        "            else:\n",
                                        "                _box(ax, x, 3.5, 0.6, 0.7, \"K\", \"#D4E6F1\", \"#2980B9\", ls=\"--\")\n",
                                        "                _box(ax, x, 2.4, 0.6, 0.7, \"V\", \"#D4E6F1\", \"#2980B9\", ls=\"--\")\n",
                                        "\n",
                                        "    # 计算量说明\n",
                                        "    if mode == \"naive\":\n",
                                        "        calc = (step + 1) * 3\n",
                                        "        note = f\"计算量: {calc} 次矩阵计算（所有 token 重新算）\"\n",
                                        "        ax.text(5, 0.6, note, ha=\"center\", fontsize=9,\n",
                                        "                bbox=dict(boxstyle=\"round\", facecolor=\"#FDEBD0\", edgecolor=\"#E67E22\"))\n",
                                        "    else:\n",
                                        "        note = \"计算量: 3 次矩阵计算（只算新 token）\"\n",
                                        "        ax.text(5, 0.6, note, ha=\"center\", fontsize=9,\n",
                                        "                bbox=dict(boxstyle=\"round\", facecolor=\"#D5F5E3\", edgecolor=\"#27AE60\"))\n",
                                        "\n",
                                        "\n",
                                        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                                        "\n",
                                        "# 上排：朴素方法\n",
                                        "for step in range(3):\n",
                                        "    tokens = [\"我\", \"爱\", \"机\"][: step + 1]\n",
                                        "    draw_step(axes[0, step], tokens, step, mode=\"naive\")\n",
                                        "\n",
                                        "# 下排：KV Cache 方法\n",
                                        "for step in range(3):\n",
                                        "    tokens = [\"我\", \"爱\", \"机\"][: step + 1]\n",
                                        "    draw_step(axes[1, step], tokens, step, mode=\"cache\")\n",
                                        "\n",
                                        "# 图例\n",
                                        "fig.text(0.12, 0.02, \"图例: 绿色=新计算  红色=重复计算  蓝色虚线=缓存读取\", fontsize=10, ha=\"left\")\n",
                                        "\n",
                                        "plt.suptitle(\"KV Cache 如何避免重复计算\", fontsize=16, fontweight=\"bold\", y=1.02)\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"\\n核心差异:\")\n",
                                        "print(\"  朴素方法: 每一步都重算所有 token 的 Q/K/V\")\n",
                                        "print(\"  KV Cache: 只算新 token 的 Q/K/V，旧 token 的 K/V 直接复用\")\n",
                                        "print(\"  结果: 计算量从 O(n^2) 降到 O(n)\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "fe87aa25",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class CausalSelfAttentionWithCache(nn.Module):\n",
                                        "    \"\"\"\n",
                                        "    带 KV Cache 的自注意力\n",
                                        "    \"\"\"\n",
                                        "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
                                        "        super().__init__()\n",
                                        "        self.n_head = n_head\n",
                                        "        self.head_dim = n_embd // n_head\n",
                                        "        \n",
                                        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
                                        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
                                        "        self.dropout = nn.Dropout(dropout)\n",
                                        "        \n",
                                        "        # Causal mask\n",
                                        "        self.register_buffer(\"mask\", torch.tril(\n",
                                        "            torch.ones(block_size, block_size)\n",
                                        "        ).view(1, 1, block_size, block_size))\n",
                                        "    \n",
                                        "    def forward(self, x, past_kv=None, use_cache=False):\n",
                                        "        \"\"\"\n",
                                        "        x: [B, T, C] 输入（如果 use_cache=True 且 past_kv 存在，T=1）\n",
                                        "        past_kv: tuple of (past_k, past_v)，之前的 K 和 V 缓存\n",
                                        "        use_cache: 是否使用和更新缓存\n",
                                        "        \n",
                                        "        返回:\n",
                                        "            output: [B, T, C]\n",
                                        "            present_kv: 更新后的缓存（如果 use_cache=True）\n",
                                        "        \"\"\"\n",
                                        "        B, T, C = x.size()\n",
                                        "        \n",
                                        "        # 计算当前的 Q, K, V\n",
                                        "        q, k, v = self.c_attn(x).split(C, dim=2)\n",
                                        "        \n",
                                        "        # 如果有缓存，拼接\n",
                                        "        if past_kv is not None:\n",
                                        "            past_k, past_v = past_kv\n",
                                        "            k = torch.cat([past_k, k], dim=1)  # [B, past_T + T, C]\n",
                                        "            v = torch.cat([past_v, v], dim=1)\n",
                                        "        \n",
                                        "        # 当前的 K, V 就是 present（用于下一次的缓存）\n",
                                        "        present_kv = (k, v) if use_cache else None\n",
                                        "        \n",
                                        "        # 获取完整序列长度\n",
                                        "        full_T = k.size(1)\n",
                                        "        \n",
                                        "        # Reshape for multi-head attention\n",
                                        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
                                        "        k = k.view(B, full_T, self.n_head, self.head_dim).transpose(1, 2)\n",
                                        "        v = v.view(B, full_T, self.n_head, self.head_dim).transpose(1, 2)\n",
                                        "        \n",
                                        "        # Attention\n",
                                        "        att = (q @ k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
                                        "        \n",
                                        "        # 只 mask 新 query 能看到的范围\n",
                                        "        # query 位置 [full_T - T : full_T]，key 位置 [0 : full_T]\n",
                                        "        if T == 1:\n",
                                        "            # 生成时只有一个 query，可以看到所有之前的 key\n",
                                        "            pass  # 不需要 mask\n",
                                        "        else:\n",
                                        "            att = att.masked_fill(self.mask[:, :, :T, :full_T] == 0, float('-inf'))\n",
                                        "        \n",
                                        "        att = F.softmax(att, dim=-1)\n",
                                        "        att = self.dropout(att)\n",
                                        "        \n",
                                        "        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
                                        "        output = self.c_proj(y)\n",
                                        "        \n",
                                        "        return output, present_kv\n",
                                        "\n",
                                        "# 测试\n",
                                        "attn = CausalSelfAttentionWithCache(n_embd=64, n_head=4, block_size=128)\n",
                                        "\n",
                                        "# 模拟增量生成\n",
                                        "x_full = torch.randn(1, 10, 64)  # 完整序列\n",
                                        "x_new = torch.randn(1, 1, 64)   # 新 token\n",
                                        "\n",
                                        "# 无缓存\n",
                                        "out1, _ = attn(x_full, use_cache=False)\n",
                                        "print(f\"无缓存输出形状: {out1.shape}\")\n",
                                        "\n",
                                        "# 有缓存\n",
                                        "out2, cache = attn(x_full, use_cache=True)\n",
                                        "out3, new_cache = attn(x_new, past_kv=cache, use_cache=True)\n",
                                        "print(f\"有缓存时新 token 输出: {out3.shape}\")\n",
                                        "print(f\"缓存 K 形状: {new_cache[0].shape}\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "2de8af09",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 3. 完整的 GPT with KV Cache"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "3b5ddf4a",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class GPTWithCache(nn.Module):\n",
                                        "    \"\"\"带 KV Cache 的 GPT\"\"\"\n",
                                        "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout=0.1):\n",
                                        "        super().__init__()\n",
                                        "        self.block_size = block_size\n",
                                        "        self.n_layer = n_layer\n",
                                        "        \n",
                                        "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
                                        "        self.wpe = nn.Embedding(block_size, n_embd)\n",
                                        "        self.drop = nn.Dropout(dropout)\n",
                                        "        \n",
                                        "        # Transformer blocks with cache support\n",
                                        "        self.blocks = nn.ModuleList([\n",
                                        "            nn.ModuleDict({\n",
                                        "                'ln_1': nn.LayerNorm(n_embd),\n",
                                        "                'attn': CausalSelfAttentionWithCache(n_embd, n_head, block_size, dropout),\n",
                                        "                'ln_2': nn.LayerNorm(n_embd),\n",
                                        "                'mlp': nn.Sequential(\n",
                                        "                    nn.Linear(n_embd, 4 * n_embd),\n",
                                        "                    nn.GELU(),\n",
                                        "                    nn.Linear(4 * n_embd, n_embd),\n",
                                        "                    nn.Dropout(dropout),\n",
                                        "                )\n",
                                        "            })\n",
                                        "            for _ in range(n_layer)\n",
                                        "        ])\n",
                                        "        \n",
                                        "        self.ln_f = nn.LayerNorm(n_embd)\n",
                                        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
                                        "        self.wte.weight = self.lm_head.weight\n",
                                        "    \n",
                                        "    def forward(self, idx, past_kv_list=None, use_cache=False):\n",
                                        "        B, T = idx.size()\n",
                                        "        \n",
                                        "        # Position IDs\n",
                                        "        if past_kv_list is not None:\n",
                                        "            past_length = past_kv_list[0][0].size(1)\n",
                                        "            pos = torch.arange(past_length, past_length + T, device=idx.device)\n",
                                        "        else:\n",
                                        "            pos = torch.arange(0, T, device=idx.device)\n",
                                        "        \n",
                                        "        # Embeddings\n",
                                        "        x = self.drop(self.wte(idx) + self.wpe(pos))\n",
                                        "        \n",
                                        "        # Transformer blocks\n",
                                        "        present_kv_list = []\n",
                                        "        for i, block in enumerate(self.blocks):\n",
                                        "            past_kv = past_kv_list[i] if past_kv_list else None\n",
                                        "            \n",
                                        "            attn_out, present_kv = block['attn'](\n",
                                        "                block['ln_1'](x), \n",
                                        "                past_kv=past_kv, \n",
                                        "                use_cache=use_cache\n",
                                        "            )\n",
                                        "            x = x + attn_out\n",
                                        "            x = x + block['mlp'](block['ln_2'](x))\n",
                                        "            \n",
                                        "            if use_cache:\n",
                                        "                present_kv_list.append(present_kv)\n",
                                        "        \n",
                                        "        x = self.ln_f(x)\n",
                                        "        logits = self.lm_head(x)\n",
                                        "        \n",
                                        "        return logits, present_kv_list if use_cache else None\n",
                                        "\n",
                                        "# 创建模型\n",
                                        "model = GPTWithCache(\n",
                                        "    vocab_size=100,\n",
                                        "    block_size=128,\n",
                                        "    n_layer=4,\n",
                                        "    n_head=4,\n",
                                        "    n_embd=64\n",
                                        ").to(device)\n",
                                        "\n",
                                        "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "2ca934f9",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 4. 性能对比"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "cf2d2bb8",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "@torch.no_grad()\n",
                                        "def generate_naive(model, idx, max_new_tokens):\n",
                                        "    \"\"\"朴素生成（无缓存）\"\"\"\n",
                                        "    for _ in range(max_new_tokens):\n",
                                        "        # 每次都处理完整序列\n",
                                        "        logits, _ = model(idx, use_cache=False)\n",
                                        "        logits = logits[:, -1, :]\n",
                                        "        probs = F.softmax(logits, dim=-1)\n",
                                        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
                                        "        idx = torch.cat((idx, idx_next), dim=1)\n",
                                        "    return idx\n",
                                        "\n",
                                        "@torch.no_grad()\n",
                                        "def generate_with_cache(model, idx, max_new_tokens):\n",
                                        "    \"\"\"带 KV Cache 的生成\"\"\"\n",
                                        "    # 首先处理 prompt\n",
                                        "    logits, past_kv = model(idx, use_cache=True)\n",
                                        "    logits = logits[:, -1, :]\n",
                                        "    probs = F.softmax(logits, dim=-1)\n",
                                        "    idx_next = torch.multinomial(probs, num_samples=1)\n",
                                        "    idx = torch.cat((idx, idx_next), dim=1)\n",
                                        "    \n",
                                        "    # 后续只处理新 token\n",
                                        "    for _ in range(max_new_tokens - 1):\n",
                                        "        logits, past_kv = model(idx_next, past_kv_list=past_kv, use_cache=True)\n",
                                        "        logits = logits[:, -1, :]\n",
                                        "        probs = F.softmax(logits, dim=-1)\n",
                                        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
                                        "        idx = torch.cat((idx, idx_next), dim=1)\n",
                                        "    \n",
                                        "    return idx\n",
                                        "\n",
                                        "# 预热\n",
                                        "prompt = torch.randint(0, 100, (1, 10)).to(device)\n",
                                        "_ = generate_with_cache(model, prompt.clone(), 10)\n",
                                        "\n",
                                        "# 测试不同长度\n",
                                        "lengths = [10, 20, 30, 40, 50]\n",
                                        "naive_times = []\n",
                                        "cache_times = []\n",
                                        "\n",
                                        "print(\"性能测试...\")\n",
                                        "for length in lengths:\n",
                                        "    # 朴素方法\n",
                                        "    prompt = torch.randint(0, 100, (1, 10)).to(device)\n",
                                        "    start = time.time()\n",
                                        "    _ = generate_naive(model, prompt.clone(), length)\n",
                                        "    naive_times.append(time.time() - start)\n",
                                        "    \n",
                                        "    # KV Cache\n",
                                        "    start = time.time()\n",
                                        "    _ = generate_with_cache(model, prompt.clone(), length)\n",
                                        "    cache_times.append(time.time() - start)\n",
                                        "    \n",
                                        "    print(f\"生成 {length} tokens: Naive={naive_times[-1]:.3f}s, Cache={cache_times[-1]:.3f}s, Speedup={naive_times[-1]/cache_times[-1]:.1f}x\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "e0109c25",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 可视化性能对比\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                                        "\n",
                                        "# 时间对比\n",
                                        "axes[0].plot(lengths, naive_times, 'ro-', linewidth=2, markersize=8, label='Naive')\n",
                                        "axes[0].plot(lengths, cache_times, 'go-', linewidth=2, markersize=8, label='KV Cache')\n",
                                        "axes[0].set_xlabel('Generated Tokens')\n",
                                        "axes[0].set_ylabel('Time (seconds)')\n",
                                        "axes[0].set_title('Generation Time Comparison')\n",
                                        "axes[0].legend()\n",
                                        "axes[0].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "# 加速比\n",
                                        "speedups = [n/c for n, c in zip(naive_times, cache_times)]\n",
                                        "axes[1].bar(lengths, speedups, color='steelblue')\n",
                                        "axes[1].set_xlabel('Generated Tokens')\n",
                                        "axes[1].set_ylabel('Speedup')\n",
                                        "axes[1].set_title('KV Cache Speedup')\n",
                                        "axes[1].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "d2372f7e",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 5. KV Cache 内存分析\n",
                                        "\n",
                                        "这些数字是**理论估算**，表示推理时 KV cache 的显存需求：\n",
                                        "- 不包含模型权重、优化器状态或激活\n",
                                        "- 与 batch_size、seq_len、层数、head_dim、精度线性相关\n",
                                        "- 如果模型使用 GQA/MQA，用 num_key_value_heads 替代 num_attention_heads\n",
                                        "\n",
                                        "公式：\n",
                                        "`KV = 2 * n_layer * batch * n_kv_head * seq_len * head_dim * dtype_bytes`"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "8cace5df",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 计算 KV Cache 内存占用（理论估算）\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "from transformers import AutoConfig\n",
                                        "\n",
                                        "\n",
                                        "def calculate_kv_cache_memory(batch_size, seq_len, n_layer, n_head, head_dim, dtype_bytes=2, n_kv_head=None):\n",
                                        "    \"\"\"\n",
                                        "    计算 KV Cache 内存占用\n",
                                        "    dtype_bytes: 2 for fp16, 4 for fp32\n",
                                        "    n_kv_head: GQA/MQA 的 KV 头数，缺省时等于 n_head\n",
                                        "    \"\"\"\n",
                                        "    kv_heads = n_kv_head if n_kv_head is not None else n_head\n",
                                        "    kv_size = 2 * batch_size * kv_heads * seq_len * head_dim * dtype_bytes\n",
                                        "    total_size = n_layer * kv_size\n",
                                        "    return total_size\n",
                                        "\n",
                                        "\n",
                                        "def load_spec_from_hf(model_id):\n",
                                        "    cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
                                        "    n_layer = cfg.num_hidden_layers\n",
                                        "    n_head = cfg.num_attention_heads\n",
                                        "    head_dim = cfg.hidden_size // cfg.num_attention_heads\n",
                                        "    n_kv_head = getattr(cfg, \"num_key_value_heads\", n_head)\n",
                                        "    return dict(n_layer=n_layer, n_head=n_head, n_kv_head=n_kv_head, head_dim=head_dim)\n",
                                        "\n",
                                        "\n",
                                        "model_ids = {\n",
                                        "    \"GPT-2 Small\": \"gpt2\",\n",
                                        "    \"GPT-2 Medium\": \"gpt2-medium\",\n",
                                        "    \"GPT-2 Large\": \"gpt2-large\",\n",
                                        "    \"LLaMA-7B\": \"meta-llama/Llama-2-7b-hf\",\n",
                                        "    \"LLaMA-13B\": \"meta-llama/Llama-2-13b-hf\",\n",
                                        "    \"Qwen2.5-0.5B\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
                                        "    \"Qwen2.5-1.5B\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
                                        "    \"Qwen2.5-7B\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
                                        "    \"DeepSeek-LLM-7B\": \"deepseek-ai/deepseek-llm-7b-base\",\n",
                                        "    \"DeepSeek-LLM-67B\": \"deepseek-ai/deepseek-llm-67b-base\",\n",
                                        "    \"ERNIE-4.5-21B\": \"baidu/ERNIE-4.5-21B-A3B-PT\",\n",
                                        "}\n",
                                        "\n",
                                        "# 兜底配置（HF 无法下载时使用）\n",
                                        "fallback_specs = {\n",
                                        "    \"GPT-2 Small\": {\"n_layer\": 12, \"n_head\": 12, \"head_dim\": 64, \"n_kv_head\": 12},\n",
                                        "    \"GPT-2 Medium\": {\"n_layer\": 24, \"n_head\": 16, \"head_dim\": 64, \"n_kv_head\": 16},\n",
                                        "    \"GPT-2 Large\": {\"n_layer\": 36, \"n_head\": 20, \"head_dim\": 64, \"n_kv_head\": 20},\n",
                                        "    \"LLaMA-7B\": {\"n_layer\": 32, \"n_head\": 32, \"head_dim\": 128, \"n_kv_head\": 32},\n",
                                        "    \"LLaMA-13B\": {\"n_layer\": 40, \"n_head\": 40, \"head_dim\": 128, \"n_kv_head\": 40},\n",
                                        "}\n",
                                        "\n",
                                        "batch_size = 1\n",
                                        "seq_len = 2048  # 常见上下文长度\n",
                                        "\n",
                                        "data = []\n",
                                        "for name, model_id in model_ids.items():\n",
                                        "    try:\n",
                                        "        spec = load_spec_from_hf(model_id)\n",
                                        "        source = model_id\n",
                                        "    except Exception as e:\n",
                                        "        if name in fallback_specs:\n",
                                        "            spec = fallback_specs[name]\n",
                                        "            source = \"fallback\"\n",
                                        "        else:\n",
                                        "            print(f\"跳过 {name}: {e}\")\n",
                                        "            continue\n",
                                        "\n",
                                        "    mem = calculate_kv_cache_memory(\n",
                                        "        batch_size=batch_size,\n",
                                        "        seq_len=seq_len,\n",
                                        "        n_layer=spec[\"n_layer\"],\n",
                                        "        n_head=spec[\"n_head\"],\n",
                                        "        head_dim=spec[\"head_dim\"],\n",
                                        "        n_kv_head=spec.get(\"n_kv_head\"),\n",
                                        "    )\n",
                                        "    data.append((name, mem, spec, source))\n",
                                        "\n",
                                        "# 按内存排序\n",
                                        "if not data:\n",
                                        "    raise ValueError(\"未能加载任何模型配置，请检查网络或模型 ID\")\n",
                                        "\n",
                                        "data.sort(key=lambda x: x[1])\n",
                                        "\n",
                                        "print(\"这是理论计算值（非实测）。假设 batch_size=1, seq_len=2048, fp16。\")\n",
                                        "print(\"公式: 2 * n_layer * batch * n_kv_head * seq_len * head_dim * dtype_bytes\")\n",
                                        "print(\"KV Cache 内存占用:\")\n",
                                        "print(\"=\" * 80)\n",
                                        "\n",
                                        "for name, mem, spec, source in data:\n",
                                        "    mb = mem / 1024**2\n",
                                        "    gb = mem / 1024**3\n",
                                        "    print(\n",
                                        "        f\"{name:16s}: {mb:8.1f} MB ({gb:.2f} GB) | \"\n",
                                        "        f\"layers={spec['n_layer']}, heads={spec['n_head']}, kv_heads={spec.get('n_kv_head', spec['n_head'])} | {source}\"\n",
                                        "    )\n",
                                        "\n",
                                        "# 绘图\n",
                                        "labels = [d[0] for d in data]\n",
                                        "values_gb = [d[1] / 1024**3 for d in data]\n",
                                        "\n",
                                        "plt.figure(figsize=(12, 5))\n",
                                        "plt.bar(range(len(labels)), values_gb, color=\"#4C78A8\")\n",
                                        "plt.xticks(range(len(labels)), labels, rotation=35, ha=\"right\")\n",
                                        "plt.ylabel(\"KV Cache Memory (GB, fp16, batch=1, seq_len=2048)\")\n",
                                        "plt.title(\"KV Cache 内存占用对比（理论估算）\")\n",
                                        "plt.grid(axis=\"y\", alpha=0.3)\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "5e5386da",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 本章总结\n",
                                        "\n",
                                        "\n",
                                        "1. **问题根源**\n",
                                        "   - 朴素实现是 O(n^2) 复杂度\n",
                                        "   - 每次生成都重复计算之前的 K, V\n",
                                        "\n",
                                        "2. **KV Cache 原理**\n",
                                        "   - 缓存之前 token 的 K 和 V\n",
                                        "   - 每次只计算新 token\n",
                                        "   - 复杂度降为 O(n)\n",
                                        "\n",
                                        "3. **工程实现**\n",
                                        "   - 修改 forward 支持 past_kv\n",
                                        "   - 正确处理 position id\n",
                                        "   - 注意内存占用\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "835eafed",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 练习题\n",
                                        "\n",
                                        "1. **内存优化**：实现滑动窗口 KV Cache\n",
                                        "2. **量化缓存**：用 int8 存储 K, V 减少内存\n",
                                        "3. **思考题**：为什么 Flash Attention 不需要显式 KV Cache？"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "8ca9bed3",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 练习空间\n",
                                        "\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
