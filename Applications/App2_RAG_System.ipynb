{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "intro",
                              "metadata": {},
                              "source": [
                                        "# åº”ç”¨2 | RAG ç³»ç»Ÿï¼šè®© LLM æ‹¥æœ‰\"å¼€å·è€ƒè¯•\"èƒ½åŠ›\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## ğŸ¯ æœ¬èŠ‚ç›®æ ‡\n",
                                        "\n",
                                        "æ„å»ºä¸€ä¸ª**ç”Ÿäº§çº§ RAG (Retrieval-Augmented Generation) ç³»ç»Ÿ**ï¼š\n",
                                        "- ä½¿ç”¨çœŸå®çš„ Embedding æ¨¡å‹ (SentenceTransformers)\n",
                                        "- å®ç°å®Œæ•´çš„å‘é‡æ•°æ®åº“\n",
                                        "- è¿æ¥çœŸå® LLM ç”Ÿæˆå›ç­”\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## ä»€ä¹ˆæ˜¯ RAGï¼Ÿ\n",
                                        "\n",
                                        "```\n",
                                        "ç”¨æˆ·é—®é¢˜\n",
                                        "    â†“\n",
                                        "å‘é‡åŒ–æŸ¥è¯¢ â†’ åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£\n",
                                        "    â†“\n",
                                        "å°†ç›¸å…³æ–‡æ¡£ + é—®é¢˜ å‘é€ç»™ LLM\n",
                                        "    â†“\n",
                                        "LLM åŸºäºæ–‡æ¡£ç”Ÿæˆå›ç­”\n",
                                        "```\n",
                                        "\n",
                                        "**æ ¸å¿ƒä¼˜åŠ¿**ï¼š\n",
                                        "- LLM å¯ä»¥å›ç­”**è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰çš„ä¿¡æ¯**\n",
                                        "- å‡å°‘å¹»è§‰ï¼ˆæœ‰æ–‡æ¡£æ”¯æ’‘ï¼‰\n",
                                        "- çŸ¥è¯†å¯ä»¥å®æ—¶æ›´æ–°"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "267e00ec",
                              "metadata": {},
                              "source": [
                                        "## è¿è¡Œå‰ï¼šå‡†å¤‡ Ollama åç«¯\n",
                                        "\n",
                                        "å¦‚æœä½ é€‰æ‹© Ollama ä½œä¸ºåç«¯ï¼Œè¯·å…ˆå®Œæˆå‡†å¤‡æ­¥éª¤ï¼š\n",
                                        "- å‚è§ [PREPARE_OLLAMA.ipynb](./PREPARE_OLLAMA.ipynb)\n",
                                        "\n",
                                        "å®Œæˆåå†ç»§ç»­æœ¬ notebookã€‚\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "setup-header",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 0. ç¯å¢ƒé…ç½®"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "setup-imports",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import sys\n",
                                        "import os\n",
                                        "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
                                        "\n",
                                        "import numpy as np\n",
                                        "from typing import List, Dict, Tuple, Optional\n",
                                        "from dataclasses import dataclass\n",
                                        "import json\n",
                                        "\n",
                                        "# å¯¼å…¥ç»Ÿä¸€çš„åç«¯æ¥å£\n",
                                        "from utils.llm_backend import get_llm_backend\n",
                                        "from utils.embedding_backend import get_embedding_backend, SimpleVectorStore\n",
                                        "\n",
                                        "print(\"âœ“ ç¯å¢ƒå‡†å¤‡å®Œæˆ!\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "da830dbd",
                              "metadata": {},
                              "source": [
                                        "## æ³¨æ„ç½‘ç»œè¿æ¥ ä½¿ç”¨sentence transformer"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "setup-backends",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ============================================================\n",
                                        "# é…ç½® Embedding å’Œ LLM åç«¯\n",
                                        "# ============================================================\n",
                                        "\n",
                                        "# Embedding æ¨¡å‹ (æ¨èä½¿ç”¨ sentence-transformers)\n",
                                        "try:\n",
                                        "    # å¤šè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±æ–‡\n",
                                        "    embedder = get_embedding_backend(\n",
                                        "        \"sentence-transformers\",\n",
                                        "        model=\"paraphrase-multilingual-MiniLM-L12-v2\"  # å¤šè¯­è¨€\n",
                                        "        # model=\"all-MiniLM-L6-v2\"  # ä»…è‹±æ–‡ï¼Œæ›´å¿«\n",
                                        "    )\n",
                                        "    print(f\"âœ“ Embedding æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {embedder.dimension}\")\n",
                                        "except Exception as e:\n",
                                        "    print(f\"âš ï¸ sentence-transformers ä¸å¯ç”¨: {e}\")\n",
                                        "    print(\"ä½¿ç”¨ TF-IDF å¤‡é€‰æ–¹æ¡ˆ...\")\n",
                                        "    from utils.embedding_backend import TFIDFEmbeddingBackend\n",
                                        "    embedder = TFIDFEmbeddingBackend()\n",
                                        "\n",
                                        "def check_ollama_ready(host=\"http://localhost:11434\"):\n",
                                        "    try:\n",
                                        "        import requests\n",
                                        "        resp = requests.get(f\"{host}/api/tags\", timeout=2)\n",
                                        "        return resp.status_code == 200\n",
                                        "    except Exception:\n",
                                        "        return False\n",
                                        "\n",
                                        "# LLM åç«¯\n",
                                        "if not check_ollama_ready():\n",
                                        "    print(\"Ollama æœªå¯åŠ¨ã€‚è¯·å…ˆæ‰§è¡Œ: ollama serve\")\n",
                                        "    print(\"å¹¶ä¸‹è½½æ¨¡å‹: ollama pull qwen3:4b\")\n",
                                        "try:\n",
                                        "    llm = get_llm_backend(\"ollama\", model=\"qwen3:4b\")\n",
                                        "    test = llm.chat([{\"role\": \"user\", \"content\": \"Hi\"}])\n",
                                        "    print(f\"âœ“ LLM è¿æ¥æˆåŠŸ\")\n",
                                        "except Exception as e:\n",
                                        "    print(f\"âš ï¸ Ollama ä¸å¯ç”¨: {e}\")\n",
                                        "    try:\n",
                                        "        llm = get_llm_backend(\"openai\", model=\"gpt-3.5-turbo\")\n",
                                        "        print(\"âœ“ ä½¿ç”¨ OpenAI åç«¯\")\n",
                                        "    except:\n",
                                        "        llm = None\n",
                                        "        print(\"âŒ è¯·é…ç½® LLM åç«¯\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "vector-db-header",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 1. å‘é‡æ•°æ®åº“å®ç°\n",
                                        "\n",
                                        "æˆ‘ä»¬ä½¿ç”¨ `SimpleVectorStore`ï¼Œå®ƒæ”¯æŒï¼š\n",
                                        "- æ·»åŠ æ–‡æ¡£å¹¶è‡ªåŠ¨å‘é‡åŒ–\n",
                                        "- ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢\n",
                                        "- æŒä¹…åŒ–å­˜å‚¨"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "create-vector-store",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# åˆ›å»ºå‘é‡æ•°æ®åº“\n",
                                        "vector_store = SimpleVectorStore(embedder)\n",
                                        "\n",
                                        "# å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£\n",
                                        "documents = [\n",
                                        "    # LLM åŸºç¡€çŸ¥è¯†\n",
                                        "    \"Transformeræ˜¯2017å¹´Googleåœ¨è®ºæ–‡'Attention is All You Need'ä¸­æå‡ºçš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚å®ƒå¼•å…¥äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—æ•°æ®ï¼Œæˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚\",\n",
                                        "    \"GPT (Generative Pre-trained Transformer) æ˜¯OpenAIå¼€å‘çš„è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚GPT-3æœ‰1750äº¿å‚æ•°ï¼ŒGPT-4æ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚\",\n",
                                        "    \"BERT (Bidirectional Encoder Representations from Transformers) æ˜¯Googleå¼€å‘çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨åŒå‘Transformerç¼–ç å™¨ï¼Œåœ¨NLUä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚\",\n",
                                        "    \"LLaMAæ˜¯Metaå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚LLaMA 2æœ‰7Bã€13Bã€70Bä¸‰ä¸ªç‰ˆæœ¬ï¼Œæ”¯æŒå•†ä¸šä½¿ç”¨ã€‚LLaMA 3è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚\",\n",
                                        "    \"ChatGPTæ˜¯åŸºäºGPT-3.5å’ŒGPT-4çš„å¯¹è¯AIï¼Œäº2022å¹´11æœˆå‘å¸ƒï¼Œä¸¤ä¸ªæœˆå†…ç”¨æˆ·çªç ´1äº¿ã€‚å®ƒä½¿ç”¨RLHFæŠ€æœ¯è¿›è¡Œå¯¹é½è®­ç»ƒã€‚\",\n",
                                        "    \n",
                                        "    # è®­ç»ƒæŠ€æœ¯\n",
                                        "    \"é¢„è®­ç»ƒ(Pre-training)æ˜¯åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè®©æ¨¡å‹å­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹ã€‚å¸¸ç”¨ç›®æ ‡åŒ…æ‹¬Next Token Predictionå’ŒMasked Language Modelingã€‚\",\n",
                                        "    \"SFT (Supervised Fine-Tuning) æ˜¯ç›‘ç£å¾®è°ƒï¼Œä½¿ç”¨é«˜è´¨é‡çš„æŒ‡ä»¤-å›ç­”å¯¹è®­ç»ƒæ¨¡å‹ï¼Œè®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤ã€‚è¿™æ˜¯æŠŠBase Modelå˜æˆChat Modelçš„å…³é”®æ­¥éª¤ã€‚\",\n",
                                        "    \"RLHF (Reinforcement Learning from Human Feedback) ä½¿ç”¨äººç±»åé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹ã€‚æµç¨‹åŒ…æ‹¬ï¼š1)è®­ç»ƒReward Model 2)ä½¿ç”¨PPOä¼˜åŒ–ç­–ç•¥æ¨¡å‹ã€‚\",\n",
                                        "    \"DPO (Direct Preference Optimization) æ˜¯ä¸€ç§ç®€åŒ–çš„å¯¹é½æ–¹æ³•ï¼Œç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ ï¼Œä¸éœ€è¦è®­ç»ƒå•ç‹¬çš„Reward Modelï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚\",\n",
                                        "    \"LoRA (Low-Rank Adaptation) æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œåªè®­ç»ƒä½ç§©çŸ©é˜µï¼Œå¤§å¤§å‡å°‘å¯è®­ç»ƒå‚æ•°é‡ï¼Œè®©æ¶ˆè´¹çº§æ˜¾å¡ä¹Ÿèƒ½å¾®è°ƒå¤§æ¨¡å‹ã€‚\",\n",
                                        "    \n",
                                        "    # æŠ€æœ¯ç»†èŠ‚\n",
                                        "    \"è‡ªæ³¨æ„åŠ›æœºåˆ¶(Self-Attention)è®©æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒä½ç½®ï¼Œé€šè¿‡Qã€Kã€VçŸ©é˜µè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚\",\n",
                                        "    \"KV Cacheæ˜¯æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œç¼“å­˜å·²è®¡ç®—çš„Keyå’ŒValueçŸ©é˜µï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œå¯ä»¥æ˜¾è‘—æå‡è‡ªå›å½’ç”Ÿæˆçš„é€Ÿåº¦ã€‚\",\n",
                                        "    \"é‡åŒ–(Quantization)é€šè¿‡é™ä½å‚æ•°ç²¾åº¦(FP16â†’INT8â†’INT4)æ¥å‡å°‘å†…å­˜å ç”¨ã€‚4-bité‡åŒ–å¯ä»¥è®©7Bæ¨¡å‹åªéœ€è¦4GBæ˜¾å­˜ã€‚\",\n",
                                        "    \"RAG (Retrieval-Augmented Generation) ç»“åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œå…ˆä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå†è®©LLMåŸºäºæ–‡æ¡£ç”Ÿæˆå›ç­”ï¼Œå‡å°‘å¹»è§‰ã€‚\",\n",
                                        "    \"Agentæ˜¯èƒ½å¤Ÿè‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œä»»åŠ¡çš„AIç³»ç»Ÿã€‚ReActæ¨¡å¼è®©Agenté€šè¿‡æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯æ¥è§£å†³é—®é¢˜ï¼Œå¯ä»¥è°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚\",\n",
                                        "]\n",
                                        "\n",
                                        "# æ·»åŠ å…ƒæ•°æ®\n",
                                        "metadata = [\n",
                                        "    {\"topic\": \"architecture\", \"id\": 1},\n",
                                        "    {\"topic\": \"models\", \"id\": 2},\n",
                                        "    {\"topic\": \"models\", \"id\": 3},\n",
                                        "    {\"topic\": \"models\", \"id\": 4},\n",
                                        "    {\"topic\": \"models\", \"id\": 5},\n",
                                        "    {\"topic\": \"training\", \"id\": 6},\n",
                                        "    {\"topic\": \"training\", \"id\": 7},\n",
                                        "    {\"topic\": \"training\", \"id\": 8},\n",
                                        "    {\"topic\": \"training\", \"id\": 9},\n",
                                        "    {\"topic\": \"training\", \"id\": 10},\n",
                                        "    {\"topic\": \"technical\", \"id\": 11},\n",
                                        "    {\"topic\": \"technical\", \"id\": 12},\n",
                                        "    {\"topic\": \"technical\", \"id\": 13},\n",
                                        "    {\"topic\": \"technical\", \"id\": 14},\n",
                                        "    {\"topic\": \"technical\", \"id\": 15},\n",
                                        "]\n",
                                        "\n",
                                        "# æ·»åŠ åˆ°å‘é‡æ•°æ®åº“\n",
                                        "vector_store.add_documents(documents, metadata)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "search-header",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 2. æµ‹è¯•å‘é‡æœç´¢"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-search",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def pretty_search(query: str, top_k: int = 3):\n",
                                        "    \"\"\"æ ¼å¼åŒ–æ˜¾ç¤ºæœç´¢ç»“æœ\"\"\"\n",
                                        "    print(f\"\\nğŸ” æŸ¥è¯¢: {query}\")\n",
                                        "    print(\"-\" * 60)\n",
                                        "    \n",
                                        "    results = vector_store.search(query, top_k=top_k)\n",
                                        "    \n",
                                        "    for i, result in enumerate(results, 1):\n",
                                        "        print(f\"\\nğŸ“„ ç»“æœ {i} (ç›¸ä¼¼åº¦: {result['score']:.4f})\")\n",
                                        "        print(f\"   ä¸»é¢˜: {result['metadata'].get('topic', 'N/A')}\")\n",
                                        "        print(f\"   å†…å®¹: {result['document'][:100]}...\")\n",
                                        "    \n",
                                        "    return results\n",
                                        "\n",
                                        "# æµ‹è¯•æœç´¢\n",
                                        "pretty_search(\"ä»€ä¹ˆæ˜¯Transformerï¼Ÿ\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-search-2",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æ›´å¤šæœç´¢æµ‹è¯•\n",
                                        "pretty_search(\"å¦‚ä½•è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Ÿ\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-search-3",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "pretty_search(\"LoRA æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•å‡å°‘æ˜¾å­˜ï¼Ÿ\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-search-4",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# è‹±æ–‡æŸ¥è¯¢ä¹Ÿå¯ä»¥\n",
                                        "pretty_search(\"What is RLHF and how does it work?\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "rag-header",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 3. æ„å»ºå®Œæ•´ RAG ç³»ç»Ÿ"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "rag-class",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class RAGSystem:\n",
                                        "    \"\"\"\n",
                                        "    å®Œæ•´çš„ RAG ç³»ç»Ÿ\n",
                                        "    \n",
                                        "    ç»„ä»¶:\n",
                                        "    - Embedding Model: æ–‡æœ¬å‘é‡åŒ–\n",
                                        "    - Vector Store: å­˜å‚¨å’Œæ£€ç´¢å‘é‡\n",
                                        "    - LLM: ç”Ÿæˆå›ç­”\n",
                                        "    \"\"\"\n",
                                        "    \n",
                                        "    SYSTEM_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
                                        "\n",
                                        "è§„åˆ™:\n",
                                        "1. åªä½¿ç”¨å‚è€ƒèµ„æ–™ä¸­çš„ä¿¡æ¯å›ç­”\n",
                                        "2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜\n",
                                        "3. å¼•ç”¨å…·ä½“å†…å®¹æ—¶è¯·å‡†ç¡®\n",
                                        "4. å›ç­”è¦ç®€æ´å‡†ç¡®\n",
                                        "\n",
                                        "å‚è€ƒèµ„æ–™:\n",
                                        "{context}\n",
                                        "\"\"\"\n",
                                        "    \n",
                                        "    def __init__(self, vector_store: SimpleVectorStore, llm):\n",
                                        "        self.vector_store = vector_store\n",
                                        "        self.llm = llm\n",
                                        "    \n",
                                        "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
                                        "        \"\"\"æ£€ç´¢ç›¸å…³æ–‡æ¡£\"\"\"\n",
                                        "        return self.vector_store.search(query, top_k=top_k)\n",
                                        "    \n",
                                        "    def format_context(self, results: List[Dict]) -> str:\n",
                                        "        \"\"\"æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºä¸Šä¸‹æ–‡\"\"\"\n",
                                        "        context_parts = []\n",
                                        "        for i, result in enumerate(results, 1):\n",
                                        "            context_parts.append(f\"[æ–‡æ¡£{i}] {result['document']}\")\n",
                                        "        return \"\\n\\n\".join(context_parts)\n",
                                        "    \n",
                                        "    def answer(self, question: str, top_k: int = 3, verbose: bool = True) -> str:\n",
                                        "        \"\"\"\n",
                                        "        RAG å®Œæ•´æµç¨‹ï¼šæ£€ç´¢ + ç”Ÿæˆ\n",
                                        "        \n",
                                        "        Args:\n",
                                        "            question: ç”¨æˆ·é—®é¢˜\n",
                                        "            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡\n",
                                        "            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹\n",
                                        "            \n",
                                        "        Returns:\n",
                                        "            str: ç”Ÿæˆçš„å›ç­”\n",
                                        "        \"\"\"\n",
                                        "        if verbose:\n",
                                        "            print(f\"\\n{'='*60}\")\n",
                                        "            print(f\"ğŸ¤” é—®é¢˜: {question}\")\n",
                                        "            print(f\"{'='*60}\")\n",
                                        "        \n",
                                        "        # Step 1: æ£€ç´¢\n",
                                        "        if verbose:\n",
                                        "            print(f\"\\nğŸ“š Step 1: æ£€ç´¢ç›¸å…³æ–‡æ¡£...\")\n",
                                        "        \n",
                                        "        results = self.retrieve(question, top_k)\n",
                                        "        \n",
                                        "        if verbose:\n",
                                        "            print(f\"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:\")\n",
                                        "            for i, r in enumerate(results, 1):\n",
                                        "                print(f\"  {i}. [{r['score']:.3f}] {r['document'][:50]}...\")\n",
                                        "        \n",
                                        "        if not results:\n",
                                        "            return \"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚\"\n",
                                        "        \n",
                                        "        # Step 2: æ„å»º Prompt\n",
                                        "        context = self.format_context(results)\n",
                                        "        system_prompt = self.SYSTEM_PROMPT.format(context=context)\n",
                                        "        \n",
                                        "        if verbose:\n",
                                        "            print(f\"\\nğŸ“ Step 2: æ„å»º Prompt...\")\n",
                                        "            print(f\"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦\")\n",
                                        "        \n",
                                        "        # Step 3: LLM ç”Ÿæˆå›ç­”\n",
                                        "        if verbose:\n",
                                        "            print(f\"\\nğŸ¤– Step 3: LLM ç”Ÿæˆå›ç­”...\")\n",
                                        "        \n",
                                        "        if self.llm is None:\n",
                                        "            # æ²¡æœ‰ LLMï¼Œè¿”å›æ£€ç´¢ç»“æœæ‘˜è¦\n",
                                        "            answer = f\"[æ£€ç´¢ç»“æœæ‘˜è¦]\\nåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£:\\n{context}\"\n",
                                        "        else:\n",
                                        "            messages = [\n",
                                        "                {\"role\": \"system\", \"content\": system_prompt},\n",
                                        "                {\"role\": \"user\", \"content\": question}\n",
                                        "            ]\n",
                                        "            answer = self.llm.chat(messages, temperature=0.3)\n",
                                        "        \n",
                                        "        if verbose:\n",
                                        "            print(f\"\\nâœ… å›ç­”:\\n{answer}\")\n",
                                        "        \n",
                                        "        return answer\n",
                                        "    \n",
                                        "    def answer_with_sources(self, question: str, top_k: int = 3) -> Dict:\n",
                                        "        \"\"\"\n",
                                        "        å¸¦æ¥æºçš„å›ç­”\n",
                                        "        \n",
                                        "        Returns:\n",
                                        "            dict: {\"answer\": str, \"sources\": List[Dict]}\n",
                                        "        \"\"\"\n",
                                        "        results = self.retrieve(question, top_k)\n",
                                        "        \n",
                                        "        if not results:\n",
                                        "            return {\n",
                                        "                \"answer\": \"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚\",\n",
                                        "                \"sources\": []\n",
                                        "            }\n",
                                        "        \n",
                                        "        context = self.format_context(results)\n",
                                        "        system_prompt = self.SYSTEM_PROMPT.format(context=context)\n",
                                        "        \n",
                                        "        if self.llm:\n",
                                        "            messages = [\n",
                                        "                {\"role\": \"system\", \"content\": system_prompt},\n",
                                        "                {\"role\": \"user\", \"content\": question}\n",
                                        "            ]\n",
                                        "            answer = self.llm.chat(messages, temperature=0.3)\n",
                                        "        else:\n",
                                        "            answer = results[0][\"document\"]\n",
                                        "        \n",
                                        "        return {\n",
                                        "            \"answer\": answer,\n",
                                        "            \"sources\": [\n",
                                        "                {\n",
                                        "                    \"content\": r[\"document\"],\n",
                                        "                    \"score\": r[\"score\"],\n",
                                        "                    \"metadata\": r[\"metadata\"]\n",
                                        "                }\n",
                                        "                for r in results\n",
                                        "            ]\n",
                                        "        }\n",
                                        "\n",
                                        "# åˆ›å»º RAG ç³»ç»Ÿ\n",
                                        "rag = RAGSystem(vector_store, llm)\n",
                                        "print(\"\\nâœ“ RAG ç³»ç»Ÿåˆ›å»ºæˆåŠŸ!\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "test-rag-header",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 4. æµ‹è¯• RAG ç³»ç»Ÿ"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-rag-1",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æµ‹è¯• 1: åŸºç¡€é—®ç­”\n",
                                        "rag.answer(\"ä»€ä¹ˆæ˜¯ Transformerï¼Ÿå®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-rag-2",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æµ‹è¯• 2: è®­ç»ƒç›¸å…³\n",
                                        "rag.answer(\"å¦‚ä½•è®© Base Model å˜æˆ Chat Modelï¼Ÿ\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-rag-3",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æµ‹è¯• 3: æŠ€æœ¯ç»†èŠ‚\n",
                                        "rag.answer(\"LoRA å’Œé‡åŒ–å¦‚ä½•å¸®åŠ©é™ä½æ˜¾å­˜ï¼Ÿ\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-rag-4",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æµ‹è¯• 4: å¸¦æ¥æºçš„å›ç­”\n",
                                        "result = rag.answer_with_sources(\"RLHF å’Œ DPO æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\")\n",
                                        "print(f\"\\nğŸ“ å›ç­”: {result['answer']}\")\n",
                                        "print(f\"\\nğŸ“š æ¥æº:\")\n",
                                        "for i, source in enumerate(result['sources'], 1):\n",
                                        "    print(f\"  {i}. [{source['score']:.3f}] {source['content'][:80]}...\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "test-rag-5",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æµ‹è¯• 5: çŸ¥è¯†åº“ä¸­ä¸å­˜åœ¨çš„é—®é¢˜\n",
                                        "rag.answer(\"Python çš„ GIL æ˜¯ä»€ä¹ˆï¼Ÿ\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "advanced-header",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 5. é«˜çº§åŠŸèƒ½ï¼šæ–‡æ¡£åˆ†å—ä¸é‡æ’åº"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "chunking",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class DocumentChunker:\n",
                                        "    \"\"\"\n",
                                        "    æ–‡æ¡£åˆ†å—å™¨\n",
                                        "    \n",
                                        "    é•¿æ–‡æ¡£éœ€è¦åˆ†æˆå°å—æ‰èƒ½æœ‰æ•ˆæ£€ç´¢\n",
                                        "    \"\"\"\n",
                                        "    \n",
                                        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
                                        "        self.chunk_size = chunk_size\n",
                                        "        self.chunk_overlap = chunk_overlap\n",
                                        "    \n",
                                        "    def chunk_text(self, text: str, metadata: Dict = None) -> List[Dict]:\n",
                                        "        \"\"\"\n",
                                        "        å°†æ–‡æœ¬åˆ†æˆé‡å çš„å°å—\n",
                                        "        \n",
                                        "        Args:\n",
                                        "            text: åŸå§‹æ–‡æœ¬\n",
                                        "            metadata: å…ƒæ•°æ®\n",
                                        "            \n",
                                        "        Returns:\n",
                                        "            List[Dict]: [{\"text\": str, \"metadata\": dict}, ...]\n",
                                        "        \"\"\"\n",
                                        "        if metadata is None:\n",
                                        "            metadata = {}\n",
                                        "        \n",
                                        "        chunks = []\n",
                                        "        start = 0\n",
                                        "        chunk_id = 0\n",
                                        "        \n",
                                        "        while start < len(text):\n",
                                        "            end = start + self.chunk_size\n",
                                        "            \n",
                                        "            # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²\n",
                                        "            if end < len(text):\n",
                                        "                # å¯»æ‰¾æœ€è¿‘çš„å¥å·\n",
                                        "                for punct in ['ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?', '\\n']:\n",
                                        "                    last_punct = text[start:end].rfind(punct)\n",
                                        "                    if last_punct > self.chunk_size // 2:\n",
                                        "                        end = start + last_punct + 1\n",
                                        "                        break\n",
                                        "            \n",
                                        "            chunk_text = text[start:end].strip()\n",
                                        "            if chunk_text:\n",
                                        "                chunk_metadata = {\n",
                                        "                    **metadata,\n",
                                        "                    \"chunk_id\": chunk_id,\n",
                                        "                    \"start\": start,\n",
                                        "                    \"end\": end\n",
                                        "                }\n",
                                        "                chunks.append({\n",
                                        "                    \"text\": chunk_text,\n",
                                        "                    \"metadata\": chunk_metadata\n",
                                        "                })\n",
                                        "                chunk_id += 1\n",
                                        "            \n",
                                        "            start = end - self.chunk_overlap\n",
                                        "        \n",
                                        "        return chunks\n",
                                        "    \n",
                                        "    def chunk_documents(self, documents: List[str], metadatas: List[Dict] = None) -> Tuple[List[str], List[Dict]]:\n",
                                        "        \"\"\"\n",
                                        "        æ‰¹é‡åˆ†å—æ–‡æ¡£\n",
                                        "        \n",
                                        "        Returns:\n",
                                        "            (texts, metadatas)\n",
                                        "        \"\"\"\n",
                                        "        if metadatas is None:\n",
                                        "            metadatas = [{} for _ in documents]\n",
                                        "        \n",
                                        "        all_texts = []\n",
                                        "        all_metadata = []\n",
                                        "        \n",
                                        "        for doc, meta in zip(documents, metadatas):\n",
                                        "            chunks = self.chunk_text(doc, meta)\n",
                                        "            for chunk in chunks:\n",
                                        "                all_texts.append(chunk[\"text\"])\n",
                                        "                all_metadata.append(chunk[\"metadata\"])\n",
                                        "        \n",
                                        "        return all_texts, all_metadata\n",
                                        "\n",
                                        "# æµ‹è¯•åˆ†å—\n",
                                        "chunker = DocumentChunker(chunk_size=200, chunk_overlap=30)\n",
                                        "\n",
                                        "long_doc = \"\"\"\n",
                                        "å¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)æ˜¯æŒ‡å‚æ•°é‡å·¨å¤§çš„è¯­è¨€æ¨¡å‹ã€‚\n",
                                        "è¿™äº›æ¨¡å‹é€šå¸¸åŸºäºTransformeræ¶æ„ï¼Œé€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ¥å­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹ã€‚\n",
                                        "é¢„è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡å¾®è°ƒ(Fine-tuning)æ¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚\n",
                                        "å¸¸è§çš„å¾®è°ƒæ–¹æ³•åŒ…æ‹¬SFT(ç›‘ç£å¾®è°ƒ)ã€RLHF(åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ )å’ŒDPO(ç›´æ¥åå¥½ä¼˜åŒ–)ã€‚\n",
                                        "ä¸ºäº†é™ä½å¾®è°ƒçš„è®¡ç®—æˆæœ¬ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚\n",
                                        "åœ¨æ¨ç†é˜¶æ®µï¼ŒKV CacheæŠ€æœ¯å¯ä»¥åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆï¼Œé‡åŒ–æŠ€æœ¯å¯ä»¥å‡å°‘å†…å­˜å ç”¨ã€‚\n",
                                        "\"\"\"\n",
                                        "\n",
                                        "chunks = chunker.chunk_text(long_doc, {\"source\": \"intro\"})\n",
                                        "print(f\"åŸæ–‡é•¿åº¦: {len(long_doc)} å­—ç¬¦\")\n",
                                        "print(f\"åˆ†æˆ {len(chunks)} ä¸ªå—:\")\n",
                                        "for i, chunk in enumerate(chunks):\n",
                                        "    print(f\"\\nå— {i+1} ({len(chunk['text'])} å­—ç¬¦):\")\n",
                                        "    print(f\"  {chunk['text'][:100]}...\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "summary",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## æœ¬èŠ‚æ€»ç»“\n",
                                        "\n",
                                        "\n",
                                        "1. **çœŸå®çš„ Embedding æ¨¡å‹**\n",
                                        "   - SentenceTransformers ç”Ÿæˆè¯­ä¹‰å‘é‡\n",
                                        "   - æ”¯æŒå¤šè¯­è¨€æ£€ç´¢\n",
                                        "\n",
                                        "2. **å‘é‡æ•°æ®åº“**\n",
                                        "   - ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢\n",
                                        "   - å…ƒæ•°æ®è¿‡æ»¤\n",
                                        "   - æŒä¹…åŒ–å­˜å‚¨\n",
                                        "\n",
                                        "3. **RAG å®Œæ•´æµç¨‹**\n",
                                        "   - æ£€ç´¢ â†’ é‡æ’ â†’ ç”Ÿæˆ\n",
                                        "   - å¸¦æ¥æºçš„å›ç­”\n",
                                        "\n",
                                        "### æ‰©å±•ç»ƒä¹ \n",
                                        "\n",
                                        "1. **æ·»åŠ  PDF è§£æ**: ä½¿ç”¨ PyPDF2 åŠ è½½ PDF æ–‡æ¡£\n",
                                        "2. **æ¥å…¥ ChromaDB**: ä½¿ç”¨ä¸“ä¸šçš„å‘é‡æ•°æ®åº“\n",
                                        "3. **å®ç°æ··åˆæ£€ç´¢**: ç»“åˆå…³é”®è¯å’Œè¯­ä¹‰æœç´¢"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "exercise",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ç»ƒä¹ : æ·»åŠ ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£çš„åŠŸèƒ½\n",
                                        "\n",
                                        "def load_text_file(file_path: str) -> str:\n",
                                        "    \"\"\"åŠ è½½æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
                                        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                                        "        return f.read()\n",
                                        "\n",
                                        "def load_markdown_file(file_path: str) -> List[str]:\n",
                                        "    \"\"\"åŠ è½½ Markdown æ–‡ä»¶ï¼ŒæŒ‰æ ‡é¢˜åˆ†å‰²\"\"\"\n",
                                        "    import re\n",
                                        "    content = load_text_file(file_path)\n",
                                        "    \n",
                                        "    # æŒ‰ ## æ ‡é¢˜åˆ†å‰²\n",
                                        "    sections = re.split(r'\\n## ', content)\n",
                                        "    return [s.strip() for s in sections if s.strip()]\n",
                                        "\n",
                                        "# ç¤ºä¾‹ç”¨æ³• (éœ€è¦å®é™…æ–‡ä»¶)\n",
                                        "# docs = load_markdown_file('knowledge_base.md')\n",
                                        "# vector_store.add_documents(docs)\n",
                                        "\n",
                                        "#print(\"âœ“ æ–‡æ¡£åŠ è½½å™¨å·²å®šä¹‰\")"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
