{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 应用 4：多智能体协作\n",
                "\n",
                "## 概览\n",
                "\n",
                "多智能体系统由多个专业化智能体协同工作，用于解决复杂问题。\n",
                "\n",
                "**本 notebook 演示真实的多智能体协作，基于 LLM 的推理驱动。**\n",
                "\n",
                "### 为什么需要多智能体系统？\n",
                "\n",
                "1. **分工协作**：每个智能体专注特定任务\n",
                "2. **涌现智能**：组合后能解决单个智能体无法完成的问题\n",
                "3. **质量控制**：智能体之间相互审阅与质检\n",
                "4. **可扩展性**：按需添加新的专家智能体\n",
                "\n",
                "## 常见的多智能体模式\n",
                "\n",
                "1. **监督者模式**：由一个智能体统一协调\n",
                "2. **辩论模式**：多个智能体讨论并达成共识\n",
                "3. **流水线模式**：由专家按序处理\n",
                "4. **群体模式**：自治智能体围绕共同目标协作\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7770a35c",
            "metadata": {},
            "source": [
                "## 运行前：准备 Ollama 后端\n",
                "\n",
                "如果你选择 Ollama 作为后端，请先完成准备步骤：\n",
                "- 参见 [PREPARE_OLLAMA.ipynb](./PREPARE_OLLAMA.ipynb)\n",
                "\n",
                "完成后再继续本 notebook。\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 环境准备与依赖\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
                "\n",
                "from typing import Dict, List, Any, Optional, Callable\n",
                "from dataclasses import dataclass, field\n",
                "from enum import Enum\n",
                "from abc import ABC, abstractmethod\n",
                "import time\n",
                "import json\n",
                "import re\n",
                "\n",
                "# Import our LLM backend\n",
                "from utils.llm_backend import get_llm_backend, BaseLLMBackend\n",
                "\n",
                "print(\"Multi-Agent System dependencies loaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1：消息协议与基础 Agent\n",
                "\n",
                "首先定义智能体之间的通信协议。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MessageType(Enum):\n",
                "    \"\"\"Types of messages agents can send.\"\"\"\n",
                "    TASK = \"task\"           # Assignment of work\n",
                "    RESULT = \"result\"       # Completed work\n",
                "    QUESTION = \"question\"   # Request for clarification\n",
                "    FEEDBACK = \"feedback\"   # Review/critique of work\n",
                "    STATUS = \"status\"       # Progress update\n",
                "    DELEGATE = \"delegate\"   # Delegation to sub-agent\n",
                "\n",
                "@dataclass\n",
                "class Message:\n",
                "    \"\"\"Structured message passed between agents.\"\"\"\n",
                "    sender: str\n",
                "    receiver: str\n",
                "    msg_type: MessageType\n",
                "    content: Any\n",
                "    context: Dict[str, Any] = field(default_factory=dict)\n",
                "    timestamp: float = field(default_factory=time.time)\n",
                "    message_id: str = field(default_factory=lambda: f\"msg_{int(time.time()*1000)}\")\n",
                "    \n",
                "    def __str__(self):\n",
                "        preview = str(self.content)[:80] + \"...\" if len(str(self.content)) > 80 else str(self.content)\n",
                "        return f\"[{self.sender} -> {self.receiver}] ({self.msg_type.value}): {preview}\"\n",
                "\n",
                "\n",
                "class BaseAgent(ABC):\n",
                "    \"\"\"\n",
                "    Base class for all LLM-powered agents.\n",
                "    \n",
                "    Each agent has:\n",
                "    - A name and role description\n",
                "    - An LLM backend for reasoning\n",
                "    - Message inbox/outbox for communication\n",
                "    - Internal state for tracking work\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self, \n",
                "        name: str, \n",
                "        role: str,\n",
                "        llm: BaseLLMBackend,\n",
                "        system_prompt: str = None\n",
                "    ):\n",
                "        self.name = name\n",
                "        self.role = role\n",
                "        self.llm = llm\n",
                "        self.system_prompt = system_prompt or f\"You are {name}, a {role}.\"\n",
                "        \n",
                "        # Communication\n",
                "        self.inbox: List[Message] = []\n",
                "        self.outbox: List[Message] = []\n",
                "        self.message_history: List[Message] = []\n",
                "        \n",
                "        # State\n",
                "        self.state: Dict[str, Any] = {}\n",
                "        self.conversation_history: List[Dict[str, str]] = []\n",
                "    \n",
                "    def receive(self, message: Message):\n",
                "        \"\"\"Receive a message into inbox.\"\"\"\n",
                "        self.inbox.append(message)\n",
                "        self.message_history.append(message)\n",
                "    \n",
                "    def send(self, receiver: str, msg_type: MessageType, content: Any, context: Dict = None) -> Message:\n",
                "        \"\"\"Create and queue a message to send.\"\"\"\n",
                "        msg = Message(\n",
                "            sender=self.name,\n",
                "            receiver=receiver,\n",
                "            msg_type=msg_type,\n",
                "            content=content,\n",
                "            context=context or {}\n",
                "        )\n",
                "        self.outbox.append(msg)\n",
                "        self.message_history.append(msg)\n",
                "        return msg\n",
                "    \n",
                "    def think(self, prompt: str, include_history: bool = True) -> str:\n",
                "        \"\"\"\n",
                "        Use LLM to reason about a prompt.\n",
                "        \n",
                "        Args:\n",
                "            prompt: The prompt to reason about\n",
                "            include_history: Whether to include conversation history\n",
                "        \n",
                "        Returns:\n",
                "            LLM response\n",
                "        \"\"\"\n",
                "        messages = []\n",
                "        \n",
                "        # Add system prompt\n",
                "        messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
                "        \n",
                "        # Add conversation history if requested\n",
                "        if include_history:\n",
                "            messages.extend(self.conversation_history[-10:])  # Last 10 exchanges\n",
                "        \n",
                "        # Add current prompt\n",
                "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
                "        \n",
                "        # Get LLM response\n",
                "        response = self.llm.chat(messages)\n",
                "        \n",
                "        # Update conversation history\n",
                "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
                "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
                "        \n",
                "        return response\n",
                "    \n",
                "    def process(self) -> List[Message]:\n",
                "        \"\"\"\n",
                "        Process all messages in inbox.\n",
                "        Returns list of response messages.\n",
                "        \"\"\"\n",
                "        responses = []\n",
                "        for message in self.inbox:\n",
                "            result = self._handle_message(message)\n",
                "            if result:\n",
                "                responses.extend(result if isinstance(result, list) else [result])\n",
                "        self.inbox.clear()\n",
                "        return responses\n",
                "    \n",
                "    @abstractmethod\n",
                "    def _handle_message(self, message: Message) -> Optional[List[Message]]:\n",
                "        \"\"\"Handle a single message. Override in subclasses.\"\"\"\n",
                "        pass\n",
                "    \n",
                "    def __repr__(self):\n",
                "        return f\"{self.name} ({self.role})\"\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2：专业化 LLM Agent\n",
                "\n",
                "为软件开发工作流创建专业化智能体：\n",
                "- **PlannerAgent**：拆解任务、协调流程\n",
                "- **CoderAgent**：用 LLM 编写代码\n",
                "- **ReviewerAgent**：用 LLM 推理进行代码审查\n",
                "- **TesterAgent**：生成并运行测试\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PlannerAgent(BaseAgent):\n",
                "    \"\"\"\n",
                "    LLM-powered task planner and coordinator.\n",
                "    \n",
                "    Responsibilities:\n",
                "    - Break down complex tasks into subtasks\n",
                "    - Assign work to appropriate agents\n",
                "    - Track progress and compile results\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, llm: BaseLLMBackend):\n",
                "        system_prompt = \"\"\"You are the Planner Agent, responsible for breaking down complex software tasks into clear, actionable subtasks.\n",
                "\n",
                "When given a task:\n",
                "1. Analyze the requirements thoroughly\n",
                "2. Break it into 3-6 specific, implementable subtasks\n",
                "3. Consider edge cases and error handling\n",
                "4. Format your response as a numbered list\n",
                "\n",
                "Be specific and technical in your planning. Each subtask should be independently implementable.\"\"\"\n",
                "        \n",
                "        super().__init__(\"Planner\", \"Task Planning & Coordination\", llm, system_prompt)\n",
                "        self.current_task = None\n",
                "        self.subtasks = []\n",
                "        self.results = {}\n",
                "        self.review_cycles = 0\n",
                "    \n",
                "    def _extract_verdict(self, review_text: str) -> str:\n",
                "        match = re.search(r'VERDICT\\s*:\\s*(.*)', review_text, re.IGNORECASE)\n",
                "        return match.group(1).strip() if match else \"\"\n",
                "    \n",
                "    def _is_review_approved(self, review_text: str) -> bool:\n",
                "        verdict = self._extract_verdict(review_text)\n",
                "        if verdict:\n",
                "            verdict_upper = verdict.upper()\n",
                "            if \"APPROVED\" in verdict_upper or \"PASS\" in verdict_upper:\n",
                "                return True\n",
                "            if \"NEEDS_REVISION\" in verdict_upper or \"FAIL\" in verdict_upper:\n",
                "                return False\n",
                "        upper = review_text.upper()\n",
                "        if \"VERDICT: APPROVED\" in upper or \"APPROVED\" in upper:\n",
                "            return True\n",
                "        if \"VERDICT: NEEDS_REVISION\" in upper or \"NEEDS_REVISION\" in upper or \"FAIL\" in upper:\n",
                "            return False\n",
                "        return False\n",
                "    \n",
                "    def _handle_message(self, message: Message) -> List[Message]:\n",
                "        if message.msg_type == MessageType.TASK:\n",
                "            self.current_task = message.content\n",
                "            self.review_cycles = 0\n",
                "            \n",
                "            # Use LLM to break down the task\n",
                "            planning_prompt = f\"\"\"Task to break down: {message.content}\n",
                "\n",
                "Create a detailed implementation plan with specific subtasks.\n",
                "Format your response as:\n",
                "\n",
                "ANALYSIS:\n",
                "[Your analysis of the task]\n",
                "\n",
                "SUBTASKS:\n",
                "1. [First subtask]\n",
                "2. [Second subtask]\n",
                "...\n",
                "\n",
                "IMPLEMENTATION_ORDER:\n",
                "[Which subtasks should be done first and why]\"\"\"\n",
                "            \n",
                "            plan = self.think(planning_prompt)\n",
                "            self.state['plan'] = plan\n",
                "            \n",
                "            # Send combined task to Coder\n",
                "            coder_task = f\"\"\"Original Task: {self.current_task}\n",
                "\n",
                "Planning Analysis:\n",
                "{plan}\n",
                "\n",
                "Please implement a complete solution covering all the subtasks above.\"\"\"\n",
                "            \n",
                "            return [self.send(\"Coder\", MessageType.TASK, coder_task)]\n",
                "        \n",
                "        elif message.msg_type == MessageType.RESULT:\n",
                "            # Forward code to Reviewer\n",
                "            self.results['code'] = message.content\n",
                "            return [self.send(\"Reviewer\", MessageType.TASK, message.content, \n",
                "                            context={'original_task': self.current_task})]\n",
                "        \n",
                "        elif message.msg_type == MessageType.FEEDBACK:\n",
                "            # Check if it's from Reviewer or Tester\n",
                "            if message.sender == \"Reviewer\":\n",
                "                self.results['review'] = message.content\n",
                "                approved = self._is_review_approved(message.content)\n",
                "                if approved:\n",
                "                    self.review_cycles = 0\n",
                "                    return [self.send(\"Tester\", MessageType.TASK, self.results['code'],\n",
                "                                    context={'original_task': self.current_task})]\n",
                "                \n",
                "                self.review_cycles += 1\n",
                "                if self.review_cycles >= 2:\n",
                "                    self.state['review_warning'] = \"Review did not explicitly approve after multiple rounds. Proceeding to tests for demo.\"\n",
                "                    return [self.send(\"Tester\", MessageType.TASK, self.results['code'],\n",
                "                                    context={'original_task': self.current_task})]\n",
                "                \n",
                "                return [self.send(\"Coder\", MessageType.FEEDBACK, message.content,\n",
                "                                context={'code': self.results['code']})]\n",
                "            \n",
                "            elif message.sender == \"Tester\":\n",
                "                self.results['tests'] = message.content\n",
                "                \n",
                "                # Compile final result\n",
                "                final_result = self._compile_final_result()\n",
                "                self.state['final_result'] = final_result\n",
                "                return []\n",
                "        \n",
                "        return []\n",
                "    \n",
                "    def _compile_final_result(self) -> str:\n",
                "        \"\"\"Compile all results into a final summary.\"\"\"\n",
                "        summary_prompt = f\"\"\"Compile a final summary of this software development task.\n",
                "\n",
                "Original Task: {self.current_task}\n",
                "\n",
                "Plan:\n",
                "{self.state.get('plan', 'N/A')}\n",
                "\n",
                "Code Implementation:\n",
                "{self.results.get('code', 'N/A')}\n",
                "\n",
                "Code Review:\n",
                "{self.results.get('review', 'N/A')}\n",
                "\n",
                "Test Results:\n",
                "{self.results.get('tests', 'N/A')}\n",
                "\n",
                "Provide a concise summary including:\n",
                "1. What was built\n",
                "2. Key implementation details\n",
                "3. Test coverage\n",
                "4. Any recommendations\"\"\"\n",
                "        \n",
                "        summary = self.think(summary_prompt)\n",
                "        code = self.results.get('code', 'N/A')\n",
                "        return f\"FINAL CODE:\\n{code}\\n\\n{summary}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CoderAgent(BaseAgent):\n",
                "    \"\"\"\n",
                "    LLM-powered code generation agent.\n",
                "    \n",
                "    Responsibilities:\n",
                "    - Write clean, functional code\n",
                "    - Follow best practices\n",
                "    - Handle edge cases\n",
                "    - Respond to review feedback\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, llm: BaseLLMBackend):\n",
                "        system_prompt = \"\"\"You are the Coder Agent, an expert Python programmer.\n",
                "\n",
                "When writing code:\n",
                "1. Write clean, readable, well-documented code\n",
                "2. Include type hints where appropriate\n",
                "3. Handle edge cases and errors gracefully\n",
                "4. Follow PEP 8 style guidelines\n",
                "5. Include brief docstrings for functions/classes\n",
                "\n",
                "Format your code response as:\n",
                "```python\n",
                "[Your code here]\n",
                "```\n",
                "\n",
                "EXPLANATION:\n",
                "[Brief explanation of your implementation]\"\"\"\n",
                "        \n",
                "        super().__init__(\"Coder\", \"Code Implementation\", llm, system_prompt)\n",
                "    \n",
                "    def _handle_message(self, message: Message) -> List[Message]:\n",
                "        if message.msg_type == MessageType.TASK:\n",
                "            # Generate code using LLM\n",
                "            coding_prompt = f\"\"\"Implement the following:\n",
                "\n",
                "{message.content}\n",
                "\n",
                "Write complete, working Python code. Include example usage at the end.\"\"\"\n",
                "            \n",
                "            code_response = self.think(coding_prompt)\n",
                "            self.state['latest_code'] = code_response\n",
                "            \n",
                "            return [self.send(\"Planner\", MessageType.RESULT, code_response)]\n",
                "        \n",
                "        elif message.msg_type == MessageType.FEEDBACK:\n",
                "            # Revise code based on feedback\n",
                "            original_code = message.context.get('code', self.state.get('latest_code', ''))\n",
                "            \n",
                "            revision_prompt = f\"\"\"Your code received the following review feedback:\n",
                "\n",
                "{message.content}\n",
                "\n",
                "Original code:\n",
                "{original_code}\n",
                "\n",
                "Please revise your code to address ALL the feedback points. Show the complete updated code.\"\"\"\n",
                "            \n",
                "            revised_code = self.think(revision_prompt)\n",
                "            self.state['latest_code'] = revised_code\n",
                "            \n",
                "            return [self.send(\"Planner\", MessageType.RESULT, revised_code)]\n",
                "        \n",
                "        return []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ReviewerAgent(BaseAgent):\n",
                "    \"\"\"\n",
                "    LLM-powered code review agent.\n",
                "    \n",
                "    Responsibilities:\n",
                "    - Review code for correctness\n",
                "    - Check for best practices\n",
                "    - Identify potential bugs\n",
                "    - Suggest improvements\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, llm: BaseLLMBackend):\n",
                "        system_prompt = \"\"\"You are the Reviewer Agent, an expert code reviewer.\n",
                "\n",
                "When reviewing code, analyze:\n",
                "1. **Correctness**: Does the code do what it's supposed to?\n",
                "2. **Error Handling**: Are edge cases and errors handled?\n",
                "3. **Code Quality**: Is it readable, maintainable, well-structured?\n",
                "4. **Security**: Any potential security issues?\n",
                "5. **Performance**: Any obvious performance problems?\n",
                "\n",
                "Format your review as:\n",
                "\n",
                "CORRECTNESS: [PASS/FAIL] - [Comments]\n",
                "ERROR_HANDLING: [PASS/NEEDS_WORK] - [Comments]\n",
                "CODE_QUALITY: [PASS/NEEDS_WORK] - [Comments]\n",
                "SECURITY: [PASS/NEEDS_WORK] - [Comments]\n",
                "\n",
                "ISSUES:\n",
                "- [List any specific issues]\n",
                "\n",
                "SUGGESTIONS:\n",
                "- [List any improvements]\n",
                "\n",
                "VERDICT: [APPROVED/NEEDS_REVISION]\n",
                "\n",
                "IMPORTANT: Output must include the VERDICT line exactly as shown above, in English.\"\"\"\n",
                "        \n",
                "        super().__init__(\"Reviewer\", \"Code Review\", llm, system_prompt)\n",
                "    \n",
                "    def _handle_message(self, message: Message) -> List[Message]:\n",
                "        if message.msg_type == MessageType.TASK:\n",
                "            original_task = message.context.get('original_task', 'Not specified')\n",
                "            \n",
                "            review_prompt = f\"\"\"Review the following code:\n",
                "\n",
                "Original Task: {original_task}\n",
                "\n",
                "Code to Review:\n",
                "{message.content}\n",
                "\n",
                "Provide a thorough code review following your review format.\"\"\"\n",
                "            \n",
                "            review = self.think(review_prompt)\n",
                "            self.state['latest_review'] = review\n",
                "            \n",
                "            return [self.send(\"Planner\", MessageType.FEEDBACK, review)]\n",
                "        \n",
                "        return []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TesterAgent(BaseAgent):\n",
                "    \"\"\"\n",
                "    LLM-powered test generation and execution agent.\n",
                "    \n",
                "    Responsibilities:\n",
                "    - Generate comprehensive test cases\n",
                "    - Execute tests safely\n",
                "    - Report results\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, llm: BaseLLMBackend):\n",
                "        system_prompt = \"\"\"You are the Tester Agent, an expert in software testing.\n",
                "\n",
                "When testing code:\n",
                "1. Generate comprehensive test cases\n",
                "2. Test normal cases, edge cases, and error cases\n",
                "3. Execute tests and report results\n",
                "4. Be thorough but practical\n",
                "\n",
                "Format your test report as:\n",
                "\n",
                "TEST_CASES:\n",
                "1. [Test name]: [Description]\n",
                "2. ...\n",
                "\n",
                "TEST_CODE:\n",
                "```python\n",
                "[Test code]\n",
                "```\n",
                "\n",
                "RESULTS:\n",
                "- Test 1: [PASS/FAIL] - [Details]\n",
                "- Test 2: [PASS/FAIL] - [Details]\n",
                "...\n",
                "\n",
                "SUMMARY: [X/Y tests passed]\"\"\"\n",
                "        \n",
                "        super().__init__(\"Tester\", \"Testing & Validation\", llm, system_prompt)\n",
                "    \n",
                "    def _handle_message(self, message: Message) -> List[Message]:\n",
                "        if message.msg_type == MessageType.TASK:\n",
                "            original_task = message.context.get('original_task', 'Not specified')\n",
                "            \n",
                "            # Extract code from message\n",
                "            code = self._extract_code(message.content)\n",
                "            \n",
                "            test_prompt = f\"\"\"Generate and run tests for the following code:\n",
                "\n",
                "Original Task: {original_task}\n",
                "\n",
                "Code to Test:\n",
                "{message.content}\n",
                "\n",
                "Generate comprehensive test cases including:\n",
                "1. Normal operation tests\n",
                "2. Edge case tests\n",
                "3. Error handling tests\n",
                "\n",
                "Write the tests and show expected results.\"\"\"\n",
                "            \n",
                "            # Generate tests using LLM\n",
                "            test_report = self.think(test_prompt)\n",
                "            \n",
                "            # Try to actually run the tests\n",
                "            execution_result = self._execute_tests(code, test_report)\n",
                "            \n",
                "            full_report = f\"{test_report}\\n\\n--- Actual Execution ---\\n{execution_result}\"\n",
                "            self.state['latest_tests'] = full_report\n",
                "            \n",
                "            return [self.send(\"Planner\", MessageType.FEEDBACK, full_report)]\n",
                "        \n",
                "        return []\n",
                "    \n",
                "    def _extract_code(self, content: str) -> str:\n",
                "        \"\"\"Extract Python code from content.\"\"\"\n",
                "        if '```python' in content:\n",
                "            match = re.search(r'```python\\n(.*?)\\n```', content, re.DOTALL)\n",
                "            if match:\n",
                "                return match.group(1)\n",
                "        return content\n",
                "    \n",
                "    def _execute_tests(self, code: str, test_report: str) -> str:\n",
                "        \"\"\"Safely execute the code and basic tests.\"\"\"\n",
                "        try:\n",
                "            # Create a safe namespace\n",
                "            namespace = {'__builtins__': __builtins__}\n",
                "            \n",
                "            # Execute the main code\n",
                "            exec(code, namespace)\n",
                "            \n",
                "            # Extract and run test code if present\n",
                "            test_code = self._extract_code(test_report)\n",
                "            if test_code and test_code != test_report:\n",
                "                exec(test_code, namespace)\n",
                "            \n",
                "            return \"Code executed successfully without errors.\"\n",
                "        except Exception as e:\n",
                "            return f\"Execution error: {type(e).__name__}: {str(e)}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3：多智能体编排器\n",
                "\n",
                "编排器负责管理智能体之间的通信与工作流。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiAgentOrchestrator:\n",
                "    \"\"\"\n",
                "    Orchestrates communication and workflow between multiple agents.\n",
                "    \n",
                "    Features:\n",
                "    - Agent registration and management\n",
                "    - Message routing and delivery\n",
                "    - Workflow execution with iteration limits\n",
                "    - Full transcript logging\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, max_iterations: int = 15):\n",
                "        self.agents: Dict[str, BaseAgent] = {}\n",
                "        self.message_log: List[Message] = []\n",
                "        self.max_iterations = max_iterations\n",
                "    \n",
                "    def register(self, agent: BaseAgent):\n",
                "        \"\"\"Register an agent with the orchestrator.\"\"\"\n",
                "        self.agents[agent.name] = agent\n",
                "        print(f\"Registered: {agent}\")\n",
                "    \n",
                "    def deliver_messages(self):\n",
                "        \"\"\"Deliver all pending messages from agent outboxes.\"\"\"\n",
                "        for agent in self.agents.values():\n",
                "            for msg in agent.outbox:\n",
                "                if msg.receiver in self.agents:\n",
                "                    self.agents[msg.receiver].receive(msg)\n",
                "                    self.message_log.append(msg)\n",
                "            agent.outbox.clear()\n",
                "    \n",
                "    def run(self, initial_task: str, verbose: bool = True) -> str:\n",
                "        \"\"\"\n",
                "        Run the multi-agent workflow.\n",
                "        \n",
                "        Args:\n",
                "            initial_task: The task to accomplish\n",
                "            verbose: Whether to print progress\n",
                "            \n",
                "        Returns:\n",
                "            Final result from the Planner\n",
                "        \"\"\"\n",
                "        if verbose:\n",
                "            print(f\"\\n{'='*60}\")\n",
                "            print(f\"Multi-Agent Workflow Started\")\n",
                "            print(f\"Task: {initial_task}\")\n",
                "            print(f\"{'='*60}\\n\")\n",
                "        \n",
                "        # Send initial task to Planner\n",
                "        initial_msg = Message(\n",
                "            sender=\"User\",\n",
                "            receiver=\"Planner\",\n",
                "            msg_type=MessageType.TASK,\n",
                "            content=initial_task\n",
                "        )\n",
                "        self.agents[\"Planner\"].receive(initial_msg)\n",
                "        self.message_log.append(initial_msg)\n",
                "        \n",
                "        last_activity = False\n",
                "        # Run until no more activity or max iterations\n",
                "        for iteration in range(self.max_iterations):\n",
                "            if verbose:\n",
                "                print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
                "            \n",
                "            any_activity = False\n",
                "            \n",
                "            for agent in self.agents.values():\n",
                "                if agent.inbox:\n",
                "                    any_activity = True\n",
                "                    if verbose:\n",
                "                        print(f\"\\n{agent.name} processing {len(agent.inbox)} message(s)...\")\n",
                "                    \n",
                "                    responses = agent.process()\n",
                "                    \n",
                "                    for msg in responses:\n",
                "                        if verbose:\n",
                "                            print(f\"  -> Sending to {msg.receiver}: {msg.msg_type.value}\")\n",
                "            \n",
                "            # Deliver messages\n",
                "            self.deliver_messages()\n",
                "            last_activity = any_activity\n",
                "            \n",
                "            if not any_activity:\n",
                "                if verbose:\n",
                "                    print(\"\\nWorkflow complete - no more activity\")\n",
                "                break\n",
                "        \n",
                "        if last_activity and verbose:\n",
                "            print(\"\\nMax iterations reached - workflow incomplete\")\n",
                "        \n",
                "        # Return final result\n",
                "        planner = self.agents.get(\"Planner\")\n",
                "        if planner and 'final_result' in planner.state:\n",
                "            return planner.state['final_result']\n",
                "        \n",
                "        return \"Workflow completed (no final result available)\"\n",
                "    \n",
                "    def get_transcript(self) -> str:\n",
                "        \"\"\"Get full message transcript.\"\"\"\n",
                "        transcript = \"\\nMessage Transcript\\n\" + \"=\"*60 + \"\\n\\n\"\n",
                "        \n",
                "        for i, msg in enumerate(self.message_log, 1):\n",
                "            transcript += f\"[{i}] {msg.sender} -> {msg.receiver} ({msg.msg_type.value})\\n\"\n",
                "            transcript += \"-\"*40 + \"\\n\"\n",
                "            transcript += f\"{msg.content}\\n\"\n",
                "            transcript += \"\\n\"\n",
                "        \n",
                "        return transcript\n",
                "    \n",
                "    def get_summary(self) -> Dict[str, Any]:\n",
                "        \"\"\"Get workflow summary statistics.\"\"\"\n",
                "        return {\n",
                "            'total_messages': len(self.message_log),\n",
                "            'messages_by_type': {\n",
                "                mt.value: len([m for m in self.message_log if m.msg_type == mt])\n",
                "                for mt in MessageType\n",
                "            },\n",
                "            'messages_by_agent': {\n",
                "                name: len([m for m in self.message_log if m.sender == name])\n",
                "                for name in self.agents.keys()\n",
                "            }\n",
                "        }\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4：运行多智能体系统\n",
                "\n",
                "让我们搭建并运行 LLM 驱动的多智能体系统。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the LLM backend\n",
                "# Options: \"ollama\", \"openai\", \"huggingface\", \"vllm\"\n",
                "\n",
                "def check_ollama_ready(host=\"http://localhost:11434\"):\n",
                "    try:\n",
                "        import requests\n",
                "        resp = requests.get(f\"{host}/api/tags\", timeout=2)\n",
                "        return resp.status_code == 200\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "if not check_ollama_ready():\n",
                "    print(\"Ollama 未启动。请先执行: ollama serve\")\n",
                "    print(\"并下载模型: ollama pull qwen3:4b\")\n",
                "\n",
                "llm = get_llm_backend(\n",
                "    backend=\"ollama\",        # Change to \"openai\" if you have API key\n",
                "    model=\"qwen3:4b\",     # Or \"gpt-4o-mini\" for OpenAI\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "print(f\"Using LLM: {llm}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the multi-agent system\n",
                "orchestrator = MultiAgentOrchestrator(max_iterations=15)\n",
                "\n",
                "# Register specialized agents (all using the same LLM backend)\n",
                "orchestrator.register(PlannerAgent(llm))\n",
                "orchestrator.register(CoderAgent(llm))\n",
                "orchestrator.register(ReviewerAgent(llm))\n",
                "orchestrator.register(TesterAgent(llm))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run a real task!\n",
                "task = \"\"\"Write a Python function factorial(n) that:\n",
                "- returns the factorial of n (n >= 0)\n",
                "- raises ValueError if n is negative\n",
                "\n",
                "Include a short example usage that prints factorial(5).\"\"\"\n",
                "\n",
                "result = orchestrator.run(task, verbose=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the final result\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"FINAL RESULT\")\n",
                "print(\"=\"*60)\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View workflow statistics\n",
                "import json\n",
                "summary = orchestrator.get_summary()\n",
                "print(\"Workflow Statistics:\")\n",
                "print(json.dumps(summary, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optionally view full transcript\n",
                "# print(orchestrator.get_transcript())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5：辩论模式\n",
                "\n",
                "另一个强大的模式：多个智能体从不同视角辩论问题。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DebateAgent(BaseAgent):\n",
                "    \"\"\"\n",
                "    LLM-powered debate agent that argues for a specific position.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, name: str, position: str, llm: BaseLLMBackend):\n",
                "        system_prompt = f\"\"\"You are {name}, arguing in favor of {position}.\n",
                "\n",
                "Your role:\n",
                "1. Present compelling arguments for your position\n",
                "2. Use facts, logic, and practical examples\n",
                "3. Address counterarguments when relevant\n",
                "4. Stay focused on the technical merits\n",
                "\n",
                "Keep arguments concise (2-3 sentences each). Be persuasive but fair.\"\"\"\n",
                "        \n",
                "        super().__init__(name, f\"Advocate for {position}\", llm, system_prompt)\n",
                "        self.position = position\n",
                "        self.round = 0\n",
                "    \n",
                "    def _handle_message(self, message: Message) -> List[Message]:\n",
                "        if message.msg_type == MessageType.TASK:\n",
                "            self.round += 1\n",
                "            topic = message.context.get('topic', 'the given topic')\n",
                "            opponent_argument = message.content\n",
                "            \n",
                "            if self.round == 1:\n",
                "                prompt = f\"\"\"Topic: {topic}\n",
                "\n",
                "This is your opening statement. Present your strongest argument for {self.position}.\n",
                "\n",
                "Format: Start with 'ARGUMENT:' followed by your point.\"\"\"\n",
                "            else:\n",
                "                prompt = f\"\"\"Topic: {topic}\n",
                "\n",
                "Your opponent argued:\n",
                "{opponent_argument}\n",
                "\n",
                "Round {self.round}: Counter their argument and present a new point for {self.position}.\n",
                "\n",
                "Format: Start with 'COUNTER:' then 'NEW POINT:'\"\"\"\n",
                "            \n",
                "            response = self.think(prompt)\n",
                "            return [self.send(\"Moderator\", MessageType.RESULT, response, \n",
                "                            context={'round': self.round, 'position': self.position})]\n",
                "        \n",
                "        return []\n",
                "\n",
                "\n",
                "class ModeratorAgent(BaseAgent):\n",
                "    \"\"\"\n",
                "    LLM-powered moderator that evaluates debate and declares winner.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, llm: BaseLLMBackend, max_rounds: int = 3):\n",
                "        system_prompt = \"\"\"You are a fair and impartial debate moderator.\n",
                "\n",
                "Your role:\n",
                "1. Evaluate arguments objectively\n",
                "2. Consider logic, evidence, and practical merit\n",
                "3. Declare a winner based on argument quality, not personal bias\n",
                "4. Explain your reasoning clearly\"\"\"\n",
                "        \n",
                "        super().__init__(\"Moderator\", \"Debate Moderator\", llm, system_prompt)\n",
                "        self.max_rounds = max_rounds\n",
                "        self.current_round = 0\n",
                "        self.arguments: Dict[str, List[str]] = {}\n",
                "        self.debaters: List[str] = []\n",
                "        self.topic = \"\"\n",
                "    \n",
                "    def start_debate(self, topic: str, debaters: List[str]):\n",
                "        \"\"\"Initialize a new debate.\"\"\"\n",
                "        self.topic = topic\n",
                "        self.debaters = debaters\n",
                "        self.current_round = 0\n",
                "        self.arguments = {d: [] for d in debaters}\n",
                "    \n",
                "    def _handle_message(self, message: Message) -> List[Message]:\n",
                "        if message.msg_type == MessageType.RESULT:\n",
                "            sender = message.sender\n",
                "            if sender in self.arguments:\n",
                "                self.arguments[sender].append(message.content)\n",
                "            \n",
                "            # Check if all debaters have submitted for this round\n",
                "            min_args = min(len(args) for args in self.arguments.values())\n",
                "            \n",
                "            if min_args > self.current_round:\n",
                "                self.current_round = min_args\n",
                "                \n",
                "                if self.current_round >= self.max_rounds:\n",
                "                    # End debate and declare winner\n",
                "                    return self._end_debate()\n",
                "                else:\n",
                "                    # Continue to next round\n",
                "                    return self._next_round()\n",
                "        \n",
                "        return []\n",
                "    \n",
                "    def _next_round(self) -> List[Message]:\n",
                "        \"\"\"Send prompts for next round.\"\"\"\n",
                "        messages = []\n",
                "        for i, debater in enumerate(self.debaters):\n",
                "            # Get opponent's last argument\n",
                "            opponent = self.debaters[(i + 1) % len(self.debaters)]\n",
                "            opponent_arg = self.arguments[opponent][-1] if self.arguments[opponent] else \"\"\n",
                "            \n",
                "            messages.append(self.send(\n",
                "                debater, \n",
                "                MessageType.TASK, \n",
                "                opponent_arg,\n",
                "                context={'topic': self.topic, 'round': self.current_round + 1}\n",
                "            ))\n",
                "        return messages\n",
                "    \n",
                "    def _end_debate(self) -> List[Message]:\n",
                "        \"\"\"Evaluate all arguments and declare winner.\"\"\"\n",
                "        # Build summary of debate\n",
                "        debate_summary = f\"DEBATE TOPIC: {self.topic}\\n\\n\"\n",
                "        \n",
                "        for round_num in range(self.max_rounds):\n",
                "            debate_summary += f\"--- Round {round_num + 1} ---\\n\"\n",
                "            for debater in self.debaters:\n",
                "                if round_num < len(self.arguments[debater]):\n",
                "                    debate_summary += f\"\\n{debater}:\\n{self.arguments[debater][round_num]}\\n\"\n",
                "            debate_summary += \"\\n\"\n",
                "        \n",
                "        # Use LLM to evaluate\n",
                "        evaluation_prompt = f\"\"\"Evaluate this debate and declare a winner.\n",
                "\n",
                "{debate_summary}\n",
                "\n",
                "Criteria:\n",
                "1. Strength of arguments (logic, evidence)\n",
                "2. Effectiveness of rebuttals\n",
                "3. Practical considerations\n",
                "\n",
                "Format your response as:\n",
                "ANALYSIS:\n",
                "[Your analysis of each side's arguments]\n",
                "\n",
                "WINNER: [Debater name]\n",
                "\n",
                "REASONING:\n",
                "[Why this side won]\"\"\"\n",
                "        \n",
                "        verdict = self.think(evaluation_prompt)\n",
                "        self.state['verdict'] = verdict\n",
                "        self.state['debate_summary'] = debate_summary\n",
                "        \n",
                "        return []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_debate(\n",
                "    topic: str,\n",
                "    position_a: str,\n",
                "    position_b: str,\n",
                "    llm: BaseLLMBackend,\n",
                "    max_rounds: int = 3,\n",
                "    verbose: bool = True\n",
                ") -> str:\n",
                "    \"\"\"\n",
                "    Run a debate between two LLM-powered agents.\n",
                "    \n",
                "    Args:\n",
                "        topic: The debate topic\n",
                "        position_a: First position to argue\n",
                "        position_b: Second position to argue\n",
                "        llm: LLM backend to use\n",
                "        max_rounds: Number of debate rounds\n",
                "        verbose: Whether to print progress\n",
                "    \n",
                "    Returns:\n",
                "        The moderator's verdict\n",
                "    \"\"\"\n",
                "    if verbose:\n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(f\"DEBATE: {topic}\")\n",
                "        print(f\"Position A: {position_a}\")\n",
                "        print(f\"Position B: {position_b}\")\n",
                "        print(f\"Rounds: {max_rounds}\")\n",
                "        print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    # Create debate orchestrator\n",
                "    debate = MultiAgentOrchestrator(max_iterations=max_rounds * 4 + 5)\n",
                "    \n",
                "    # Create agents\n",
                "    advocate_a = DebateAgent(\"Advocate-A\", position_a, llm)\n",
                "    advocate_b = DebateAgent(\"Advocate-B\", position_b, llm)\n",
                "    moderator = ModeratorAgent(llm, max_rounds=max_rounds)\n",
                "    \n",
                "    debate.register(advocate_a)\n",
                "    debate.register(advocate_b)\n",
                "    debate.register(moderator)\n",
                "    \n",
                "    # Initialize debate\n",
                "    moderator.start_debate(topic, [\"Advocate-A\", \"Advocate-B\"])\n",
                "    \n",
                "    # Send opening prompts\n",
                "    advocate_a.receive(Message(\n",
                "        \"Moderator\", \"Advocate-A\", MessageType.TASK, \n",
                "        \"Present your opening argument.\",\n",
                "        context={'topic': topic, 'round': 1}\n",
                "    ))\n",
                "    advocate_b.receive(Message(\n",
                "        \"Moderator\", \"Advocate-B\", MessageType.TASK,\n",
                "        \"Present your opening argument.\",\n",
                "        context={'topic': topic, 'round': 1}\n",
                "    ))\n",
                "    \n",
                "    # Run debate\n",
                "    for iteration in range(debate.max_iterations):\n",
                "        if verbose and iteration % 2 == 0:\n",
                "            current_round = min(\n",
                "                len(moderator.arguments.get(\"Advocate-A\", [])),\n",
                "                len(moderator.arguments.get(\"Advocate-B\", []))\n",
                "            ) + 1\n",
                "            if current_round <= max_rounds:\n",
                "                print(f\"Round {current_round}...\")\n",
                "        \n",
                "        any_activity = False\n",
                "        for agent in debate.agents.values():\n",
                "            if agent.inbox:\n",
                "                any_activity = True\n",
                "                agent.process()\n",
                "        \n",
                "        debate.deliver_messages()\n",
                "        \n",
                "        if not any_activity:\n",
                "            break\n",
                "    \n",
                "    return moderator.state.get('verdict', 'Debate incomplete')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run a debate!\n",
                "verdict = run_debate(\n",
                "    topic=\"Best backend language for a new startup\",\n",
                "    position_a=\"Python (Django/FastAPI)\",\n",
                "    position_b=\"Go (Gin/Echo)\",\n",
                "    llm=llm,\n",
                "    max_rounds=3,\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"VERDICT\")\n",
                "print(\"=\"*60)\n",
                "print(verdict)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 6：可视化\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.patches import FancyBboxPatch\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "def _edge_points(start, end, offset=0.75):\n",
                "    dx = end[0] - start[0]\n",
                "    dy = end[1] - start[1]\n",
                "    dist = max(np.hypot(dx, dy), 1e-6)\n",
                "    start_pt = (start[0] + dx / dist * offset, start[1] + dy / dist * offset)\n",
                "    end_pt = (end[0] - dx / dist * offset, end[1] - dy / dist * offset)\n",
                "    return start_pt, end_pt\n",
                "\n",
                "\n",
                "def _draw_node(ax, center, label, color, width=2.2, height=0.9):\n",
                "    x, y = center\n",
                "    rect = FancyBboxPatch(\n",
                "        (x - width / 2, y - height / 2),\n",
                "        width,\n",
                "        height,\n",
                "        boxstyle=\"round,pad=0.2,rounding_size=0.15\",\n",
                "        facecolor=color,\n",
                "        edgecolor=\"#111827\",\n",
                "        linewidth=1.5,\n",
                "    )\n",
                "    ax.add_patch(rect)\n",
                "    ax.text(\n",
                "        x,\n",
                "        y,\n",
                "        label,\n",
                "        ha=\"center\",\n",
                "        va=\"center\",\n",
                "        fontsize=10,\n",
                "        fontweight=\"bold\",\n",
                "        color=\"#111827\",\n",
                "    )\n",
                "\n",
                "\n",
                "def _draw_arrow(ax, start, end, color=\"#111827\", label=None, rad=0.0, lw=1.6):\n",
                "    s, e = _edge_points(start, end)\n",
                "    ax.annotate(\n",
                "        \"\",\n",
                "        xy=e,\n",
                "        xytext=s,\n",
                "        arrowprops=dict(\n",
                "            arrowstyle=\"-|>\",\n",
                "            color=color,\n",
                "            lw=lw,\n",
                "            connectionstyle=f\"arc3,rad={rad}\",\n",
                "        ),\n",
                "    )\n",
                "    if label:\n",
                "        mx, my = (s[0] + e[0]) / 2, (s[1] + e[1]) / 2\n",
                "        ax.text(\n",
                "            mx,\n",
                "            my,\n",
                "            label,\n",
                "            fontsize=8,\n",
                "            color=color,\n",
                "            ha=\"center\",\n",
                "            va=\"center\",\n",
                "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#FFFFFF\", ec=\"none\", alpha=0.9),\n",
                "        )\n",
                "\n",
                "\n",
                "def visualize_multi_agent_system():\n",
                "    plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
                "\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
                "    fig.patch.set_facecolor(\"#F8FAFC\")\n",
                "\n",
                "    # === Left: Pipeline ===\n",
                "    ax1 = axes[0]\n",
                "    ax1.set_xlim(0, 12)\n",
                "    ax1.set_ylim(0, 10)\n",
                "    ax1.set_aspect(\"equal\")\n",
                "    ax1.axis(\"off\")\n",
                "    ax1.set_title(\"Multi-Agent Pipeline (Software Delivery)\", fontsize=14, fontweight=\"bold\", pad=16)\n",
                "    ax1.set_facecolor(\"#F1F5F9\")\n",
                "\n",
                "    positions = {\n",
                "        \"User\": (6, 9),\n",
                "        \"Planner\": (6, 7),\n",
                "        \"Coder\": (3, 5),\n",
                "        \"Reviewer\": (6, 5),\n",
                "        \"Tester\": (9, 5),\n",
                "    }\n",
                "\n",
                "    colors = {\n",
                "        \"User\": \"#E2E8F0\",\n",
                "        \"Planner\": \"#DBEAFE\",\n",
                "        \"Coder\": \"#D1FAE5\",\n",
                "        \"Reviewer\": \"#FDE68A\",\n",
                "        \"Tester\": \"#FBCFE8\",\n",
                "    }\n",
                "\n",
                "    for name, pos in positions.items():\n",
                "        _draw_node(ax1, pos, name, colors[name])\n",
                "\n",
                "    main_flow = [\n",
                "        (\"User\", \"Planner\", \"1 Task\"),\n",
                "        (\"Planner\", \"Coder\", \"2 Plan\"),\n",
                "        (\"Coder\", \"Reviewer\", \"3 Code\"),\n",
                "        (\"Reviewer\", \"Tester\", \"4 Review\"),\n",
                "    ]\n",
                "\n",
                "    for src, dst, label in main_flow:\n",
                "        _draw_arrow(ax1, positions[src], positions[dst], color=\"#111827\", label=label)\n",
                "\n",
                "    _draw_arrow(\n",
                "        ax1,\n",
                "        positions[\"Tester\"],\n",
                "        positions[\"Planner\"],\n",
                "        color=\"#059669\",\n",
                "        label=\"5 Results\",\n",
                "        rad=-0.1,\n",
                "    )\n",
                "\n",
                "    _draw_arrow(\n",
                "        ax1,\n",
                "        positions[\"Reviewer\"],\n",
                "        positions[\"Coder\"],\n",
                "        color=\"#DC2626\",\n",
                "        label=\"Revision\",\n",
                "        rad=0.25,\n",
                "    )\n",
                "\n",
                "    # === Right: Debate ===\n",
                "    ax2 = axes[1]\n",
                "    ax2.set_xlim(0, 12)\n",
                "    ax2.set_ylim(0, 10)\n",
                "    ax2.set_aspect(\"equal\")\n",
                "    ax2.axis(\"off\")\n",
                "    ax2.set_title(\"Debate Pattern\", fontsize=14, fontweight=\"bold\", pad=16)\n",
                "    ax2.set_facecolor(\"#F1F5F9\")\n",
                "\n",
                "    debate_pos = {\n",
                "        \"Moderator\": (6, 8),\n",
                "        \"Advocate A\": (3, 5),\n",
                "        \"Advocate B\": (9, 5),\n",
                "        \"Verdict\": (6, 2),\n",
                "    }\n",
                "\n",
                "    debate_colors = {\n",
                "        \"Moderator\": \"#FEF3C7\",\n",
                "        \"Advocate A\": \"#BFDBFE\",\n",
                "        \"Advocate B\": \"#FDBA74\",\n",
                "        \"Verdict\": \"#BBF7D0\",\n",
                "    }\n",
                "\n",
                "    for name, pos in debate_pos.items():\n",
                "        _draw_node(ax2, pos, name, debate_colors[name], width=2.4)\n",
                "\n",
                "    _draw_arrow(ax2, debate_pos[\"Advocate A\"], debate_pos[\"Moderator\"], color=\"#2563EB\", label=\"Argument\")\n",
                "    _draw_arrow(ax2, debate_pos[\"Advocate B\"], debate_pos[\"Moderator\"], color=\"#2563EB\", label=\"Argument\")\n",
                "    _draw_arrow(ax2, debate_pos[\"Moderator\"], debate_pos[\"Advocate A\"], color=\"#6B7280\", label=\"Prompt\", rad=-0.2)\n",
                "    _draw_arrow(ax2, debate_pos[\"Moderator\"], debate_pos[\"Advocate B\"], color=\"#6B7280\", label=\"Prompt\", rad=0.2)\n",
                "    _draw_arrow(ax2, debate_pos[\"Moderator\"], debate_pos[\"Verdict\"], color=\"#16A34A\", label=\"Decision\")\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "visualize_multi_agent_system()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 总结\n",
                "\n",
                "### 我们构建了什么\n",
                "\n",
                "1. **真实的 LLM 驱动智能体**：每个智能体使用真实的 LLM 推理，而非模式匹配\n",
                "2. **软件开发流水线**：Planner -> Coder -> Reviewer -> Tester，并带反馈回路\n",
                "3. **辩论系统**：多视角观点争论，主持人客观评估\n",
                "\n",
                "### 关键模式\n",
                "\n",
                "| 模式 | 适用场景 | 智能体 |\n",
                "|---------|----------|--------|\n",
                "| **流水线** | 顺序处理 | Planner -> Coder -> Reviewer -> Tester |\n",
                "| **监督者** | 协同调度 | Planner 协调所有智能体 |\n",
                "| **辩论** | 多视角分析 | 辩手 + 主持人 |\n",
                "| **反馈回路** | 质量提升 | Reviewer -> Coder 修订 |\n",
                "\n",
                "### 生产环境注意事项\n",
                "\n",
                "- **持久化**：将智能体状态与消息存入数据库\n",
                "- **异步处理**：使用队列进行可扩展的消息传递\n",
                "- **错误处理**：智能体失败时优雅降级\n",
                "- **监控**：记录所有智能体交互用于调试\n",
                "- **成本控制**：为每个智能体设置 token 预算\n",
                "\n",
                "### 适用场景\n",
                "\n",
                "- **软件开发**：自动化代码生成与审查\n",
                "- **内容生产**：Research -> Write -> Edit 流水线\n",
                "- **决策支持**：多视角分析\n",
                "- **客户服务**：Triage -> Specialist -> Resolution\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
