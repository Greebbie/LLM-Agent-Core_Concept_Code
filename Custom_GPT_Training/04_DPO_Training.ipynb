{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "# 自建GPT训练流程 - Part 4: DPO偏好对齐\n",
                                        "\n",
                                        "## 概述\n",
                                        "\n",
                                        "本notebook演示如何使用**DPO (Direct Preference Optimization)** 对SFT模型进行偏好对齐，让模型学会生成人类更喜欢的回答。\n",
                                        "\n",
                                        "**设备建议：** CPU 可跑（训练时间更长），GPU 可加速。\n",
                                        "\n",
                                        "### DPO vs RLHF\n",
                                        "\n",
                                        "```\n",
                                        "传统RLHF:\n",
                                        "  SFT模型 → 训练Reward Model → PPO强化学习 → 对齐模型\n",
                                        "  (复杂、不稳定、计算量大)\n",
                                        "\n",
                                        "DPO:\n",
                                        "  SFT模型 → 直接用偏好数据优化 → 对齐模型\n",
                                        "  (简单、稳定、高效)\n",
                                        "```\n",
                                        "\n",
                                        "### DPO的核心思想\n",
                                        "\n",
                                        "给定同一个prompt的两个回答：\n",
                                        "- **Chosen (好回答)**: 人类更喜欢的回答\n",
                                        "- **Rejected (差回答)**: 人类不喜欢的回答\n",
                                        "\n",
                                        "DPO目标：增加chosen的概率，降低rejected的概率\n",
                                        "\n",
                                        "```\n",
                                        "Loss = -log(σ(β * (log π(chosen) - log π_ref(chosen)) \n",
                                        "              - β * (log π(rejected) - log π_ref(rejected))))\n",
                                        "```\n",
                                        "\n",
                                        "其中π是当前模型，π_ref是参考模型(通常是SFT模型)，β控制偏离程度"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 环境设置"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "from torch.utils.data import Dataset, DataLoader\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "from tqdm.auto import tqdm\n",
                                        "import os\n",
                                        "import sys\n",
                                        "import copy\n",
                                        "\n",
                                        "# 兼容从项目根目录或本目录运行\n",
                                        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\")) if os.path.basename(os.getcwd()) == \"Custom_GPT_Training\" else os.getcwd()\n",
                                        "CUSTOM_GPT_DIR = os.path.join(PROJECT_ROOT, \"Custom_GPT_Training\")\n",
                                        "sys.path.insert(0, CUSTOM_GPT_DIR)\n",
                                        "\n",
                                        "from custom_gpt import (\n",
                                        "    CustomGPT, \n",
                                        "    GPTConfig, \n",
                                        "    SimpleTokenizer,\n",
                                        "    count_parameters\n",
                                        ")\n",
                                        "\n",
                                        "# 设备选择\n",
                                        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                                        "print(f\"使用设备: {device}\")\n",
                                        "\n",
                                        "# 可复现性\n",
                                        "torch.manual_seed(42)\n",
                                        "np.random.seed(42)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 1：DPO 偏好数据\n",
                                        "\n",
                                        "本路线聚焦“短知识问答”，偏好对中 chosen 偏好结构化“要点”风格（定义+作用），rejected 为非结构化回答。\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# DPO偏好数据：从 data/ 读取（训练/验证/测试拆分）\n",
                                        "import json\n",
                                        "from pathlib import Path\n",
                                        "\n",
                                        "def resolve_data_dir():\n",
                                        "    candidates = [Path.cwd(), Path.cwd().parent]\n",
                                        "    for base in candidates:\n",
                                        "        data_dir = base / \"data\"\n",
                                        "        if data_dir.exists():\n",
                                        "            return str(data_dir)\n",
                                        "    return os.path.join(os.getcwd(), \"data\")\n",
                                        "\n",
                                        "DATA_DIR = resolve_data_dir()\n",
                                        "\n",
                                        "\n",
                                        "def load_jsonl(path):\n",
                                        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
                                        "        return [json.loads(line) for line in f if line.strip()]\n",
                                        "\n",
                                        "DPO_TRAIN = load_jsonl(os.path.join(DATA_DIR, \"custom_dpo_train.jsonl\"))\n",
                                        "DPO_VAL = load_jsonl(os.path.join(DATA_DIR, \"custom_dpo_val.jsonl\"))\n",
                                        "DPO_TEST = load_jsonl(os.path.join(DATA_DIR, \"custom_dpo_test.jsonl\"))\n",
                                        "\n",
                                        "print(f\"DPO训练集: {len(DPO_TRAIN)} 条 | 验证集: {len(DPO_VAL)} 条 | 测试集: {len(DPO_TEST)} 条\")\n",
                                        "print(f\"示例: {DPO_TRAIN[0]}\")\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 2: DPO数据集"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ChatML格式化器（复用）\n",
                                        "class ChatMLFormatter:\n",
                                        "    SYSTEM_PROMPT = \"你是一个有帮助的AI助手。\"\n",
                                        "    SYSTEM_TOKEN = \"<|system|>\"\n",
                                        "    USER_TOKEN = \"<|user|>\"\n",
                                        "    ASSISTANT_TOKEN = \"<|assistant|>\"\n",
                                        "    END_TOKEN = \"<|endoftext|>\"\n",
                                        "    \n",
                                        "    @classmethod\n",
                                        "    def format(cls, instruction, response, include_system=True):\n",
                                        "        parts = []\n",
                                        "        if include_system:\n",
                                        "            parts.append(f\"{cls.SYSTEM_TOKEN}{cls.SYSTEM_PROMPT}{cls.END_TOKEN}\")\n",
                                        "        parts.append(f\"{cls.USER_TOKEN}{instruction}{cls.END_TOKEN}\")\n",
                                        "        parts.append(f\"{cls.ASSISTANT_TOKEN}{response}{cls.END_TOKEN}\")\n",
                                        "        return \"\\n\".join(parts)\n",
                                        "    \n",
                                        "    @classmethod\n",
                                        "    def format_prompt_only(cls, instruction, include_system=True):\n",
                                        "        parts = []\n",
                                        "        if include_system:\n",
                                        "            parts.append(f\"{cls.SYSTEM_TOKEN}{cls.SYSTEM_PROMPT}{cls.END_TOKEN}\")\n",
                                        "        parts.append(f\"{cls.USER_TOKEN}{instruction}{cls.END_TOKEN}\")\n",
                                        "        parts.append(f\"{cls.ASSISTANT_TOKEN}\")\n",
                                        "        return \"\\n\".join(parts)"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class DPODataset(Dataset):\n",
                                        "    \"\"\"\n",
                                        "    DPO数据集\n",
                                        "    \n",
                                        "    返回:\n",
                                        "    - chosen_ids: 好回答的完整token序列\n",
                                        "    - rejected_ids: 差回答的完整token序列\n",
                                        "    - prompt_length: prompt部分的长度（用于只计算response部分的概率）\n",
                                        "    \"\"\"\n",
                                        "    \n",
                                        "    def __init__(self, data, tokenizer, max_length=256):\n",
                                        "        self.tokenizer = tokenizer\n",
                                        "        self.max_length = max_length\n",
                                        "        self.data = data\n",
                                        "        \n",
                                        "        # 字符级分词器会直接处理ChatML标记；仅在word模式时扩充词表\n",
                                        "        if getattr(tokenizer, \"mode\", \"char\") != \"char\" and hasattr(tokenizer, \"add_special_token\"):\n",
                                        "            special_tokens = [\n",
                                        "                ChatMLFormatter.SYSTEM_TOKEN,\n",
                                        "                ChatMLFormatter.USER_TOKEN,\n",
                                        "                ChatMLFormatter.ASSISTANT_TOKEN,\n",
                                        "                ChatMLFormatter.END_TOKEN\n",
                                        "            ]\n",
                                        "            for token in special_tokens:\n",
                                        "                if token not in tokenizer.token_to_id:\n",
                                        "                    tokenizer.add_special_token(token)\n",
                                        "        \n",
                                        "        print(f\"DPO数据集: {len(data)} 条\")\n",
                                        "    \n",
                                        "    def __len__(self):\n",
                                        "        return len(self.data)\n",
                                        "    \n",
                                        "    def __getitem__(self, idx):\n",
                                        "        item = self.data[idx]\n",
                                        "        \n",
                                        "        # 格式化\n",
                                        "        chosen_text = ChatMLFormatter.format(item['prompt'], item['chosen'])\n",
                                        "        rejected_text = ChatMLFormatter.format(item['prompt'], item['rejected'])\n",
                                        "        prompt_text = ChatMLFormatter.format_prompt_only(item['prompt'])\n",
                                        "        \n",
                                        "        # 编码\n",
                                        "        chosen_ids = self.tokenizer.encode(chosen_text, add_bos=True, add_eos=False)\n",
                                        "        rejected_ids = self.tokenizer.encode(rejected_text, add_bos=True, add_eos=False)\n",
                                        "        prompt_ids = self.tokenizer.encode(prompt_text, add_bos=True, add_eos=False)\n",
                                        "        \n",
                                        "        prompt_length = len(prompt_ids)\n",
                                        "        \n",
                                        "        # 截断\n",
                                        "        chosen_ids = chosen_ids[:self.max_length]\n",
                                        "        rejected_ids = rejected_ids[:self.max_length]\n",
                                        "        \n",
                                        "        # 填充到固定长度\n",
                                        "        max_len = self.max_length\n",
                                        "        prompt_length = min(prompt_length, len(chosen_ids), len(rejected_ids), max_len)\n",
                                        "        \n",
                                        "        chosen_mask = [1] * len(chosen_ids) + [0] * (max_len - len(chosen_ids))\n",
                                        "        rejected_mask = [1] * len(rejected_ids) + [0] * (max_len - len(rejected_ids))\n",
                                        "        \n",
                                        "        chosen_ids = chosen_ids + [self.tokenizer.pad_token_id] * (max_len - len(chosen_ids))\n",
                                        "        rejected_ids = rejected_ids + [self.tokenizer.pad_token_id] * (max_len - len(rejected_ids))\n",
                                        "        \n",
                                        "        return {\n",
                                        "            'chosen_ids': torch.tensor(chosen_ids, dtype=torch.long),\n",
                                        "            'rejected_ids': torch.tensor(rejected_ids, dtype=torch.long),\n",
                                        "            'chosen_mask': torch.tensor(chosen_mask, dtype=torch.float),\n",
                                        "            'rejected_mask': torch.tensor(rejected_mask, dtype=torch.float),\n",
                                        "            'prompt_length': prompt_length\n",
                                        "        }"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 加载tokenizer\n",
                                        "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \"models\", \"custom_gpt\")\n",
                                        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                                        "tokenizer_path = os.path.join(CHECKPOINT_DIR, \"tokenizer.pkl\")\n",
                                        "if os.path.exists(tokenizer_path):\n",
                                        "    tokenizer = SimpleTokenizer.load(tokenizer_path)\n",
                                        "    print(f\"加载tokenizer, 词表大小: {len(tokenizer)}\")\n",
                                        "else:\n",
                                        "    print(\"未找到预训练tokenizer，使用DPO数据构建新词表...\")\n",
                                        "    tokenizer = SimpleTokenizer(vocab_size=5000, mode=\"char\", min_freq=1)\n",
                                        "    tokenizer.build_vocab([\n",
                                        "        record[\"prompt\"] + \" \" + record[\"chosen\"] + \" \" + record[\"rejected\"]\n",
                                        "        for record in DPO_TRAIN\n",
                                        "    ])\n",
                                        "    tokenizer.save(tokenizer_path)\n",
                                        "    print(f\"新词表已保存, 词表大小: {len(tokenizer)}\")\n",
                                        "\n",
                                        "# 创建数据集\n",
                                        "MAX_LENGTH = 256\n",
                                        "BATCH_SIZE = 4\n",
                                        "\n",
                                        "train_dataset = DPODataset(DPO_TRAIN, tokenizer, max_length=MAX_LENGTH)\n",
                                        "val_dataset = DPODataset(DPO_VAL, tokenizer, max_length=MAX_LENGTH)\n",
                                        "\n",
                                        "print(f\"\\n训练集: {len(train_dataset)} 样本\")\n",
                                        "print(f\"验证集: {len(val_dataset)} 样本\")\n",
                                        "\n",
                                        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                                        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 3：DPO 训练器\n",
                                        "\n",
                                        "DPO的核心是计算：\n",
                                        "```\n",
                                        "Loss = -log(σ(β * (log_ratio_chosen - log_ratio_rejected)))\n",
                                        "\n",
                                        "其中:\n",
                                        "log_ratio = log π(response) - log π_ref(response)\n",
                                        "```\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class DPOTrainer:\n",
                                        "    \"\"\"\n",
                                        "    DPO Trainer\n",
                                        "    \n",
                                        "    实现Direct Preference Optimization算法:\n",
                                        "    1. 使用参考模型(SFT模型)计算基准概率\n",
                                        "    2. 训练模型增加chosen的相对概率，降低rejected的相对概率\n",
                                        "    3. β参数控制与参考模型的偏离程度\n",
                                        "    \"\"\"\n",
                                        "    \n",
                                        "    def __init__(\n",
                                        "        self,\n",
                                        "        model,\n",
                                        "        ref_model,\n",
                                        "        train_loader,\n",
                                        "        val_loader,\n",
                                        "        beta=0.1,  # DPO温度参数\n",
                                        "        lr=1e-5,   # DPO通常用更小的学习率\n",
                                        "        weight_decay=0.01,\n",
                                        "        max_grad_norm=1.0,\n",
                                        "        device='cpu'\n",
                                        "    ):\n",
                                        "        self.model = model.to(device)\n",
                                        "        self.ref_model = ref_model.to(device)\n",
                                        "        self.ref_model.eval()  # 参考模型不训练\n",
                                        "        \n",
                                        "        # 冻结参考模型\n",
                                        "        for param in self.ref_model.parameters():\n",
                                        "            param.requires_grad = False\n",
                                        "        \n",
                                        "        self.train_loader = train_loader\n",
                                        "        self.val_loader = val_loader\n",
                                        "        self.beta = beta\n",
                                        "        self.device = device\n",
                                        "        self.max_grad_norm = max_grad_norm\n",
                                        "        \n",
                                        "        # 优化器\n",
                                        "        self.optimizer = torch.optim.AdamW(\n",
                                        "            model.parameters(),\n",
                                        "            lr=lr,\n",
                                        "            weight_decay=weight_decay\n",
                                        "        )\n",
                                        "        \n",
                                        "        # 训练历史\n",
                                        "        self.train_losses = []\n",
                                        "        self.val_losses = []\n",
                                        "        self.chosen_rewards = []\n",
                                        "        self.rejected_rewards = []\n",
                                        "    \n",
                                        "    def compute_log_probs(self, model, input_ids, attention_mask, prompt_length):\n",
                                        "        \"\"\"\n",
                                        "        计算response部分的log概率\n",
                                        "        \n",
                                        "        只计算prompt之后的token的概率（response部分）\n",
                                        "        \"\"\"\n",
                                        "        with torch.set_grad_enabled(model.training):\n",
                                        "            outputs = model(input_ids[:, :-1], attention_mask=attention_mask[:, :-1])\n",
                                        "            logits = outputs['logits']\n",
                                        "        \n",
                                        "        # 计算log概率\n",
                                        "        log_probs = F.log_softmax(logits, dim=-1)\n",
                                        "        \n",
                                        "        # 取每个位置预测的下一个token的log概率\n",
                                        "        labels = input_ids[:, 1:]  # 右移一位作为标签\n",
                                        "        per_token_log_probs = torch.gather(log_probs, dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
                                        "        \n",
                                        "        # 创建response mask（只考虑prompt之后的token）\n",
                                        "        response_mask = attention_mask[:, 1:].clone()\n",
                                        "        batch_size, seq_len = response_mask.shape\n",
                                        "        for i in range(batch_size):\n",
                                        "            # prompt_length[i] - 1 因为labels右移了一位\n",
                                        "            start = max(int(prompt_length[i].item()) - 1, 0)\n",
                                        "            start = min(start, seq_len)\n",
                                        "            if start > 0:\n",
                                        "                response_mask[i, :start] = 0\n",
                                        "        \n",
                                        "        # 对response部分求和\n",
                                        "        response_log_probs = (per_token_log_probs * response_mask).sum(dim=-1)\n",
                                        "        \n",
                                        "        return response_log_probs\n",
                                        "    \n",
                                        "    def compute_dpo_loss(self, batch):\n",
                                        "        \"\"\"\n",
                                        "        计算DPO loss\n",
                                        "        \n",
                                        "        Loss = -log(σ(β * (log_ratio_chosen - log_ratio_rejected)))\n",
                                        "        \"\"\"\n",
                                        "        chosen_ids = batch['chosen_ids'].to(self.device)\n",
                                        "        rejected_ids = batch['rejected_ids'].to(self.device)\n",
                                        "        chosen_mask = batch['chosen_mask'].to(self.device)\n",
                                        "        rejected_mask = batch['rejected_mask'].to(self.device)\n",
                                        "        prompt_length = batch['prompt_length'].to(self.device)\n",
                                        "        \n",
                                        "        # 当前模型的log概率\n",
                                        "        policy_chosen_logps = self.compute_log_probs(\n",
                                        "            self.model, chosen_ids, chosen_mask, prompt_length\n",
                                        "        )\n",
                                        "        policy_rejected_logps = self.compute_log_probs(\n",
                                        "            self.model, rejected_ids, rejected_mask, prompt_length\n",
                                        "        )\n",
                                        "        \n",
                                        "        # 参考模型的log概率\n",
                                        "        with torch.no_grad():\n",
                                        "            ref_chosen_logps = self.compute_log_probs(\n",
                                        "                self.ref_model, chosen_ids, chosen_mask, prompt_length\n",
                                        "            )\n",
                                        "            ref_rejected_logps = self.compute_log_probs(\n",
                                        "                self.ref_model, rejected_ids, rejected_mask, prompt_length\n",
                                        "            )\n",
                                        "        \n",
                                        "        # 计算log ratio\n",
                                        "        chosen_log_ratio = policy_chosen_logps - ref_chosen_logps\n",
                                        "        rejected_log_ratio = policy_rejected_logps - ref_rejected_logps\n",
                                        "        \n",
                                        "        # DPO loss\n",
                                        "        logits = self.beta * (chosen_log_ratio - rejected_log_ratio)\n",
                                        "        loss = -F.logsigmoid(logits).mean()\n",
                                        "        \n",
                                        "        # 记录reward（用于监控）\n",
                                        "        chosen_reward = self.beta * chosen_log_ratio.detach().mean().item()\n",
                                        "        rejected_reward = self.beta * rejected_log_ratio.detach().mean().item()\n",
                                        "        \n",
                                        "        return loss, chosen_reward, rejected_reward\n",
                                        "    \n",
                                        "    def train_epoch(self, epoch, total_epochs):\n",
                                        "        \"\"\"训练一个epoch\"\"\"\n",
                                        "        self.model.train()\n",
                                        "        total_loss = 0\n",
                                        "        total_chosen_reward = 0\n",
                                        "        total_rejected_reward = 0\n",
                                        "        \n",
                                        "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
                                        "        for batch in pbar:\n",
                                        "            loss, chosen_reward, rejected_reward = self.compute_dpo_loss(batch)\n",
                                        "            \n",
                                        "            self.optimizer.zero_grad()\n",
                                        "            loss.backward()\n",
                                        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
                                        "            self.optimizer.step()\n",
                                        "            \n",
                                        "            total_loss += loss.item()\n",
                                        "            total_chosen_reward += chosen_reward\n",
                                        "            total_rejected_reward += rejected_reward\n",
                                        "            \n",
                                        "            self.train_losses.append(loss.item())\n",
                                        "            self.chosen_rewards.append(chosen_reward)\n",
                                        "            self.rejected_rewards.append(rejected_reward)\n",
                                        "            \n",
                                        "            pbar.set_postfix({\n",
                                        "                'loss': f\"{loss.item():.4f}\",\n",
                                        "                'reward_margin': f\"{chosen_reward - rejected_reward:.4f}\"\n",
                                        "            })\n",
                                        "        \n",
                                        "        n = len(self.train_loader)\n",
                                        "        return total_loss/n, total_chosen_reward/n, total_rejected_reward/n\n",
                                        "    \n",
                                        "    @torch.no_grad()\n",
                                        "    def evaluate(self):\n",
                                        "        \"\"\"验证集评估\"\"\"\n",
                                        "        self.model.eval()\n",
                                        "        total_loss = 0\n",
                                        "        total_chosen_reward = 0\n",
                                        "        total_rejected_reward = 0\n",
                                        "        \n",
                                        "        for batch in self.val_loader:\n",
                                        "            loss, chosen_reward, rejected_reward = self.compute_dpo_loss(batch)\n",
                                        "            total_loss += loss.item()\n",
                                        "            total_chosen_reward += chosen_reward\n",
                                        "            total_rejected_reward += rejected_reward\n",
                                        "        \n",
                                        "        n = len(self.val_loader)\n",
                                        "        avg_loss = total_loss / n\n",
                                        "        self.val_losses.append(avg_loss)\n",
                                        "        \n",
                                        "        return avg_loss, total_chosen_reward/n, total_rejected_reward/n\n",
                                        "    \n",
                                        "    def train(self, epochs, save_dir=None):\n",
                                        "        \"\"\"完整训练流程\"\"\"\n",
                                        "        print(f\"\\n开始DPO训练\")\n",
                                        "        print(f\"{'='*50}\")\n",
                                        "        print(f\"  模型参数: {count_parameters(self.model)}\")\n",
                                        "        print(f\"  Beta: {self.beta}\")\n",
                                        "        print(f\"  训练样本: {len(self.train_loader.dataset)}\")\n",
                                        "        print(f\"  Epochs: {epochs}\")\n",
                                        "        print(f\"{'='*50}\\n\")\n",
                                        "        \n",
                                        "        best_val_loss = float('inf')\n",
                                        "        \n",
                                        "        for epoch in range(epochs):\n",
                                        "            train_loss, train_chosen, train_rejected = self.train_epoch(epoch, epochs)\n",
                                        "            val_loss, val_chosen, val_rejected = self.evaluate()\n",
                                        "            \n",
                                        "            print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
                                        "            print(f\"  训练Loss: {train_loss:.4f}\")\n",
                                        "            print(f\"  验证Loss: {val_loss:.4f}\")\n",
                                        "            print(f\"  训练Reward边际: {train_chosen - train_rejected:.4f}\")\n",
                                        "            print(f\"  验证Reward边际: {val_chosen - val_rejected:.4f}\")\n",
                                        "            \n",
                                        "            if save_dir and val_loss < best_val_loss:\n",
                                        "                best_val_loss = val_loss\n",
                                        "                self.model.save_pretrained(os.path.join(save_dir, \"dpo_model\"))\n",
                                        "                print(f\"  ✓ 保存最佳模型\")\n",
                                        "        \n",
                                        "        return {\n",
                                        "            'train_losses': self.train_losses,\n",
                                        "            'val_losses': self.val_losses,\n",
                                        "            'chosen_rewards': self.chosen_rewards,\n",
                                        "            'rejected_rewards': self.rejected_rewards\n",
                                        "        }"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 4: 执行DPO训练"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 加载SFT模型作为起点和参考模型\n",
                                        "sft_path = os.path.join(CHECKPOINT_DIR, \"sft_model\")\n",
                                        "\n",
                                        "if os.path.exists(sft_path):\n",
                                        "    print(\"加载SFT模型...\")\n",
                                        "    model = CustomGPT.from_pretrained(sft_path)\n",
                                        "    ref_model = CustomGPT.from_pretrained(sft_path)  # 参考模型（不训练）\n",
                                        "else:\n",
                                        "    print(\"SFT模型不存在，创建新模型...\")\n",
                                        "    config = GPTConfig(\n",
                                        "        vocab_size=len(tokenizer),\n",
                                        "        max_seq_len=MAX_LENGTH,\n",
                                        "        d_model=384,\n",
                                        "        n_heads=6,\n",
                                        "        n_layers=6,\n",
                                        "        d_ff=1536\n",
                                        "    )\n",
                                        "    model = CustomGPT(config)\n",
                                        "    ref_model = CustomGPT(config)\n",
                                        "    ref_model.load_state_dict(copy.deepcopy(model.state_dict()))\n",
                                        "\n",
                                        "print(f\"模型参数: {count_parameters(model)}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 创建DPO Trainer\n",
                                        "trainer = DPOTrainer(\n",
                                        "    model=model,\n",
                                        "    ref_model=ref_model,\n",
                                        "    train_loader=train_loader,\n",
                                        "    val_loader=val_loader,\n",
                                        "    beta=0.1,    # DPO温度\n",
                                        "    lr=1e-5,     # 小学习率\n",
                                        "    weight_decay=0.01,\n",
                                        "    device=device\n",
                                        ")\n",
                                        "\n",
                                        "# 执行训练\n",
                                        "EPOCHS = 2\n",
                                        "history = trainer.train(epochs=EPOCHS, save_dir=CHECKPOINT_DIR)"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 可视化训练过程\n",
                                        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                                        "\n",
                                        "# DPO Loss\n",
                                        "axes[0].plot(history['train_losses'], 'b-', alpha=0.5)\n",
                                        "window = 10\n",
                                        "if len(history['train_losses']) > window:\n",
                                        "    smoothed = np.convolve(history['train_losses'], np.ones(window)/window, mode='valid')\n",
                                        "    axes[0].plot(range(window-1, len(history['train_losses'])), smoothed, 'r-', linewidth=2)\n",
                                        "axes[0].set_xlabel('Step')\n",
                                        "axes[0].set_ylabel('Loss')\n",
                                        "axes[0].set_title('DPO training Loss')\n",
                                        "axes[0].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "# Reward对比\n",
                                        "axes[1].plot(history['chosen_rewards'], 'g-', alpha=0.5, label='Chosen')\n",
                                        "axes[1].plot(history['rejected_rewards'], 'r-', alpha=0.5, label='Rejected')\n",
                                        "axes[1].set_xlabel('Step')\n",
                                        "axes[1].set_ylabel('Reward')\n",
                                        "axes[1].set_title('Chosen vs Rejected Reward')\n",
                                        "axes[1].legend()\n",
                                        "axes[1].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "# Reward边际\n",
                                        "margin = [c - r for c, r in zip(history['chosen_rewards'], history['rejected_rewards'])]\n",
                                        "axes[2].plot(margin, 'purple', alpha=0.5)\n",
                                        "if len(margin) > window:\n",
                                        "    smoothed = np.convolve(margin, np.ones(window)/window, mode='valid')\n",
                                        "    axes[2].plot(range(window-1, len(margin)), smoothed, 'purple', linewidth=2)\n",
                                        "axes[2].axhline(y=0, color='gray', linestyle='--')\n",
                                        "axes[2].set_xlabel('Step')\n",
                                        "axes[2].set_ylabel('Margin')\n",
                                        "axes[2].set_title('Reward Margin (Chosen - Rejected)')\n",
                                        "axes[2].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(f\"\\n最终Reward边际: {margin[-1]:.4f}\")\n",
                                        "print(\"(正值表示模型更偏好chosen回答)\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 5: 测试DPO模型"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def chat(model, tokenizer, instruction, max_new_tokens=100, temperature=0.7):\n",
                                        "    \"\"\"与模型对话\"\"\"\n",
                                        "    model.eval()\n",
                                        "    \n",
                                        "    prompt = ChatMLFormatter.format_prompt_only(instruction)\n",
                                        "    input_ids = torch.tensor([tokenizer.encode(prompt, add_bos=True, add_eos=False)]).to(device)\n",
                                        "    \n",
                                        "    with torch.no_grad():\n",
                                        "        output_ids = model.generate(\n",
                                        "            input_ids,\n",
                                        "            max_new_tokens=max_new_tokens,\n",
                                        "            temperature=temperature,\n",
                                        "            top_k=50,\n",
                                        "            do_sample=True\n",
                                        "        )\n",
                                        "    \n",
                                        "    full_response = tokenizer.decode(output_ids[0].tolist())\n",
                                        "    \n",
                                        "    if ChatMLFormatter.ASSISTANT_TOKEN in full_response:\n",
                                        "        response = full_response.split(ChatMLFormatter.ASSISTANT_TOKEN)[-1]\n",
                                        "        if ChatMLFormatter.END_TOKEN in response:\n",
                                        "            response = response.split(ChatMLFormatter.END_TOKEN)[0]\n",
                                        "    else:\n",
                                        "        response = full_response\n",
                                        "    \n",
                                        "    return response.strip()"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# 加载DPO模型\n",
                                        "dpo_model = CustomGPT.from_pretrained(\n",
                                        "    os.path.join(CHECKPOINT_DIR, \"dpo_model\")\n",
                                        ").to(device)\n",
                                        "\n",
                                        "# 加载SFT模型用于对比\n",
                                        "sft_model = CustomGPT.from_pretrained(\n",
                                        "    os.path.join(CHECKPOINT_DIR, \"sft_model\")\n",
                                        ").to(device)\n",
                                        "\n",
                                        "print(\"模型加载完成\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# SFT vs DPO 对比\n",
                                        "test_items = DPO_TEST[:3]\n",
                                        "\n",
                                        "print(\"SFT模型 vs DPO模型 对比(偏好任务):\")\n",
                                        "print(\"=\" * 70)\n",
                                        "\n",
                                        "for item in test_items:\n",
                                        "    prompt = item[\"prompt\"]\n",
                                        "    chosen = item[\"chosen\"]\n",
                                        "    rejected = item[\"rejected\"]\n",
                                        "\n",
                                        "    sft_response = chat(sft_model, tokenizer, prompt)\n",
                                        "    dpo_response = chat(dpo_model, tokenizer, prompt)\n",
                                        "\n",
                                        "    print(f\"\\n问题: {prompt}\")\n",
                                        "    print(f\"Chosen: {chosen}\")\n",
                                        "    print(f\"Rejected: {rejected}\")\n",
                                        "\n",
                                        "    print(f\"\\n[SFT模型]:\")\n",
                                        "    print(f\"{sft_response}\")\n",
                                        "\n",
                                        "    print(f\"\\n[DPO模型]:\")\n",
                                        "    print(f\"{dpo_response}\")\n",
                                        "    print(\"\\n\" + \"=\" * 70)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "### 更直观的比较结果可参考Ch10_DPO章节"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 总结\n",
                                        "\n",
                                        "### 本notebook完成的内容\n",
                                        "\n",
                                        "1. **DPO偏好数据**\n",
                                        "   - (prompt, chosen, rejected)三元组\n",
                                        "   - chosen是人类偏好的回答\n",
                                        "   - rejected是不偏好的回答\n",
                                        "\n",
                                        "2. **DPO算法实现**\n",
                                        "   - 使用参考模型计算基准概率\n",
                                        "   - 优化log ratio差异\n",
                                        "   - β参数控制偏离程度\n",
                                        "\n",
                                        "3. **训练监控**\n",
                                        "   - DPO Loss\n",
                                        "   - Chosen/Rejected Reward\n",
                                        "   - Reward边际（正值表示偏好正确）\n",
                                        "\n",
                                        "### DPO vs RLHF\n",
                                        "\n",
                                        "| 方面 | RLHF | DPO |\n",
                                        "|------|------|-----|\n",
                                        "| 复杂度 | 需要Reward Model + PPO | 直接优化 |\n",
                                        "| 稳定性 | PPO难调参 | 更稳定 |\n",
                                        "| 计算量 | 大 | 小 |\n",
                                        "| 效果 | 成熟 | 接近 |\n",
                                        "\n",
                                        "### 下一步: 评估对比 (05_Evaluation.ipynb)\n",
                                        "\n",
                                        "在下一个notebook中，我们将系统地评估和对比：\n",
                                        "- Base模型（预训练后）\n",
                                        "- SFT模型\n",
                                        "- DPO模型\n",
                                        "\n",
                                        "通过多个维度（困惑度、生成质量、偏好一致性）进行定量和定性分析。"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 4
}
