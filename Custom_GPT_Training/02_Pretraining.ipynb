{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自建GPT训练流程 - Part 2: 预训练 (Pretraining)\n",
    "\n",
    "## 概述\n",
    "\n",
    "本notebook演示如何对我们的小型GPT进行**预训练**，使用经典的**Next-Token Prediction**任务。\n",
    "\n",
    "**设备建议：** CPU 可跑（GPU 可加速）。\n",
    "\n",
    "### 预训练的核心思想\n",
    "\n",
    "```\n",
    "输入序列: [The, quick, brown, fox, jumps]\n",
    "                ↓      ↓      ↓     ↓      ↓\n",
    "预测目标: [quick, brown,  fox, jumps, over]\n",
    "\n",
    "模型学习: P(next_token | previous_tokens)\n",
    "```\n",
    "\n",
    "### 预训练 vs SFT vs DPO\n",
    "\n",
    "| 阶段 | 目标 | 数据 | 结果 |\n",
    "|------|------|------|------|\n",
    "| **预训练** | 学习语言规律 | 大量原始文本 | 会\"说话\"的模型 |\n",
    "| **SFT** | 学习遵循指令 | (指令, 回答)对 | 会\"听话\"的模型 |\n",
    "| **DPO** | 学习人类偏好 | (好回答, 差回答)对 | 会\"选择\"的模型 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预训练关键机制（对齐主章 Ch6-Ch7）\n",
    "\n",
    "- **数据 -> Token**：文本被分词器切成 token，再拼成固定 block_size。\n",
    "- **标签右移**：input_ids[:-1] 预测 input_ids[1:]，即 Teacher Forcing。\n",
    "- **因果掩码**：仅允许关注历史 token，避免信息泄露。\n",
    "- **损失与评估**：交叉熵损失，常用指标为 Loss/Perplexity。\n",
    "- **训练稳定性**：Warmup + Cosine、梯度裁剪，避免小模型发散。\n",
    "\n",
    "> 本路线强调“从零可控”的完整链路，不追求 SOTA；主章 GPT-2 仅作工程演示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 兼容从项目根目录或本目录运行\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\")) if os.path.basename(os.getcwd()) == \"Custom_GPT_Training\" else os.getcwd()\n",
    "CUSTOM_GPT_DIR = os.path.join(PROJECT_ROOT, \"Custom_GPT_Training\")\n",
    "sys.path.insert(0, CUSTOM_GPT_DIR)\n",
    "\n",
    "from custom_gpt import (\n",
    "    CustomGPT, \n",
    "    GPTConfig, \n",
    "    SimpleTokenizer,\n",
    "    count_parameters\n",
    ")\n",
    "\n",
    "# 设备选择\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 可复现性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 准备预训练数据\n",
    "\n",
    "预训练需要大量的通用文本数据。这里读取 `data/custom_pretrain_corpus.txt` 作为示例语料，\n",
    "内容保持客观、通用，并与后续的 SFT/DPO 任务尽量区分，以便更清晰地观察微调效果。\n",
    "\n",
    "在实际应用中，可以替换为更大规模语料，例如：\n",
    "- Wikipedia dump\n",
    "- Common Crawl\n",
    "- Books corpus\n",
    "- GitHub code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例预训练语料库（实际应用中会大得多）\n",
    "# 读取 data/custom_pretrain_corpus.txt（通用中文语料，避免与后续SFT/DPO任务重合）\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_data_dir():\n",
    "    candidates = [Path.cwd(), Path.cwd().parent]\n",
    "    for base in candidates:\n",
    "        data_dir = base / \"data\"\n",
    "        if data_dir.exists():\n",
    "            return str(data_dir)\n",
    "    return os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "DATA_DIR = resolve_data_dir()\n",
    "PRETRAIN_PATH = os.path.join(DATA_DIR, \"custom_pretrain_corpus.txt\")\n",
    "\n",
    "if os.path.exists(PRETRAIN_PATH):\n",
    "    with open(PRETRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        PRETRAIN_CORPUS = [line.strip() for line in f if line.strip()]\n",
    "else:\n",
    "    PRETRAIN_CORPUS = [\n",
    "        \"预训练语料文件不存在，使用内置示例文本。\",\n",
    "        \"建议在 data/custom_pretrain_corpus.txt 中提供更丰富的中文语料。\",\n",
    "        \"示例配置: {\\\"version\\\":\\\"1.0\\\",\\\"status\\\":\\\"ok\\\",\\\"retry\\\":3}。\",\n",
    "    ]\n",
    "\n",
    "# 扩展语料（重复）以保证演示训练步数充足\n",
    "target_size = 2000\n",
    "if len(PRETRAIN_CORPUS) < target_size:\n",
    "    repeat = target_size // len(PRETRAIN_CORPUS) + 1\n",
    "    PRETRAIN_CORPUS = (PRETRAIN_CORPUS * repeat)[:target_size]\n",
    "\n",
    "print(f\"预训练语料: {len(PRETRAIN_CORPUS)} 条文本\")\n",
    "print(f\"总字符数: {sum(len(t) for t in PRETRAIN_CORPUS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 构建预训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    预训练数据集\n",
    "    \n",
    "    将文本转换为固定长度的训练样本，用于next-token prediction。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "            tokenizer: 分词器\n",
    "            max_length: 最大序列长度\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 将所有文本编码并拼接成一个长序列\n",
    "        all_ids = []\n",
    "        for text in texts:\n",
    "            ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "            all_ids.extend(ids)\n",
    "        \n",
    "        # 切分成训练样本\n",
    "        self.samples = []\n",
    "        for i in range(0, len(all_ids) - max_length, max_length // 2):  # 50% overlap\n",
    "            sample = all_ids[i:i + max_length + 1]  # +1 for labels\n",
    "            if len(sample) == max_length + 1:\n",
    "                self.samples.append(sample)\n",
    "        \n",
    "        print(f\"创建了 {len(self.samples)} 个训练样本\")\n",
    "        print(f\"每个样本长度: {max_length}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # input_ids: [0, 1, 2, ..., n-1]\n",
    "        # labels: [1, 2, 3, ..., n]\n",
    "        input_ids = torch.tensor(sample[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(sample[1:], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建或加载分词器\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \"models\", \"custom_gpt\")\n",
    "tokenizer_path = os.path.join(CHECKPOINT_DIR, \"tokenizer.pkl\")\n",
    "\n",
    "# 为ChatML保留必要字符，保证后续SFT/DPO可用\n",
    "CHATML_SEED = [\n",
    "    \"<|system|>你是一个有帮助的AI助手。<|endoftext|>\",\n",
    "    \"<|user|>示例问题<|endoftext|>\",\n",
    "    \"<|assistant|>示例回答<|endoftext|>\",\n",
    "]\n",
    "PRETRAIN_CORPUS = PRETRAIN_CORPUS + CHATML_SEED\n",
    "\n",
    "REBUILD_TOKENIZER = True\n",
    "\n",
    "if os.path.exists(tokenizer_path) and not REBUILD_TOKENIZER:\n",
    "    print(\"加载已有分词器...\")\n",
    "    tokenizer = SimpleTokenizer.load(tokenizer_path)\n",
    "else:\n",
    "    print(\"构建新分词器...\")\n",
    "    tokenizer = SimpleTokenizer(vocab_size=5000, mode=\"char\", min_freq=1)  # 中文用字符级\n",
    "    tokenizer.build_vocab(PRETRAIN_CORPUS)\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    tokenizer.save(tokenizer_path)\n",
    "\n",
    "print(f\"词表大小: {len(tokenizer)}\")\n",
    "\n",
    "# 测试分词\n",
    "test_text = \"深度学习是人工智能的核心技术。\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\n测试分词:\")\n",
    "print(f\"  原文: {test_text}\")\n",
    "print(f\"  编码: {encoded[:20]}...\")\n",
    "print(f\"  解码: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset = PretrainDataset(PRETRAIN_CORPUS, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "# 划分训练/验证集 (90/10)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"\\n训练集: {len(train_dataset)} 样本\")\n",
    "print(f\"验证集: {len(val_dataset)} 样本\")\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 查看一个batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\n样本batch:\")\n",
    "print(f\"  input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"  labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 预训练Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainTrainer:\n",
    "    \"\"\"\n",
    "    预训练Trainer\n",
    "    \n",
    "    功能:\n",
    "    - 标准的next-token prediction训练\n",
    "    - 学习率warmup和cosine decay\n",
    "    - 梯度裁剪\n",
    "    - 验证集评估\n",
    "    - checkpoint保存\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,\n",
    "        max_grad_norm=1.0,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.base_lr = lr\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=(0.9, 0.95)\n",
    "        )\n",
    "        \n",
    "        # 训练历史\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.lrs = []\n",
    "        self.global_step = 0\n",
    "    \n",
    "    def get_lr(self, step, total_steps):\n",
    "        \"\"\"学习率调度: warmup + cosine decay\"\"\"\n",
    "        if step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            return self.base_lr * step / self.warmup_steps\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (step - self.warmup_steps) / (total_steps - self.warmup_steps)\n",
    "            return self.base_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    def train_epoch(self, epoch, total_epochs):\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_steps = len(self.train_loader) * total_epochs\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "        for batch in pbar:\n",
    "            # 更新学习率\n",
    "            lr = self.get_lr(self.global_step, total_steps)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            self.lrs.append(lr)\n",
    "            \n",
    "            # 准备数据\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = self.model(input_ids, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "            \n",
    "            # 更新参数\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # 记录\n",
    "            total_loss += loss.item()\n",
    "            self.train_losses.append(loss.item())\n",
    "            self.global_step += 1\n",
    "            \n",
    "            # 更新进度条\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'lr': f\"{lr:.2e}\"\n",
    "            })\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        \"\"\"验证集评估\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in self.val_loader:\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_ids, labels=labels)\n",
    "            total_loss += outputs['loss'].item()\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        self.val_losses.append(avg_loss)\n",
    "        \n",
    "        # 计算困惑度\n",
    "        perplexity = np.exp(avg_loss)\n",
    "        \n",
    "        return avg_loss, perplexity\n",
    "    \n",
    "    def train(self, epochs, save_dir=None):\n",
    "        \"\"\"完整训练流程\"\"\"\n",
    "        print(f\"\\n开始预训练\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"  模型参数: {count_parameters(self.model)}\")\n",
    "        print(f\"  训练样本: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"  验证样本: {len(self.val_loader.dataset)}\")\n",
    "        print(f\"  Epochs: {epochs}\")\n",
    "        print(f\"  设备: {self.device}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 训练\n",
    "            train_loss = self.train_epoch(epoch, epochs)\n",
    "            \n",
    "            # 验证\n",
    "            val_loss, perplexity = self.evaluate()\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"  训练Loss: {train_loss:.4f}\")\n",
    "            print(f\"  验证Loss: {val_loss:.4f}\")\n",
    "            print(f\"  困惑度: {perplexity:.2f}\")\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if save_dir and val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.model.save_pretrained(os.path.join(save_dir, \"pretrained_model\"))\n",
    "                print(f\"  ✓ 保存最佳模型 (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        return {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.lrs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 执行预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型配置\n",
    "config = GPTConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_seq_len=MAX_LENGTH,\n",
    "    d_model=384,\n",
    "    n_heads=6,\n",
    "    n_layers=6,\n",
    "    d_ff=1536,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"模型配置:\")\n",
    "print(f\"  vocab_size: {config.vocab_size}\")\n",
    "print(f\"  d_model: {config.d_model}\")\n",
    "print(f\"  n_layers: {config.n_layers}\")\n",
    "print(f\"  n_heads: {config.n_heads}\")\n",
    "print(f\"  预估参数量: ~{config.num_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "model = CustomGPT(config)\n",
    "print(f\"实际参数量: {count_parameters(model)}\")\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = PretrainTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    max_grad_norm=1.0,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行预训练\n",
    "EPOCHS = 10  # 实际应用中需要更多epochs\n",
    "\n",
    "history = trainer.train(\n",
    "    epochs=EPOCHS,\n",
    "    save_dir=CHECKPOINT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 训练Loss\n",
    "axes[0].plot(history['train_losses'], 'b-', alpha=0.7, linewidth=0.5)\n",
    "# 平滑曲线\n",
    "window = 50\n",
    "if len(history['train_losses']) > window:\n",
    "    smoothed = np.convolve(history['train_losses'], np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(range(window-1, len(history['train_losses'])), smoothed, 'r-', linewidth=2, label='平滑')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 验证Loss\n",
    "axes[1].plot(history['val_losses'], 'g-o', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Val Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 学习率\n",
    "axes[2].plot(history['learning_rates'], 'purple', linewidth=1)\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Lr (Warmup + Cosine)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 测试预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "pretrained_model = CustomGPT.from_pretrained(\n",
    "    os.path.join(CHECKPOINT_DIR, \"pretrained_model\")\n",
    ").to(device)\n",
    "\n",
    "print(f\"加载预训练模型: {count_parameters(pretrained_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.8, top_k=50):\n",
    "    \"\"\"生成文本\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 编码prompt\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt, add_bos=True, add_eos=False)]).to(device)\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # 解码\n",
    "    generated = tokenizer.decode(output_ids[0].tolist())\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试生成\n",
    "test_prompts = [\n",
    "    \"深度学习\",\n",
    "    \"机器学习是\",\n",
    "    \"Transformer架构\",\n",
    "    \"语言模型\",\n",
    "]\n",
    "\n",
    "print(\"预训练模型生成测试:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(\n",
    "        pretrained_model, \n",
    "        tokenizer, \n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"生成: {generated}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同温度对比\n",
    "prompt = \"机器学习\"\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\n不同温度的生成效果:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(\n",
    "        pretrained_model, \n",
    "        tokenizer, \n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        temperature=temp\n",
    "    )\n",
    "    print(f\"\\n温度={temp}: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 计算困惑度 (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_perplexity(model, dataloader, device):\n",
    "    \"\"\"计算模型在数据集上的困惑度\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        total_loss += outputs['loss'].item() * input_ids.numel()\n",
    "        total_tokens += input_ids.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# 计算困惑度\n",
    "ppl, loss = compute_perplexity(pretrained_model, val_loader, device)\n",
    "print(f\"预训练模型评估:\")\n",
    "print(f\"  验证Loss: {loss:.4f}\")\n",
    "print(f\"  困惑度 (Perplexity): {ppl:.2f}\")\n",
    "print(f\"\\n说明: 困惑度越低表示模型越好。对于词表大小{len(tokenizer)}的模型，\")\n",
    "print(f\"      随机猜测的困惑度约为{len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 本notebook完成的内容\n",
    "\n",
    "1. **数据准备**\n",
    "   - 构建预训练语料库\n",
    "   - 创建PretrainDataset类（滑动窗口切分）\n",
    "   - 划分训练/验证集\n",
    "\n",
    "2. **预训练Trainer**\n",
    "   - 标准的next-token prediction训练\n",
    "   - 学习率warmup + cosine decay\n",
    "   - 梯度裁剪\n",
    "   - checkpoint保存\n",
    "\n",
    "3. **模型评估**\n",
    "   - 计算困惑度 (Perplexity)\n",
    "   - 文本生成测试\n",
    "   - 不同温度对比\n",
    "\n",
    "### 预训练模型的能力\n",
    "\n",
    "预训练后的模型学会了：\n",
    "- 基本的语言模式（词语搭配、句子结构）\n",
    "- 领域知识（AI/ML相关术语和概念）\n",
    "- 文本续写能力\n",
    "\n",
    "但还不会：\n",
    "- 遵循指令（需要SFT）\n",
    "- 生成高质量、人类偏好的回答（需要DPO）\n",
    "\n",
    "### 下一步: SFT训练 (03_SFT_Training.ipynb)\n",
    "\n",
    "在下一个notebook中，我们将对预训练模型进行指令微调（SFT），让它学会遵循用户指令。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
