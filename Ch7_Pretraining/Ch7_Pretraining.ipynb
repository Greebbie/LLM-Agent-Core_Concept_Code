{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9584809d",
   "metadata": {},
   "source": [
    "# Ch7 | Pre-training å®æˆ˜ï¼šç‚¼åˆ¶ä½ çš„ TinyLlama\n",
    "\n",
    "---\n",
    "\n",
    "**ç›®æ ‡ï¼š** ä»é›¶è®­ç»ƒä¸€ä¸ªèƒ½ç”Ÿæˆæ–‡æœ¬çš„è¯­è¨€æ¨¡å‹\n",
    "\n",
    "**æ ¸å¿ƒé—®é¢˜ï¼š** å¦‚ä½•è®©æ¨¡å‹å­¦ä¼š\"è¯´äººè¯\"ï¼Ÿ\n",
    "\n",
    "**è®¾å¤‡å»ºè®®ï¼š** CPU å¯è·‘ï¼ˆæ¼”ç¤ºä¸ºä¸»ï¼‰ï¼ŒGPU å¯åŠ é€Ÿã€‚\n",
    "\n",
    "**è·¯çº¿è¯´æ˜ï¼š** æœ¬ç« ä¸ºé¢„è®­ç»ƒæ¼”ç¤ºï¼›åç»­ Ch8â€“Ch10 è¿›å…¥ GPT-2 åŸºåº§çš„å·¥ç¨‹å¾®è°ƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« å†…å®¹\n",
    "\n",
    "1. **æ•°æ®å·¥ç¨‹**ï¼šDataset å’Œ DataLoader\n",
    "2. **è®­ç»ƒå¾ªç¯**ï¼šå®Œæ•´çš„é¢„è®­ç»ƒæµç¨‹\n",
    "3. **Checkpoint**ï¼šæ¨¡å‹ä¿å­˜ä¸åŠ è½½\n",
    "4. **å®æˆ˜**ï¼šåœ¨ç”Ÿæˆçš„æ•°æ®é›†ä¸Šè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_ch7_001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ å‰ç½®çŸ¥è¯†ï¼šä»€ä¹ˆæ˜¯é¢„è®­ç»ƒï¼Ÿ\n",
    "\n",
    "### è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒä»»åŠ¡\n",
    "\n",
    "LLM çš„é¢„è®­ç»ƒç›®æ ‡éå¸¸ç®€å•ï¼š\n",
    "\n",
    "$$é¢„æµ‹ä¸‹ä¸€ä¸ªè¯$$\n",
    "\n",
    "```\n",
    "è¾“å…¥: \"ä»Šå¤©å¤©æ°”çœŸ\"\n",
    "ç›®æ ‡: \"å¥½\"\n",
    "\n",
    "è¾“å…¥: \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ª\"\n",
    "ç›®æ ‡: \"åˆ†æ”¯\"\n",
    "```\n",
    "\n",
    "å°±è¿™ä¹ˆç®€å•ï¼ä½†é€šè¿‡åœ¨**æµ·é‡æ–‡æœ¬**ä¸Šåšè¿™ä¸ªä»»åŠ¡ï¼Œæ¨¡å‹èƒ½å­¦åˆ°ï¼š\n",
    "- è¯­æ³•è§„åˆ™\n",
    "- å¸¸è¯†çŸ¥è¯†\n",
    "- æ¨ç†èƒ½åŠ›\n",
    "\n",
    "### é¢„è®­ç»ƒçš„æ•°æ®è§„æ¨¡\n",
    "\n",
    "| æ¨¡å‹ | è®­ç»ƒæ•°æ®é‡ | å‚æ•°é‡ |\n",
    "|:---|:---|:---|\n",
    "| GPT-2 | 40 GB | 1.5B |\n",
    "| GPT-3 | 570 GB | 175B |\n",
    "| LLaMA | 1.4 TB | 7B-65B |\n",
    "\n",
    "\n",
    "### è®­ç»ƒæµç¨‹\n",
    "\n",
    "```\n",
    "åŸå§‹æ–‡æœ¬ â†’ Tokenizer â†’ æ•°å­—åºåˆ— â†’ Dataset â†’ DataLoader â†’ æ¨¡å‹ â†’ Loss â†’ æ›´æ–°å‚æ•°\n",
    "```\n",
    "\n",
    "å…³é”®ç»„ä»¶ï¼š\n",
    "1. **Dataset**ï¼šå°è£…æ•°æ®ï¼Œæä¾› (è¾“å…¥, ç›®æ ‡) å¯¹\n",
    "2. **DataLoader**ï¼šæ‰¹é‡åŠ è½½ï¼Œæ‰“ä¹±é¡ºåº\n",
    "3. **è®­ç»ƒå¾ªç¯**ï¼šå‰å‘ â†’ è®¡ç®—Loss â†’ åå‘ â†’ æ›´æ–°\n",
    "\n",
    "### å­¦ä¹ ç‡è°ƒåº¦\n",
    "\n",
    "è®­ç»ƒæŠ€å·§ï¼šå­¦ä¹ ç‡ä¸æ˜¯å›ºå®šçš„ï¼\n",
    "\n",
    "```\n",
    "å¼€å§‹: å­¦ä¹ ç‡ä»0æ…¢æ…¢å¢åŠ  (Warmup)\n",
    "ä¸­é—´: ä¿æŒè¾ƒé«˜å­¦ä¹ ç‡\n",
    "ç»“æŸ: å­¦ä¹ ç‡é€æ¸é™ä½ (Decay)\n",
    "```\n",
    "\n",
    "### æœ¬ç« ç›®æ ‡\n",
    "\n",
    "- å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæœ¬åœ°ä¸­æ–‡è¯­æ–™ï¼‰\n",
    "- å®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯\n",
    "- å­¦ä¼šä¿å­˜å’ŒåŠ è½½ Checkpoint\n",
    "- ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee8bdd",
   "metadata": {},
   "source": [
    "## 0. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Microsoft YaHei\", \"SimHei\", \"Noto Sans CJK SC\", \"Arial Unicode MS\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m8ms2raqo5r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ LLM é¢„è®­ç»ƒå…¨æ™¯å›¾\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import numpy as np\n",
    "\n",
    "def visualize_pretraining_pipeline():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ– LLM é¢„è®­ç»ƒçš„å®Œæ•´æµç¨‹\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.set_xlim(0, 16)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('LLM é¢„è®­ç»ƒæµç¨‹å…¨æ™¯', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # é¢œè‰²å®šä¹‰\n",
    "    colors = {\n",
    "        'data': '#E8F8F5',\n",
    "        'process': '#FEF9E7', \n",
    "        'model': '#FADBD8',\n",
    "        'train': '#D6EAF8',\n",
    "        'output': '#D5F5E3'\n",
    "    }\n",
    "    \n",
    "    # ===== é˜¶æ®µ1: æ•°æ®å‡†å¤‡ =====\n",
    "    ax.add_patch(FancyBboxPatch((0.5, 7.5), 3, 2, boxstyle='round,pad=0.1',\n",
    "                                facecolor=colors['data'], edgecolor='#1ABC9C', linewidth=2))\n",
    "    ax.text(2, 9, 'æ•°æ®å‡†å¤‡', fontsize=12, ha='center', fontweight='bold')\n",
    "    ax.text(2, 8.3, 'â€¢ æ”¶é›†è¯­æ–™åº“\\nâ€¢ æ¸…æ´—å»é‡\\nâ€¢ è´¨é‡è¿‡æ»¤', fontsize=9, ha='center', va='top')\n",
    "    \n",
    "    # ===== é˜¶æ®µ2: Tokenization =====\n",
    "    ax.add_patch(FancyBboxPatch((4, 7.5), 3, 2, boxstyle='round,pad=0.1',\n",
    "                                facecolor=colors['process'], edgecolor='#F39C12', linewidth=2))\n",
    "    ax.text(5.5, 9, 'Tokenization', fontsize=12, ha='center', fontweight='bold')\n",
    "    ax.text(5.5, 8.3, 'â€¢ BPE/SentencePiece\\nâ€¢ æ„å»ºè¯è¡¨\\nâ€¢ ç¼–ç æ–‡æœ¬', fontsize=9, ha='center', va='top')\n",
    "    \n",
    "    # ===== é˜¶æ®µ3: æ•°æ®åŠ è½½ =====\n",
    "    ax.add_patch(FancyBboxPatch((8, 7.5), 3, 2, boxstyle='round,pad=0.1',\n",
    "                                facecolor=colors['process'], edgecolor='#F39C12', linewidth=2))\n",
    "    ax.text(9.5, 9, 'DataLoader', fontsize=12, ha='center', fontweight='bold')\n",
    "    ax.text(9.5, 8.3, 'â€¢ åˆ†å—æ‰“åŒ…\\nâ€¢ Shuffle\\nâ€¢ Batch ç”Ÿæˆ', fontsize=9, ha='center', va='top')\n",
    "    \n",
    "    # ===== é˜¶æ®µ4: æ¨¡å‹ =====\n",
    "    ax.add_patch(FancyBboxPatch((12, 7.5), 3.5, 2, boxstyle='round,pad=0.1',\n",
    "                                facecolor=colors['model'], edgecolor='#E74C3C', linewidth=2))\n",
    "    ax.text(13.75, 9, 'Transformer', fontsize=12, ha='center', fontweight='bold')\n",
    "    ax.text(13.75, 8.3, 'â€¢ Embedding\\nâ€¢ N Ã— Blocks\\nâ€¢ LM Head', fontsize=9, ha='center', va='top')\n",
    "    \n",
    "    # ===== é˜¶æ®µ5: è®­ç»ƒå¾ªç¯ (ä¸­é—´å¤§æ¡†) =====\n",
    "    ax.add_patch(FancyBboxPatch((2, 2.5), 12, 4, boxstyle='round,pad=0.1',\n",
    "                                facecolor=colors['train'], edgecolor='#3498DB', linewidth=3))\n",
    "    ax.text(8, 6.2, 'è®­ç»ƒå¾ªç¯ (Training Loop)', fontsize=14, ha='center', fontweight='bold', color='#2980B9')\n",
    "    \n",
    "    # è®­ç»ƒæ­¥éª¤\n",
    "    steps = [\n",
    "        (3, 4.5, 'Forward\\nå‰å‘ä¼ æ’­', '#85C1E9'),\n",
    "        (6, 4.5, 'Loss\\nè®¡ç®—æŸå¤±', '#F9E79F'),\n",
    "        (9, 4.5, 'Backward\\nåå‘ä¼ æ’­', '#F5B7B1'),\n",
    "        (12, 4.5, 'Step\\næ›´æ–°å‚æ•°', '#ABEBC6'),\n",
    "    ]\n",
    "    \n",
    "    for x, y, text, color in steps:\n",
    "        ax.add_patch(FancyBboxPatch((x-0.8, y-0.6), 1.6, 1.2, boxstyle='round,pad=0.05',\n",
    "                                    facecolor=color, edgecolor='#333', linewidth=1.5))\n",
    "        ax.text(x, y, text, fontsize=9, ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # å¾ªç¯ç®­å¤´\n",
    "    for i in range(3):\n",
    "        ax.annotate('', xy=(steps[i+1][0]-0.9, 4.5), xytext=(steps[i][0]+0.9, 4.5),\n",
    "                   arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "    \n",
    "    # å›åˆ°å¼€å§‹çš„å¾ªç¯\n",
    "    ax.annotate('', xy=(3, 3.8), xytext=(12, 3.2),\n",
    "               arrowprops=dict(arrowstyle='->', color='#666', lw=1.5, \n",
    "                              connectionstyle='arc3,rad=-0.3'))\n",
    "    ax.text(7.5, 2.9, 'ä¸‹ä¸€ä¸ª batch', fontsize=9, ha='center', color='#666')\n",
    "    \n",
    "    # ===== è®­ç»ƒæŠ€å·§æ ‡æ³¨ =====\n",
    "    techniques = [\n",
    "        (1, 1.5, 'Learning Rate\\nSchedule', '#E8DAEF'),\n",
    "        (5, 1.5, 'Gradient\\nAccumulation', '#D5F5E3'),\n",
    "        (9, 1.5, 'Mixed\\nPrecision', '#FCF3CF'),\n",
    "        (13, 1.5, 'Checkpoint\\nSaving', '#FADBD8'),\n",
    "    ]\n",
    "    \n",
    "    for x, y, text, color in techniques:\n",
    "        ax.add_patch(FancyBboxPatch((x-1, y-0.5), 2.5, 1.2, boxstyle='round,pad=0.05',\n",
    "                                    facecolor=color, edgecolor='#999', linewidth=1))\n",
    "        ax.text(x+0.25, y+0.1, text, fontsize=8, ha='center', va='center')\n",
    "    \n",
    "    # è¿æ¥ç®­å¤´\n",
    "    arrows = [(3.5, 7.5, 4, 8.5), (7, 8.5, 8, 8.5), (11, 8.5, 12, 8.5)]\n",
    "    for x1, y1, x2, y2 in arrows:\n",
    "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                   arrowprops=dict(arrowstyle='->', color='#666', lw=2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n é¢„è®­ç»ƒæ ¸å¿ƒè¦ç‚¹:\")\n",
    "    print(\"   1. æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡ (åƒåœ¾è¿›åƒåœ¾å‡º)\")\n",
    "    print(\"   2. å­¦ä¹ ç‡è°ƒåº¦è‡³å…³é‡è¦ (warmup + cosine decay)\")\n",
    "    print(\"   3. æ¢¯åº¦ç´¯ç§¯å¯ä»¥æ¨¡æ‹Ÿæ›´å¤§ batch size\")\n",
    "    print(\"   4. å®šæœŸä¿å­˜ checkpoint é˜²æ­¢æ„å¤–\")\n",
    "\n",
    "visualize_pretraining_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640b7b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. å‡†å¤‡æ•°æ®ï¼šä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63dde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–æœ¬åœ°ä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_data_dir():\n",
    "    candidates = [Path.cwd(), Path.cwd().parent]\n",
    "    for base in candidates:\n",
    "        data_dir = base / \"data\"\n",
    "        if data_dir.exists():\n",
    "            return str(data_dir)\n",
    "    return os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "DATA_DIR = resolve_data_dir()\n",
    "PROJECT_ROOT = str(Path(DATA_DIR).parent)\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "data_path = os.path.join(DATA_DIR, \"custom_pretrain_corpus.txt\")\n",
    "BEST_MODEL_PATH = os.path.join(MODELS_DIR, \"ch7_pretrain_best_model.pt\")\n",
    "CHECKPOINT_PATH = os.path.join(MODELS_DIR, \"ch7_pretrain_checkpoint.pt\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"æœªæ‰¾åˆ°é¢„è®­ç»ƒè¯­æ–™: {data_path}\")\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"æ•°æ®é›†å¤§å°: {len(text):,} å­—ç¬¦\")\n",
    "print(\"\\nå‰500å­—ç¬¦é¢„è§ˆ:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºå­—ç¬¦çº§è¯è¡¨\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# ç¼–ç è§£ç å‡½æ•°\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(f\"è¯è¡¨å¤§å°: {vocab_size}\")\n",
    "print(f\"è¯è¡¨: {''.join(chars)}\")\n",
    "\n",
    "# æµ‹è¯•ï¼ˆä½¿ç”¨è¯­æ–™ä¸­çš„å­—ç¬¦ï¼Œé¿å… OOVï¼‰\n",
    "test_str = text[:10]\n",
    "encoded = encode(test_str)\n",
    "decoded = decode(encoded)\n",
    "print(f\"\\næµ‹è¯•ç¼–ç è§£ç :\")\n",
    "print(f\"  åŸæ–‡: {test_str}\")\n",
    "print(f\"  ç¼–ç : {encoded}\")\n",
    "print(f\"  è§£ç : {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a02b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. åˆ›å»º Dataset å’Œ DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    æ–‡æœ¬æ•°æ®é›†ï¼šæ¯ä¸ªæ ·æœ¬æ˜¯ (input, target) å¯¹\n",
    "    target æ˜¯ input å³ç§»ä¸€ä½\n",
    "    \"\"\"\n",
    "    def __init__(self, text, block_size):\n",
    "        self.data = torch.tensor(encode(text), dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†\n",
    "n = int(0.9 * len(text))\n",
    "train_text = text[:n]\n",
    "val_text = text[n:]\n",
    "\n",
    "block_size = 64  # ä¸Šä¸‹æ–‡é•¿åº¦\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TextDataset(train_text, block_size)\n",
    "val_dataset = TextDataset(val_text, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset):,}\")\n",
    "print(f\"éªŒè¯é›†å¤§å°: {len(val_dataset):,}\")\n",
    "print(f\"æ¯ä¸ª batch: {batch_size} ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ— {block_size} tokens\")\n",
    "\n",
    "# æŸ¥çœ‹ä¸€ä¸ª batch\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"\\nBatch å½¢çŠ¶: x={x.shape}, y={y.shape}\")\n",
    "print(f\"\\næ ·æœ¬å±•ç¤º:\")\n",
    "print(f\"  è¾“å…¥:  {decode(x[0].tolist())}\")\n",
    "print(f\"  ç›®æ ‡:  {decode(y[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9dabec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. å®šä¹‰æ¨¡å‹ï¼ˆä½¿ç”¨ä¸Šä¸€ç« çš„ GPTï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_embd = config['n_embd']\n",
    "        self.head_dim = config['n_embd'] // config['n_head']\n",
    "        \n",
    "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
    "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "        self.register_buffer(\"mask\", torch.tril(\n",
    "            torch.ones(config['block_size'], config['block_size'])\n",
    "        ).view(1, 1, config['block_size'], config['block_size']))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config['n_embd'], 4 * config['n_embd'])\n",
    "        self.c_proj = nn.Linear(4 * config['n_embd'], config['n_embd'])\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(F.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
    "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
    "            drop = nn.Dropout(config['dropout']),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
    "            ln_f = nn.LayerNorm(config['n_embd']),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        \n",
    "        x = self.transformer.drop(self.transformer.wte(idx) + self.transformer.wpe(pos))\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config['block_size'] else idx[:, -self.config['block_size']:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# æ¨¡å‹é…ç½®\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'block_size': block_size,\n",
    "    'n_layer': 4,\n",
    "    'n_head': 4,\n",
    "    'n_embd': 128,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc88a06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. è®­ç»ƒå¾ªç¯\n",
    "\n",
    "> è¯´æ˜ï¼šæœ¬ç« ä¸»è¦æ¼”ç¤ºæµç¨‹ï¼Œè®­ç»ƒé…ç½®ä¸æ•°æ®è§„æ¨¡ä»…ç”¨äºç¤ºä¾‹ã€‚æ›´å®Œæ•´çš„è®­ç»ƒå®è·µè§ Custom_GPT_Trainingã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4lyf4m6hyto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ å­¦ä¹ ç‡è°ƒåº¦å¯è§†åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_lr_schedules():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    total_steps = 10000\n",
    "    warmup_steps = 1000\n",
    "    base_lr = 3e-4\n",
    "    min_lr = 1e-5\n",
    "    \n",
    "    steps = np.arange(total_steps)\n",
    "    \n",
    "    # 1. Constant (å›ºå®šå­¦ä¹ ç‡)\n",
    "    ax = axes[0, 0]\n",
    "    lr_constant = np.ones(total_steps) * base_lr\n",
    "    ax.plot(steps, lr_constant, 'b-', linewidth=2)\n",
    "    ax.set_title('Constant LR (å›ºå®šå­¦ä¹ ç‡)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Training Steps')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_ylim(0, base_lr * 1.2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(5000, base_lr * 0.5, 'ä¸æ¨è\\nå¼€å§‹å¤ªå¤§ï¼Œç»“å°¾å¤ªå¤§', fontsize=10, ha='center',\n",
    "           bbox=dict(boxstyle='round', facecolor='#FFE5E5'))\n",
    "    \n",
    "    # 2. Linear Decay (çº¿æ€§è¡°å‡)\n",
    "    ax = axes[0, 1]\n",
    "    lr_linear = base_lr * (1 - steps / total_steps)\n",
    "    lr_linear = np.maximum(lr_linear, min_lr)\n",
    "    ax.plot(steps, lr_linear, 'g-', linewidth=2)\n",
    "    ax.set_title('Linear Decay (çº¿æ€§è¡°å‡)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Training Steps')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(5000, base_lr * 0.6, 'ä¸€èˆ¬\\nç¼ºå°‘warmup', fontsize=10, ha='center',\n",
    "           bbox=dict(boxstyle='round', facecolor='#FFF3CD'))\n",
    "    \n",
    "    # 3. Warmup + Cosine Decay (âœ“ æ¨è!)\n",
    "    ax = axes[1, 0]\n",
    "    lr_warmup_cosine = np.zeros(total_steps)\n",
    "    # Warmup phase\n",
    "    lr_warmup_cosine[:warmup_steps] = base_lr * steps[:warmup_steps] / warmup_steps\n",
    "    # Cosine decay phase\n",
    "    decay_steps = total_steps - warmup_steps\n",
    "    for i in range(warmup_steps, total_steps):\n",
    "        progress = (i - warmup_steps) / decay_steps\n",
    "        lr_warmup_cosine[i] = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    ax.plot(steps, lr_warmup_cosine, 'r-', linewidth=2)\n",
    "    ax.axvline(x=warmup_steps, color='orange', linestyle='--', alpha=0.7, label='Warmup ends')\n",
    "    ax.fill_between(steps[:warmup_steps], 0, lr_warmup_cosine[:warmup_steps], alpha=0.3, color='orange')\n",
    "    ax.set_title('Warmup + Cosine Decay æ¨è', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Training Steps')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(6000, base_lr * 0.7, 'æœ€ä½³å®è·µ\\nâ€¢ Warmupé¿å…å¼€å§‹éœ‡è¡\\nâ€¢ Cosineå¹³æ»‘æ”¶æ•›', \n",
    "           fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='#D5F5E3'))\n",
    "    \n",
    "    # 4. å¯¹æ¯”æ‰€æœ‰ç­–ç•¥\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(steps, lr_constant, 'b-', linewidth=2, label='Constant', alpha=0.7)\n",
    "    ax.plot(steps, lr_linear, 'g-', linewidth=2, label='Linear', alpha=0.7)\n",
    "    ax.plot(steps, lr_warmup_cosine, 'r-', linewidth=2, label='Warmup+Cosine', alpha=0.9)\n",
    "    ax.set_title('ç­–ç•¥å¯¹æ¯”', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Training Steps')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å¯¹æ¯”', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å­¦ä¹ ç‡è°ƒåº¦è¦ç‚¹:\")\n",
    "    print(\"   â€¢ Warmup: å¼€å§‹æ—¶å­¦ä¹ ç‡ä»0ç¼“æ…¢å¢åŠ ï¼Œé¿å…åˆæœŸä¸ç¨³å®š\")\n",
    "    print(\"   â€¢ Cosine Decay: å¹³æ»‘ä¸‹é™åˆ°æœ€å°å€¼ï¼Œå¸®åŠ©æ¨¡å‹æ”¶æ•›\")\n",
    "    print(\"   â€¢ å…¸å‹é…ç½®: warmup_ratio=0.1, min_lr=base_lr/10\")\n",
    "\n",
    "visualize_lr_schedules()\n",
    "\n",
    "# å®ç°å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "def get_lr_scheduler(optimizer, warmup_steps, total_steps, min_lr_ratio=0.1):\n",
    "    \"\"\"\n",
    "    åˆ›å»º Warmup + Cosine Decay å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            # Warmup phase\n",
    "            return current_step / warmup_steps\n",
    "        else:\n",
    "            # Cosine decay phase\n",
    "            progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return min_lr_ratio + 0.5 * (1 - min_lr_ratio) * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(\"\\nå­¦ä¹ ç‡è°ƒåº¦å™¨ä»£ç :\")\n",
    "print(\"scheduler = get_lr_scheduler(optimizer, warmup_steps=1000, total_steps=10000)\")\n",
    "print(\"# åœ¨æ¯ä¸ªè®­ç»ƒstepåè°ƒç”¨: scheduler.step()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒé…ç½®\n",
    "learning_rate = 3e-4\n",
    "max_iters = 2000\n",
    "eval_interval = 200\n",
    "eval_iters = 50\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = {'train': 0, 'val': 0}\n",
    "    \n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        for x, y in loader:\n",
    "            if count >= eval_iters:\n",
    "                break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "        losses[split] = total_loss / count\n",
    "    \n",
    "    model.train()\n",
    "    return losses\n",
    "\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9259d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒä¸»å¾ªç¯\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "start_time = time.time()\n",
    "iter_count = 0\n",
    "\n",
    "for epoch in range(max_iters // len(train_loader) + 1):\n",
    "    for x, y in train_loader:\n",
    "        if iter_count >= max_iters:\n",
    "            break\n",
    "            \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # è¯„ä¼°\n",
    "        if iter_count % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Iter {iter_count:4d} | Train Loss: {losses['train']:.4f} | Val Loss: {losses['val']:.4f} | Time: {elapsed:.1f}s\")\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "                torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        \n",
    "        iter_count += 1\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯ Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_losses, label='Val Loss', color='orange')\n",
    "plt.xlabel('Evaluation Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7b40a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ç”Ÿæˆæ–‡æœ¬ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b26d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "prompt = \"ä¼ä¸šå¹´åº¦é¢„ç®—é€šå¸¸æŒ‰éƒ¨é—¨æ‹†åˆ†ï¼Œ\\n\"\n",
    "context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Generated Text:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç”Ÿæˆ\n",
    "generated = model.generate(context, max_new_tokens=200, temperature=0.8, top_k=40)\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°è¯•ä¸åŒçš„ prompt\n",
    "prompts = [\n",
    "    \"ä¼ä¸šå¹´åº¦é¢„ç®—é€šå¸¸æŒ‰éƒ¨é—¨æ‹†åˆ†ï¼Œ\\n\",\n",
    "    \"éœ€æ±‚è¯„å®¡åº”è®°å½•å˜æ›´åŸå› ï¼Œ\\n\",\n",
    "    \"ç³»ç»Ÿå®¹é‡è§„åˆ’éœ€è¦è€ƒè™‘\\n\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=100, temperature=0.8, top_k=40)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ba455",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Checkpoint ç®¡ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„ checkpointï¼ˆåŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Checkpoint loaded from {path}\")\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "# ä¿å­˜å®Œæ•´ checkpoint\n",
    "save_checkpoint(model, optimizer, iter_count, best_val_loss, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c5526",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "1. **æ•°æ®å·¥ç¨‹**\n",
    "   - Dataset å°è£…æ•°æ®é€»è¾‘\n",
    "   - DataLoader å¤„ç†æ‰¹é‡åŠ è½½å’Œ shuffle\n",
    "\n",
    "2. **è®­ç»ƒå¾ªç¯**\n",
    "   - Forward â†’ Loss â†’ Backward â†’ Step\n",
    "   - å®šæœŸè¯„ä¼°éªŒè¯é›†\n",
    "   - ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "\n",
    "3. **Checkpoint**\n",
    "   - ä¿å­˜æ¨¡å‹æƒé‡\n",
    "   - ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯ç»§ç»­è®­ç»ƒï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71075007",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒ\n",
    "\n",
    "1. **å¢åŠ æ¨¡å‹è§„æ¨¡**ï¼šå°è¯• 6 å±‚ã€256 ç»´\n",
    "2. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šæ·»åŠ  warmup å’Œ cosine decay\n",
    "3. **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨ AMP åŠ é€Ÿè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ ç©ºé—´\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
