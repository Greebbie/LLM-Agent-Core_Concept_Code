{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a343b7",
   "metadata": {},
   "source": [
    "# Ch6 | Tokenizerï¼šæ•°æ®é¢„å¤„ç†çš„è‰ºæœ¯\n",
    "\n",
    "---\n",
    "\n",
    "**ç›®æ ‡ï¼š** ç†è§£ Tokenizer çš„å·¥ä½œåŸç†\n",
    "\n",
    "**æ ¸å¿ƒé—®é¢˜ï¼š** å¦‚ä½•æŠŠæ–‡æœ¬å˜æˆæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« å†…å®¹\n",
    "\n",
    "1. **åŸºç¡€åˆ†è¯**ï¼šå­—ç¬¦çº§ vs è¯çº§\n",
    "2. **BPE ç®—æ³•**ï¼šByte-Pair Encoding åŸç†\n",
    "3. **å®æˆ˜**ï¼šä½¿ç”¨ tiktoken\n",
    "4. **å‘ç‚¹åˆ†æ**ï¼šTokenizer å¯¼è‡´çš„ Bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_ch6_001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ å‰ç½®çŸ¥è¯†ï¼šæ–‡æœ¬åˆ°æ•°å­—çš„æ¡¥æ¢\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦ Tokenizerï¼Ÿ\n",
    "\n",
    "ç¥ç»ç½‘ç»œåªèƒ½å¤„ç†æ•°å­—ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ï¼š\n",
    "\n",
    "```\n",
    "\"Hello World\" â†’ [15496, 2159] â†’ ç¥ç»ç½‘ç»œ â†’ [1312] â†’ \"I\"\n",
    "    æ–‡æœ¬           æ•°å­—          å¤„ç†         æ•°å­—     æ–‡æœ¬\n",
    "```\n",
    "\n",
    "**Tokenizer** å°±æ˜¯è´Ÿè´£ æ–‡æœ¬ â†” æ•°å­— è½¬æ¢çš„å·¥å…·ã€‚\n",
    "\n",
    "### Tokenizer çš„æ¼”è¿›\n",
    "\n",
    "| æ–¹æ³• | ä¾‹å­ | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n",
    "|:---|:---|:---|:---|\n",
    "| å­—ç¬¦çº§ | \"cat\" â†’ [\"c\",\"a\",\"t\"] | è¯è¡¨å° | åºåˆ—å¤ªé•¿ |\n",
    "| è¯çº§ | \"cat\" â†’ [\"cat\"] | åºåˆ—çŸ­ | è¯è¡¨å·¨å¤§ï¼ŒOOVé—®é¢˜ |\n",
    "| å­è¯çº§ | \"unhappy\" â†’ [\"un\",\"happy\"] | å¹³è¡¡ | éœ€è¦å­¦ä¹  |\n",
    "\n",
    "**ç°ä»£ LLM éƒ½ä½¿ç”¨å­è¯çº§ Tokenizer**ï¼ˆå¦‚ BPEï¼‰\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯ BPEï¼Ÿ\n",
    "\n",
    "BPE (Byte-Pair Encoding) çš„æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "\n",
    "1. ä»å­—ç¬¦å¼€å§‹\n",
    "2. ç»Ÿè®¡æœ€å¸¸è§çš„ç›¸é‚»å­—ç¬¦å¯¹\n",
    "3. åˆå¹¶å®ƒä»¬æˆä¸ºæ–°çš„ token\n",
    "4. é‡å¤ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯è¡¨å¤§å°\n",
    "\n",
    "```\n",
    "è®­ç»ƒè¯­æ–™: \"low low lower lowest\"\n",
    "åˆå§‹: l o w e r s t\n",
    "åˆå¹¶1: lo w e r s t  (l+o æœ€å¸¸è§)\n",
    "åˆå¹¶2: low e r s t  (lo+w æœ€å¸¸è§)\n",
    "...\n",
    "æœ€ç»ˆ: low er est\n",
    "```\n",
    "\n",
    "### Tokenizer çš„\"å‘\"\n",
    "\n",
    "âš ï¸ Tokenizer ä¼šå¯¼è‡´ä¸€äº›åç›´è§‰çš„é—®é¢˜ï¼š\n",
    "\n",
    "- åŒä¸€ä¸ªè¯å¯èƒ½è¢«åˆ†æˆä¸åŒçš„ token\n",
    "- ä¸åŒè¯­è¨€çš„æ•ˆç‡å·®å¼‚å¾ˆå¤§\n",
    "- æ•°å­—å¤„ç†å¯èƒ½å‡ºé—®é¢˜\n",
    "\n",
    "### æœ¬ç« ç›®æ ‡\n",
    "\n",
    "- ç†è§£å­—ç¬¦çº§/è¯çº§/å­è¯çº§åˆ†è¯çš„åŒºåˆ«\n",
    "- ä»é›¶å®ç° BPE ç®—æ³•\n",
    "- äº†è§£ Tokenizer å¯¼è‡´çš„å¸¸è§ Bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157b6e8",
   "metadata": {},
   "source": [
    "## 0. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecaa91e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ä¸ºä»€ä¹ˆéœ€è¦ Tokenizerï¼Ÿ\n",
    "\n",
    "æ¨¡å‹åªèƒ½å¤„ç†æ•°å­—ï¼Œä¸èƒ½ç›´æ¥å¤„ç†æ–‡æœ¬ã€‚\n",
    "\n",
    "```\n",
    "\"Hello World\" â†’ [15496, 2159] â†’ æ¨¡å‹ â†’ [1312, 716] â†’ \"I am\"\n",
    "```\n",
    "\n",
    "### åˆ†è¯ç­–ç•¥å¯¹æ¯”\n",
    "\n",
    "| ç­–ç•¥ | è¯è¡¨å¤§å° | åºåˆ—é•¿åº¦ | é—®é¢˜ |\n",
    "|:---|:---|:---|:---|\n",
    "| å­—ç¬¦çº§ | ~100 | å¾ˆé•¿ | åºåˆ—å¤ªé•¿ï¼Œæ•ˆç‡ä½ |\n",
    "| è¯çº§ | å‡ åä¸‡ | çŸ­ | OOVé—®é¢˜ï¼ˆæœªç™»å½•è¯ï¼‰|\n",
    "| å­è¯çº§(BPE) | ~50k | é€‚ä¸­ | æœ€ä½³å¹³è¡¡ âœ“ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºä¸åŒåˆ†è¯ç­–ç•¥\n",
    "text = \"Hello, I'm learning machine learning!\"\n",
    "\n",
    "# 1. å­—ç¬¦çº§\n",
    "char_tokens = list(text)\n",
    "print(f\"å­—ç¬¦çº§åˆ†è¯ ({len(char_tokens)} tokens):\")\n",
    "print(char_tokens[:20], \"...\")\n",
    "\n",
    "# 2. è¯çº§ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†ï¼‰\n",
    "word_tokens = text.split()\n",
    "print(f\"\\nè¯çº§åˆ†è¯ ({len(word_tokens)} tokens):\")\n",
    "print(word_tokens)\n",
    "\n",
    "# 3. å­è¯çº§ï¼ˆæ¨¡æ‹Ÿï¼‰\n",
    "subword_tokens = ['Hello', ',', ' I', \"'m\", ' learning', ' machine', ' learning', '!']\n",
    "print(f\"\\nå­è¯çº§åˆ†è¯ ({len(subword_tokens)} tokens):\")\n",
    "print(subword_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a821fdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. BPE ç®—æ³•ï¼šä»å¤´å®ç°\n",
    "\n",
    "BPE (Byte-Pair Encoding) çš„æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "\n",
    "1. ä»å­—ç¬¦å¼€å§‹\n",
    "2. ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡\n",
    "3. åˆå¹¶æœ€é¢‘ç¹çš„å¯¹\n",
    "4. é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯è¡¨å¤§å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757013c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    \"\"\"ç»Ÿè®¡ç›¸é‚»å¯¹çš„é¢‘ç‡\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"åˆå¹¶æŒ‡å®šçš„å¯¹\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    \n",
    "    return new_vocab\n",
    "\n",
    "# åˆå§‹è¯è¡¨ï¼ˆæ¯ä¸ªè¯å·²ç»æŒ‰å­—ç¬¦åˆ†å¼€ï¼‰\n",
    "corpus = \"low low low low low lower lower newest newest newest newest newest newest widest widest widest\"\n",
    "words = corpus.split()\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# æ„å»ºåˆå§‹è¯è¡¨ï¼šæ¯ä¸ªè¯æŒ‰å­—ç¬¦åˆ†å¼€ï¼ŒåŠ ä¸Šç»“æŸç¬¦\n",
    "vocab = {}\n",
    "for word, freq in word_freq.items():\n",
    "    vocab[' '.join(list(word)) + ' </w>'] = freq\n",
    "\n",
    "print(\"åˆå§‹è¯è¡¨:\")\n",
    "for word, freq in vocab.items():\n",
    "    print(f\"  '{word}': {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6808e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE è®­ç»ƒè¿‡ç¨‹\n",
    "num_merges = 10\n",
    "\n",
    "print(\"BPE è®­ç»ƒè¿‡ç¨‹:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    \n",
    "    # æ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    print(f\"\\nç¬¬ {i+1} æ¬¡åˆå¹¶: {best_pair} (é¢‘ç‡: {pairs[best_pair]})\")\n",
    "    \n",
    "    # åˆå¹¶\n",
    "    vocab = merge_vocab(best_pair, vocab)\n",
    "    print(f\"  å½“å‰è¯è¡¨: {list(vocab.keys())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"æœ€ç»ˆè¯è¡¨:\")\n",
    "for word, freq in vocab.items():\n",
    "    print(f\"  '{word}': {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25049e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ä½¿ç”¨ tiktokenï¼ˆOpenAI çš„ Tokenizerï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # GPT-4 ä½¿ç”¨çš„ç¼–ç \n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    text = \"Hello, ä½ å¥½ï¼I'm learning about tokenizers.\"\n",
    "    tokens = enc.encode(text)\n",
    "    \n",
    "    print(f\"åŸæ–‡: {text}\")\n",
    "    print(f\"\\nToken IDs ({len(tokens)} tokens): {tokens}\")\n",
    "    print(f\"\\nè§£ç æ¯ä¸ª token:\")\n",
    "    for t in tokens:\n",
    "        print(f\"  {t}: '{enc.decode([t])}'\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"è¯·å®‰è£… tiktoken: pip install tiktoken\")\n",
    "    print(\"\\næ¨¡æ‹Ÿè¾“å‡º:\")\n",
    "    print(\"åŸæ–‡: Hello, ä½ å¥½ï¼I'm learning about tokenizers.\")\n",
    "    print(\"Token IDs: [15496, 11, 220, 57668, 53901, 6447, ...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab47361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯”è¾ƒä¸åŒè¯­è¨€çš„ token æ•ˆç‡\n",
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    texts = {\n",
    "        \"English\": \"Machine learning is a branch of artificial intelligence.\",\n",
    "        \"Chinese\": \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\",\n",
    "        \"Code\": \"def hello(): return 'Hello, World!'\",\n",
    "    }\n",
    "    \n",
    "    print(\"ä¸åŒè¯­è¨€çš„ Token æ•ˆç‡:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for lang, text in texts.items():\n",
    "        tokens = enc.encode(text)\n",
    "        ratio = len(text) / len(tokens)\n",
    "        print(f\"\\n{lang}:\")\n",
    "        print(f\"  æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦\")\n",
    "        print(f\"  Token æ•°: {len(tokens)}\")\n",
    "        print(f\"  æ•ˆç‡: {ratio:.2f} å­—ç¬¦/token\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"English: ~5.2 å­—ç¬¦/token\")\n",
    "    print(\"Chinese: ~1.5 å­—ç¬¦/token (æ•ˆç‡è¾ƒä½)\")\n",
    "    print(\"Code: ~3.8 å­—ç¬¦/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd44c42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Tokenizer å¯¼è‡´çš„ç»å…¸ Bug\n",
    "\n",
    "Tokenizer ä¸æ˜¯å®Œç¾çš„ï¼Œå®ƒä¼šå¯¼è‡´ä¸€äº›\"å¥‡æ€ª\"çš„è¡Œä¸ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug 1: æ— æ³•æ­£ç¡®åè½¬å­—ç¬¦ä¸²\n",
    "print(\"Bug 1: å­—ç¬¦ä¸²åè½¬\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    word = \"lollipop\"\n",
    "    tokens = enc.encode(word)\n",
    "    \n",
    "    print(f\"åŸè¯: {word}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"æ¯ä¸ªtoken: {[enc.decode([t]) for t in tokens]}\")\n",
    "    print(f\"\\nå¦‚æœ LLM æŒ‰ token åè½¬: {[enc.decode([t]) for t in tokens[::-1]]}\")\n",
    "    print(f\"æ­£ç¡®çš„åè½¬åº”è¯¥æ˜¯: {word[::-1]}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"åŸè¯: lollipop\")\n",
    "    print(\"Tokens: ['loll', 'ipop'] (åˆ†è¯æ–¹å¼)\")\n",
    "    print(\"LLM å¯èƒ½åè½¬ä¸º: 'ipoploll' è€Œä¸æ˜¯ 'popolil'\")\n",
    "\n",
    "print(\"\\nåŸå› : LLM æ“ä½œçš„æ˜¯ tokenï¼Œä¸æ˜¯å­—ç¬¦ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug 2: æ•°å­¦è®¡ç®—å›°éš¾\n",
    "print(\"Bug 2: æ•°å­—å¤„ç†\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    numbers = [\"123\", \"1234\", \"12345\", \"123456789\"]\n",
    "    \n",
    "    for num in numbers:\n",
    "        tokens = enc.encode(num)\n",
    "        print(f\"{num} -> {len(tokens)} tokens: {[enc.decode([t]) for t in tokens]}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"123 -> 1 token: ['123']\")\n",
    "    print(\"1234 -> 1 token: ['1234']\")\n",
    "    print(\"12345 -> 2 tokens: ['123', '45']\")\n",
    "    print(\"123456789 -> 3 tokens: ['123', '456', '789']\")\n",
    "\n",
    "print(\"\\nå¤§æ•°å­—è¢«åˆ†æˆå¤šä¸ª tokenï¼Œè®¡ç®—å˜å¾—å›°éš¾ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug 3: ç©ºæ ¼æ•æ„Ÿ\n",
    "print(\"Bug 3: ç©ºæ ¼å½±å“åˆ†è¯\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    texts = [\"hello\", \" hello\", \"  hello\", \"Hello\"]\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = enc.encode(text)\n",
    "        print(f\"'{text}' -> {len(tokens)} tokens: {tokens}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"'hello' -> 1 token\")\n",
    "    print(\"' hello' -> 1 token (ä¸åŒçš„!)\")\n",
    "    print(\"'  hello' -> 2 tokens\")\n",
    "    print(\"'Hello' -> 1 token (ä¸åŒçš„!)\")\n",
    "\n",
    "print(\"\\nç©ºæ ¼å’Œå¤§å°å†™ä¼šå½±å“åˆ†è¯ç»“æœï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd12ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. å¯è§†åŒ–è¯è¡¨è¦†ç›–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿè¯è¡¨åˆ†æ\n",
    "# å¸¸è§ token ç±»å‹åˆ†å¸ƒ\n",
    "\n",
    "categories = ['English words', 'Chinese chars', 'Numbers', 'Punctuation', 'Code tokens', 'Special']\n",
    "sizes = [45, 15, 5, 10, 20, 5]\n",
    "colors = ['steelblue', 'coral', 'gold', 'lightgreen', 'plum', 'gray']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# é¥¼å›¾\n",
    "axes[0].pie(sizes, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Typical Vocabulary Composition (GPT-4 style)')\n",
    "\n",
    "# æ¡å½¢å›¾ï¼šä¸åŒè¯­è¨€çš„æ•ˆç‡\n",
    "languages = ['English', 'Spanish', 'German', 'Chinese', 'Japanese', 'Code']\n",
    "efficiency = [5.2, 4.8, 4.5, 1.8, 2.0, 3.8]\n",
    "\n",
    "axes[1].barh(languages, efficiency, color='steelblue')\n",
    "axes[1].set_xlabel('Characters per Token')\n",
    "axes[1].set_title('Tokenization Efficiency by Language')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"è§‚å¯Ÿï¼š\")\n",
    "print(\"  - è‹±è¯­æ•ˆç‡æœ€é«˜ï¼ˆè¯è¡¨ä¸»è¦ä¸ºè‹±è¯­ä¼˜åŒ–ï¼‰\")\n",
    "print(\"  - ä¸­æ–‡æ¯ä¸ªå­—ç¬¦çº¦å  1-2 ä¸ª token\")\n",
    "print(\"  - ä»£ç æœ‰ä¸“é—¨çš„ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6d252",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "\n",
    "1. **Tokenizer çš„å¿…è¦æ€§**\n",
    "   - æ¨¡å‹åªèƒ½å¤„ç†æ•°å­—\n",
    "   - å­è¯çº§åˆ†è¯æ˜¯æœ€ä½³å¹³è¡¡\n",
    "\n",
    "2. **BPE ç®—æ³•**\n",
    "   - ä»å­—ç¬¦å¼€å§‹ï¼Œé€æ­¥åˆå¹¶é«˜é¢‘å¯¹\n",
    "   - è‡ªåŠ¨å­¦ä¹ å¸¸è§å­è¯\n",
    "\n",
    "3. **å®è·µæ³¨æ„äº‹é¡¹**\n",
    "   - ä¸åŒè¯­è¨€æ•ˆç‡ä¸åŒ\n",
    "   - Tokenizer ä¼šå¯¼è‡´ä¸€äº›\"Bug\"\n",
    "   - ç†è§£è¿™äº›é™åˆ¶å¾ˆé‡è¦\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1296402",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒ\n",
    "\n",
    "1. **å®ç°å®Œæ•´ BPE**ï¼šæ·»åŠ  encode å’Œ decode åŠŸèƒ½\n",
    "2. **æ¯”è¾ƒ Tokenizer**ï¼šå¯¹æ¯” GPT-2, GPT-4, LLaMA çš„åˆ†è¯å·®å¼‚\n",
    "3. **æ€è€ƒé¢˜**ï¼šä¸ºä»€ä¹ˆä¸­æ–‡åˆ†è¯æ•ˆç‡ä½ï¼Ÿå¦‚ä½•æ”¹è¿›ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ff513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ ç©ºé—´\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
