{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b11468",
   "metadata": {},
   "source": [
    "# Ch3 | Self-Attentionï¼šè®©è¯å‘é‡åœ¨ä¸Šä¸‹æ–‡ä¸­\"æµåŠ¨\"\n",
    "\n",
    "---\n",
    "\n",
    "**ç›®æ ‡ï¼š** ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†\n",
    "\n",
    "**æ ¸å¿ƒé—®é¢˜ï¼š** å¦‚ä½•è®©è¯å‘é‡æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€å˜åŒ–ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« å†…å®¹\n",
    "\n",
    "1. **ä¸ºä»€ä¹ˆéœ€è¦ Attention**ï¼šé™æ€ Embedding çš„å±€é™\n",
    "2. **æ‰‹å†™ Attention**ï¼šä»é›¶å®ç° Q, K, V\n",
    "3. **å¯è§†åŒ–**ï¼šæ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾\n",
    "4. **Masked Attention**ï¼šä¸ºä»€ä¹ˆä¸èƒ½\"å·çœ‹ç­”æ¡ˆ\"ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_ch3_001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ å‰ç½®çŸ¥è¯†ï¼šè¯­è¨€ç†è§£çš„æŒ‘æˆ˜\n",
    "\n",
    "### Embedding çš„å±€é™æ€§\n",
    "\n",
    "ä¸Šä¸€ç« æˆ‘ä»¬å­¦ä¹ äº† Embeddingï¼Œä½†å®ƒæœ‰ä¸€ä¸ªå¤§é—®é¢˜ï¼š\n",
    "\n",
    "```\n",
    "\"è‹¹æœå¾ˆå¥½åƒ\"  â†’  è‹¹æœ = [0.2, 0.8, ...]  (æ°´æœ)\n",
    "\"è‹¹æœå‘å¸ƒæ–°æ‰‹æœº\"  â†’  è‹¹æœ = [0.2, 0.8, ...]  (åŒæ ·çš„å‘é‡ï¼)\n",
    "```\n",
    "\n",
    "**é—®é¢˜ï¼šåŒä¸€ä¸ªè¯åœ¨ä¸åŒè¯­å¢ƒä¸‹å«ä¹‰ä¸åŒï¼Œä½† Embedding æ˜¯å›ºå®šçš„ï¼**\n",
    "\n",
    "### è¯­å¢ƒçš„é‡è¦æ€§\n",
    "\n",
    "äººç±»ç†è§£è¯­è¨€æ—¶ï¼Œä¼šæ ¹æ®**ä¸Šä¸‹æ–‡**æ¥ç†è§£æ¯ä¸ªè¯ï¼š\n",
    "\n",
    "```\n",
    "\"æˆ‘å–œæ¬¢åƒè‹¹æœ\"  â†’  è‹¹æœ = æ°´æœ ğŸ\n",
    "\"æˆ‘åœ¨ç”¨è‹¹æœæ‰‹æœº\"  â†’  è‹¹æœ = å…¬å¸ \n",
    "```\n",
    "\n",
    "### å¦‚ä½•è®©æ¨¡å‹\"çœ‹ä¸Šä¸‹æ–‡\"ï¼Ÿ\n",
    "\n",
    "**Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ï¼šè®©æ¯ä¸ªè¯**çœ‹çœ‹å‘¨å›´çš„è¯**ï¼Œç„¶åæ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´è‡ªå·±çš„è¡¨ç¤ºã€‚\n",
    "\n",
    "æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "```\n",
    "é™æ€ Embedding:  è‹¹æœ â†’ [å›ºå®šå‘é‡]\n",
    "+\n",
    "Self-Attention:  è‹¹æœ + \"æˆ‘å–œæ¬¢åƒ\" â†’ [åŠ¨æ€å‘é‡ï¼Œè¡¨ç¤ºæ°´æœ]\n",
    "                 è‹¹æœ + \"å‘å¸ƒæ–°æ‰‹æœº\" â†’ [åŠ¨æ€å‘é‡ï¼Œè¡¨ç¤ºå…¬å¸]\n",
    "```\n",
    "\n",
    "### æœ¬ç« ç›®æ ‡\n",
    "\n",
    "- ç†è§£ä¸ºä»€ä¹ˆéœ€è¦ Attention\n",
    "- æŒæ¡ Q (Query), K (Key), V (Value) çš„ç›´è§‰\n",
    "- å®ç° Self-Attention å¹¶å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n",
    "- ç†è§£ Causal Maskï¼ˆä¸ºä»€ä¹ˆä¸èƒ½\"å·çœ‹æœªæ¥\"ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a18690",
   "metadata": {},
   "source": [
    "## 0. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib import font_manager as fm\n",
    "import matplotlib as mpl\n",
    "font_path = \"../assets/fonts/NotoSansCJKsc-Regular.otf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "\n",
    "mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "mpl.rcParams[\"font.sans-serif\"] = [\"Noto Sans CJK SC\", \"DejaVu Sans\"]\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242d3e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ä¸ºä»€ä¹ˆéœ€è¦ Attentionï¼Ÿ\n",
    "\n",
    "### é™æ€ Embedding çš„é—®é¢˜\n",
    "\n",
    "è€ƒè™‘è¿™ä¸¤ä¸ªå¥å­ï¼š\n",
    "- \"è‹¹æœå¾ˆå¥½åƒ\" ï¼ˆæ°´æœï¼‰\n",
    "- \"è‹¹æœå‘å¸ƒäº†æ–°æ‰‹æœº\" ï¼ˆå…¬å¸ï¼‰\n",
    "\n",
    "é™æ€ Embedding ä¸­ï¼Œ\"è‹¹æœ\"åªæœ‰ä¸€ä¸ªå›ºå®šå‘é‡ï¼Œæ— æ³•åŒºåˆ†ä¸¤ç§å«ä¹‰ï¼\n",
    "\n",
    "### è§£å†³æ–¹æ¡ˆï¼šSelf-Attention\n",
    "\n",
    "è®©æ¯ä¸ªè¯\"çœ‹çœ‹\"å‘¨å›´çš„è¯ï¼Œç„¶åæ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´è‡ªå·±çš„è¡¨ç¤ºã€‚\n",
    "\n",
    "```\n",
    "é™æ€ Embedding: è‹¹æœ â†’ [å›ºå®šå‘é‡]\n",
    "åŠ¨æ€ Attention: è‹¹æœ + ä¸Šä¸‹æ–‡ â†’ [åŠ¨æ€å‘é‡]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d13bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. æ‰‹å†™ Self-Attention\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "- **Q (Query)**: æˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ\n",
    "- **K (Key)**: æˆ‘æœ‰ä»€ä¹ˆï¼Ÿ\n",
    "- **V (Value)**: æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "ç®€å•ç†è§£ï¼š\n",
    "1. Q å’Œ K ç‚¹ç§¯ â†’ è®¡ç®—\"å…³æ³¨åº¦\"\n",
    "2. Softmax â†’ è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\n",
    "3. ç”¨æ¦‚ç‡åŠ æƒ V â†’ å¾—åˆ°æœ€ç»ˆè¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "am2enpbcdki",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q/K/V ç›´è§‚ç†è§£ï¼šå›¾ä¹¦é¦†æ¯”å–»\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Microsoft YaHei\", \"SimHei\", \"Noto Sans CJK SC\", \"Arial Unicode MS\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import numpy as np\n",
    "\n",
    "def visualize_qkv_intuition():\n",
    "    \"\"\"\n",
    "    ç”¨å›¾ä¹¦é¦†æ¯”å–»æ¥è§£é‡Š Q/K/V\n",
    "    - Query: ä½ æƒ³æ‰¾ä»€ä¹ˆä¹¦ï¼Ÿ\n",
    "    - Key: ä¹¦æ¶ä¸Šä¹¦çš„æ ‡ç­¾\n",
    "    - Value: ä¹¦çš„å®é™…å†…å®¹\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "    # ===== Scene 1: Library analogy =====\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('å›¾ä¹¦é¦†æ¯”å–»', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "    # Person (Query)\n",
    "    ax.add_patch(plt.Circle((2, 5), 0.8, color='#FF6B6B', alpha=0.8))\n",
    "    ax.text(2, 5, 'Q', fontsize=20, ha='center', va='center')\n",
    "    ax.text(2, 3.5, 'Query\\n\"I want ML books\"', fontsize=9, ha='center',\n",
    "           bbox=dict(boxstyle='round', facecolor='#FFE5E5', edgecolor='#FF6B6B'))\n",
    "\n",
    "    # Shelf (Keys)\n",
    "    books = [\n",
    "        (6, 8, 'æ·±åº¦å­¦ä¹ ', 0.9),\n",
    "        (6, 6.5, 'æœºå™¨å­¦ä¹ å…¥é—¨', 0.95),\n",
    "        (6, 5, 'Pythonç¼–ç¨‹', 0.3),\n",
    "        (6, 3.5, 'ç»Ÿè®¡å­¦åŸºç¡€', 0.5),\n",
    "    ]\n",
    "\n",
    "    for x, y, label, score in books:\n",
    "        color_intensity = 0.3 + 0.7 * score\n",
    "        ax.add_patch(FancyBboxPatch((x-0.8, y-0.4), 3.5, 0.8,\n",
    "                                    boxstyle='round,pad=0.1',\n",
    "                                    facecolor=(78/255, 205/255, 196/255, color_intensity),\n",
    "                                    edgecolor='#333', linewidth=1.5))\n",
    "        ax.text(x+0.9, y, f'{label}\\nRelevance: {score:.0%}', fontsize=8, ha='center', va='center')\n",
    "\n",
    "    ax.text(7.5, 9.3, 'Keys (ä¹¦çš„æ ‡ç­¾)', fontsize=10, ha='center', fontweight='bold')\n",
    "\n",
    "    # Arrows\n",
    "    ax.annotate('', xy=(5, 5.5), xytext=(3, 5),\n",
    "               arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "    ax.text(4, 5.8, 'åŒ¹é…åº¦è®¡ç®—', fontsize=8, ha='center')\n",
    "\n",
    "    # ===== Scene 2: Math steps =====\n",
    "    ax = axes[1]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('æ•°å­¦è¿‡ç¨‹', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "    # Step 1\n",
    "    ax.text(5, 9, 'Step 1: Q Ã— K^T = ç›¸ä¼¼åº¦åˆ†æ•°', fontsize=11, ha='center', fontweight='bold')\n",
    "    ax.add_patch(FancyBboxPatch((1, 7.2), 2.5, 1.2, boxstyle='round', facecolor='#FF6B6B', alpha=0.3))\n",
    "    ax.text(2.25, 7.8, 'Q = [0.5, 0.8]', fontsize=9, ha='center', family='monospace')\n",
    "\n",
    "    ax.text(4, 7.8, 'Ã—', fontsize=14, ha='center')\n",
    "\n",
    "    ax.add_patch(FancyBboxPatch((4.5, 7.2), 2.5, 1.2, boxstyle='round', facecolor='#4ECDC4', alpha=0.3))\n",
    "    ax.text(5.75, 7.8, 'K = [[...],\\n     [...]]', fontsize=8, ha='center', family='monospace')\n",
    "\n",
    "    ax.text(7.5, 7.8, '=', fontsize=14, ha='center')\n",
    "\n",
    "    ax.add_patch(FancyBboxPatch((8, 7.2), 1.5, 1.2, boxstyle='round', facecolor='#FFEAA7', alpha=0.5))\n",
    "    ax.text(8.75, 7.8, '[3, 5, 1]', fontsize=9, ha='center', family='monospace')\n",
    "\n",
    "    # Step 2\n",
    "    ax.text(5, 5.5, 'Step 2: Softmax = å½’ä¸€åŒ–ä¸ºæ¦‚ç‡', fontsize=11, ha='center', fontweight='bold')\n",
    "    ax.add_patch(FancyBboxPatch((2, 4), 6, 1, boxstyle='round', facecolor='#DDA0DD', alpha=0.3))\n",
    "    ax.text(5, 4.5, 'softmax([3, 5, 1]) = [0.12, 0.87, 0.01]', fontsize=10, ha='center', family='monospace')\n",
    "\n",
    "    # Step 3\n",
    "    ax.text(5, 2.5, 'Step 3: æƒé‡ Ã— V = åŠ æƒè¾“å‡º', fontsize=11, ha='center', fontweight='bold')\n",
    "    ax.add_patch(FancyBboxPatch((1, 0.8), 8, 1.2, boxstyle='round', facecolor='#98D8C8', alpha=0.4))\n",
    "    ax.text(5, 1.4, '0.12Ã—V1 + 0.87Ã—V2 + 0.01Ã—V3 = weighted output', fontsize=10, ha='center', family='monospace')\n",
    "\n",
    "    # ===== Scene 3: Key intuition =====\n",
    "    ax = axes[2]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('æ ¸å¿ƒæ´å¯Ÿ', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "    insights = [\n",
    "        (5, 8.5, 'Q = current token asks:\\n\"What do I need?\"', '#FF6B6B'),\n",
    "        (5, 6.5, 'K = all tokens answer:\\n\"What can I provide?\"', '#4ECDC4'),\n",
    "        (5, 4.5, 'V = content to provide:\\n\"Here is my info\"', '#9B59B6'),\n",
    "        (5, 2.5, 'Output = weighted mix\\nby relevance', '#27AE60'),\n",
    "    ]\n",
    "\n",
    "    for x, y, text, color in insights:\n",
    "        ax.add_patch(FancyBboxPatch((1, y-0.8), 8, 1.6, boxstyle='round,pad=0.3',\n",
    "                                    facecolor=color, alpha=0.2, edgecolor=color, linewidth=2))\n",
    "        ax.text(x, y, text, fontsize=10, ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nKey takeaways:\")\n",
    "    print(\"   Q*K^T -> who to attend to (attention scores)\")\n",
    "    print(\"   Softmax -> how much to attend (probabilities)\")\n",
    "    print(\"   *V -> mix values by attention (weighted sum)\")\n",
    "\n",
    "visualize_qkv_intuition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention_alammar_img",
   "metadata": {},
   "source": [
    "![attention_alammar](attention_alammar.png)\n",
    "\n",
    "æ¥æºï¼šAlammar çš„å¯è§†åŒ–ç¤ºæ„å›¾ https://jalammar.github.io/illustrated-transformer/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "score_to_z_explainer",
   "metadata": {},
   "source": [
    "### ä» scores åˆ° z çš„è®¡ç®—è¿‡ç¨‹\n",
    "\n",
    "è®¾è¾“å…¥ `x` å½¢çŠ¶ä¸º `[batch, seq_len, d_model]`ï¼Œè¿™é‡Œæœ€ç®€ç‰ˆæœ¬å– `Q=K=V=x`ã€‚ä¸‹é¢æŒ‰æ•°å­—æ­¥éª¤ç»˜åˆ¶æ•´ä¸ªè®¡ç®—é“¾è·¯ï¼š\n",
    "\n",
    "1. **è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆscoresï¼‰**\n",
    "   $$\\text{scores} = Q K^T$$\n",
    "   å¯¹æ¯ä¸ª batchï¼Œæ¯ä¸ª query ä½ç½® $i$ ä¸æ‰€æœ‰ key ä½ç½® $j$ åšç‚¹ç§¯ï¼š\n",
    "   $$\\text{scores}_{i,j} = \\sum_{d=1}^{d_k} Q_{i,d} K_{j,d}$$\n",
    "   å¾—åˆ°å½¢çŠ¶ `[batch, seq_len, seq_len]`ï¼Œè¡¨ç¤ºæ¯ä¸ª query å¯¹æ‰€æœ‰ key çš„åŸå§‹åŒ¹é…åˆ†ã€‚\n",
    "\n",
    "2. **ç¼©æ”¾ scoresï¼ˆscaled scoresï¼‰**\n",
    "   $$\\text{scores} = \\frac{\\text{scores}}{\\sqrt{d_k}}$$\n",
    "   ç”¨ $\\\\sqrt{d_k}$ è¿›è¡Œå½’ä¸€åŒ–ï¼Œé¿å…ç‚¹ç§¯éšç»´åº¦å¢å¤§è€Œè¿‡åº¦æ”¾å¤§ï¼Œä½¿ softmax æ›´ç¨³å®šã€‚\n",
    "\n",
    "3. **Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡**\n",
    "   $$A = \\text{softmax}(\\text{scores})$$\n",
    "   åœ¨æœ€åä¸€ç»´ï¼ˆkey ç»´åº¦ï¼‰åš softmaxï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæ»¡è¶³å„è¡Œå’Œä¸º 1ã€‚\n",
    "\n",
    "4. **åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡ºï¼ˆzï¼‰**\n",
    "   $$z = A V$$\n",
    "   ç­‰ä»·äºå¯¹æ¯ä¸ª query ä½ç½® $i$ï¼š\n",
    "   $$z_i = \\sum_{j=1}^{seq\\_len} A_{i,j} V_j$$\n",
    "   è¾“å‡º `z` çš„å½¢çŠ¶ä¸º `[batch, seq_len, d_model]`ï¼Œå°±æ˜¯è¿æ¥ä¸Šä¸‹æ–‡åçš„æ–°è¡¨ç¤ºã€‚\n",
    "\n",
    "ä»£ç å¯¹ç…§ï¼š`scores -> attention_weights -> output` å°±å¯¹åº”ä¸Šé¢çš„ `scores -> A -> z`ã€‚\n",
    "\n",
    "### æŒ‰å›¾ä¸­çš„ä¾‹å­ï¼ˆinput -> Q/K/V -> zï¼‰\n",
    "\n",
    "è®¾ d_k=64ï¼Œå› æ­¤ $\\sqrt{d_k}=8$ã€‚ä¸ºäº†å¯¹é½å›¾é‡Œçš„åˆ†æ•°ï¼Œæ„é€ ä¸€ä¸ªç®€å•çš„æ•°å€¼è¾“å…¥ï¼Œå¹¶å– $W_q=W_k=W_v=I$ï¼ˆæ‰€ä»¥ Q=K=V=xï¼‰ã€‚\n",
    "\n",
    "**0) input xï¼ˆ64 ç»´å‘é‡ï¼Œæ‰€æœ‰ç»´åº¦å–ç›¸åŒæ•°å€¼ï¼‰**\n",
    "```\n",
    "x1 = [1.323, 1.323, ..., 1.323]  (64 ç»´)\n",
    "x2 = [1.134, 1.134, ..., 1.134]  (64 ç»´)\n",
    "```\n",
    "\n",
    "**1) Q, K, V**\n",
    "```\n",
    "q1 = k1 = v1 = x1\n",
    "q2 = k2 = v2 = x2\n",
    "```\n",
    "\n",
    "**2) scores = Q K^Tï¼ˆå›¾ä¸­ç»™çš„ 112 å’Œ 96ï¼‰**\n",
    "```\n",
    "q1 Â· k1 = 64 * 1.323 * 1.323 â‰ˆ 112\n",
    "q1 Â· k2 = 64 * 1.323 * 1.134 â‰ˆ 96\n",
    "```\n",
    "\n",
    "**3) scaled scores = scores / sqrt(d_k)**\n",
    "```\n",
    "[112, 96] / 8 = [14, 12]\n",
    "```\n",
    "\n",
    "**4) attention weights = softmax([14, 12])**\n",
    "```\n",
    "exp(14)=1.202e6, exp(12)=1.627e5, sum=1.365e6\n",
    "A = [0.881, 0.119]  (â‰ˆ [0.88, 0.12])\n",
    "```\n",
    "\n",
    "**5) z1 = A Vï¼ˆå¯¹åº”å›¾é‡Œçš„ z1ï¼‰**\n",
    "```\n",
    "z1 = 0.881*v1 + 0.119*v2\n",
    "æ¯ä¸€ç»´ = 0.881*1.323 + 0.119*1.134 â‰ˆ 1.300\n",
    "z1 = [1.300, 1.300, ..., 1.300]  (64 ç»´)\n",
    "```\n",
    "\n",
    "å›¾ä¸­å±•ç¤ºçš„æ˜¯ q1 å¯¹ k1/k2 çš„è®¡ç®—ï¼Œq2 åŒç†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9760428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(x, d_k):\n",
    "    \"\"\"\n",
    "    æœ€ç®€å•çš„ Self-Attention å®ç°\n",
    "    \n",
    "    x: [batch, seq_len, d_model] è¾“å…¥åºåˆ—\n",
    "    d_k: æ³¨æ„åŠ›ç»´åº¦\n",
    "    \n",
    "    è¿”å›: [batch, seq_len, d_model] åŠ æƒåçš„è¾“å‡º\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    \n",
    "    # ä¸ºç®€å•èµ·è§ï¼ŒQ=K=V=xï¼ˆå®é™…ä¸­ä¼šæœ‰æŠ•å½±çŸ©é˜µï¼‰\n",
    "    Q = x  # [batch, seq_len, d_model]\n",
    "    K = x\n",
    "    V = x\n",
    "    \n",
    "    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, seq_len, seq_len]\n",
    "    \n",
    "    # 2. ç¼©æ”¾ï¼ˆé˜²æ­¢ç‚¹ç§¯è¿‡å¤§ï¼‰\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # 3. Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡\n",
    "    attention_weights = F.softmax(scores, dim=-1)  # [batch, seq_len, seq_len]\n",
    "    \n",
    "    # 4. åŠ æƒæ±‚å’Œ\n",
    "    output = torch.matmul(attention_weights, V)  # [batch, seq_len, d_model]\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# æµ‹è¯•\n",
    "batch_size, seq_len, d_model = 1, 5, 8\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = self_attention(x, d_k=d_model)\n",
    "\n",
    "print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "print(f\"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aaf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(weights[0].detach().numpy(), \n",
    "            annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=[f'pos_{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'pos_{i}' for i in range(seq_len)])\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.show()\n",
    "\n",
    "print(\"æ¯è¡Œå’Œä¸º1ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰:\", weights[0].sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beaf7ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. æ ‡å‡† Self-Attention å®ç°ï¼ˆå¸¦æŠ•å½±çŸ©é˜µï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00662f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    æ ‡å‡†çš„ Self-Attention å±‚\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_k=None):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k or d_model\n",
    "        \n",
    "        # Q, K, V æŠ•å½±çŸ©é˜µ\n",
    "        self.W_q = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        self.W_o = nn.Linear(self.d_k, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        \n",
    "        # æŠ•å½±å¾—åˆ° Q, K, V\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_k]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        # å¯é€‰: åº”ç”¨ mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # åŠ æƒæ±‚å’Œ\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# åˆ›å»ºå¹¶æµ‹è¯•\n",
    "attention = SelfAttention(d_model=32, d_k=16)\n",
    "x = torch.randn(2, 10, 32)  # 2ä¸ªå¥å­ï¼Œæ¯å¥10ä¸ªè¯ï¼Œæ¯è¯32ç»´\n",
    "\n",
    "output, weights = attention(x)\n",
    "print(f\"è¾“å…¥: {x.shape}\")\n",
    "print(f\"è¾“å‡º: {output.shape}\")\n",
    "print(f\"å‚æ•°é‡: {sum(p.numel() for p in attention.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab02a09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Masked Attentionï¼šé˜²æ­¢\"å·çœ‹ç­”æ¡ˆ\"\n",
    "\n",
    "åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œç”Ÿæˆç¬¬ t ä¸ªè¯æ—¶ï¼Œåªèƒ½çœ‹åˆ°å‰é¢çš„è¯ï¼ˆ1 åˆ° t-1ï¼‰ï¼Œä¸èƒ½çœ‹åˆ°åé¢çš„è¯ã€‚\n",
    "\n",
    "è¿™å°±éœ€è¦ **Causal Maskï¼ˆå› æœæ©ç ï¼‰**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1561402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºä¸‹ä¸‰è§’æ©ç çŸ©é˜µ\n",
    "    1 = å¯ä»¥çœ‹\n",
    "    0 = ä¸èƒ½çœ‹ï¼ˆä¼šè¢«è®¾ä¸º -infï¼‰\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "# å¯è§†åŒ– mask\n",
    "seq_len = 6\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mask.numpy(), annot=True, fmt='.0f', cmap='Greens',\n",
    "            xticklabels=[f't={i}' for i in range(seq_len)],\n",
    "            yticklabels=[f't={i}' for i in range(seq_len)])\n",
    "plt.xlabel('Key (è¢«çœ‹çš„ä½ç½®)')\n",
    "plt.ylabel('Query (å½“å‰ä½ç½®)')\n",
    "plt.title('Causal Mask (1=å¯è§, 0=ä¸å¯è§)')\n",
    "plt.show()\n",
    "\n",
    "print(\"è§£é‡Šï¼š\")\n",
    "print(\"  t=0 æ—¶ï¼Œåªèƒ½çœ‹è‡ªå·±\")\n",
    "print(\"  t=1 æ—¶ï¼Œèƒ½çœ‹ t=0 å’Œ t=1\")\n",
    "print(\"  t=5 æ—¶ï¼Œèƒ½çœ‹æ‰€æœ‰ä½ç½®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a05332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åº”ç”¨ Causal Mask çš„æ•ˆæœ\n",
    "x = torch.randn(1, 6, 32)\n",
    "attention = SelfAttention(d_model=32, d_k=16)\n",
    "\n",
    "# æ—  mask\n",
    "output_no_mask, weights_no_mask = attention(x)\n",
    "\n",
    "# æœ‰ mask\n",
    "mask = create_causal_mask(6).unsqueeze(0)  # [1, 6, 6]\n",
    "output_masked, weights_masked = attention(x, mask)\n",
    "\n",
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(weights_no_mask[0].detach().numpy(), ax=axes[0],\n",
    "            annot=True, fmt='.2f', cmap='Blues')\n",
    "axes[0].set_title('Without Mask (åŒå‘Attention)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "sns.heatmap(weights_masked[0].detach().numpy(), ax=axes[1],\n",
    "            annot=True, fmt='.2f', cmap='Blues')\n",
    "axes[1].set_title('With Causal Mask (å•å‘Attention)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"è§‚å¯Ÿï¼šæœ‰ Mask æ—¶ï¼Œä¸Šä¸‰è§’éƒ¨åˆ†æƒé‡ä¸º 0ï¼ˆä¸èƒ½å·çœ‹æœªæ¥ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb95ccb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multi-Head Attentionï¼šå¤šå¤´æ³¨æ„åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ccd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    å¤šå¤´æ³¨æ„åŠ›ï¼šå¤šä¸ª Attention å¹¶è¡Œï¼Œç„¶åæ‹¼æ¥\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 1. çº¿æ€§æŠ•å½±\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # 2. åˆ†å‰²æˆå¤šä¸ªå¤´\n",
    "        # [batch, seq_len, d_model] -> [batch, n_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3. è®¡ç®—æ³¨æ„åŠ›\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 4. æ‹¼æ¥å¤šä¸ªå¤´\n",
    "        # [batch, n_heads, seq_len, d_k] -> [batch, seq_len, d_model]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 5. è¾“å‡ºæŠ•å½±\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# æµ‹è¯•\n",
    "mha = MultiHeadAttention(d_model=64, n_heads=8)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"è¾“å…¥: {x.shape}\")\n",
    "print(f\"è¾“å‡º: {output.shape}\")\n",
    "print(f\"æ³¨æ„åŠ›æƒé‡: {weights.shape}  (8ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´10x10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d478ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ä¸åŒå¤´çš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    sns.heatmap(weights[0, i].detach().numpy(), ax=axes[i],\n",
    "                cmap='Blues', cbar=False)\n",
    "    axes[i].set_title(f'Head {i+1}')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: 8ä¸ªå¤´çš„æ³¨æ„åŠ›æ¨¡å¼', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"è§‚å¯Ÿï¼šä¸åŒçš„å¤´å­¦ä¹ äº†ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62a7e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ç”¨çœŸå®æ–‡æœ¬æ¼”ç¤º Attention\n",
    "\n",
    "è®©æˆ‘ä»¬çœ‹çœ‹ Attention åœ¨çœŸå®æ–‡æœ¬ä¸Šçš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34baefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„å¥å­ç¼–ç \n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "seq_len = len(sentence)\n",
    "\n",
    "# å‡è®¾æˆ‘ä»¬æœ‰é¢„è®­ç»ƒçš„è¯å‘é‡\n",
    "d_model = 32\n",
    "embeddings = torch.randn(seq_len, d_model)  # æ¨¡æ‹Ÿ embedding\n",
    "\n",
    "# åº”ç”¨ self-attention\n",
    "attention = SelfAttention(d_model=d_model, d_k=16)\n",
    "output, weights = attention(embeddings.unsqueeze(0))\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(weights[0].detach().numpy(), \n",
    "            annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=sentence,\n",
    "            yticklabels=sentence)\n",
    "plt.xlabel('Key (è¢«å…³æ³¨çš„è¯)')\n",
    "plt.ylabel('Query (å½“å‰è¯)')\n",
    "plt.title('Self-Attention Weights for \"The cat sat on the mat\"')\n",
    "plt.show()\n",
    "\n",
    "print(\"è§£è¯»ï¼š\")\n",
    "print(\"  - æ¯ä¸€è¡Œä»£è¡¨æŸä¸ªè¯å…³æ³¨å…¶ä»–æ‰€æœ‰è¯çš„ç¨‹åº¦\")\n",
    "print(\"  - æƒé‡é«˜è¡¨ç¤ºè¿™ä¸¤ä¸ªè¯å…³ç³»æ›´å¯†åˆ‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c79a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "\n",
    "1. **Self-Attention çš„æœ¬è´¨**\n",
    "   - Q, K, V ä¸‰ä¸ªæŠ•å½±\n",
    "   - é€šè¿‡ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦\n",
    "   - Softmax è½¬æ¢ä¸ºæ¦‚ç‡\n",
    "   - åŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨ç¤º\n",
    "\n",
    "2. **Causal Mask**\n",
    "   - é˜²æ­¢æ¨¡å‹\"å·çœ‹æœªæ¥\"\n",
    "   - ç”¨ä¸‹ä¸‰è§’çŸ©é˜µå®ç°\n",
    "\n",
    "3. **Multi-Head Attention**\n",
    "   - å¤šä¸ª Attention å¤´å¹¶è¡Œ\n",
    "   - æ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„å…³æ³¨æ¨¡å¼\n",
    "   - æœ€åæ‹¼æ¥è¾“å‡º\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e71263",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒ\n",
    "\n",
    "1. **ä¿®æ”¹ d_k**ï¼šå°è¯•ä¸åŒçš„æ³¨æ„åŠ›ç»´åº¦ï¼Œè§‚å¯Ÿæ•ˆæœ\n",
    "2. **å¯è§†åŒ–çœŸå®æ¨¡å‹**ï¼šç”¨ HuggingFace çš„æ¨¡å‹æå–æ³¨æ„åŠ›æƒé‡å¹¶å¯è§†åŒ–\n",
    "3. **æ€è€ƒ**ï¼šä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšd_kï¼Ÿä¸é™¤ä¼šæ€æ ·ï¼Ÿ\n",
    "4. åŒæ ·çš„ï¼Œæˆ‘ä»¬æƒ³è®¡ç®—q2çš„åˆ†æ•°å’Œz2å¦‚ä½•è®¡ç®—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874adf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ ç©ºé—´\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
