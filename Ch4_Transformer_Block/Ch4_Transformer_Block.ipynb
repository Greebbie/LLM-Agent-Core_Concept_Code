{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "64ba65a6",
                              "metadata": {},
                              "source": [
                                        "# Ch4 | Transformer Blockï¼šæ­å»ºç°ä»£ LLM ç§¯æœ¨\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "**ç›®æ ‡ï¼š** ç†è§£ Transformer çš„æ ¸å¿ƒç»„ä»¶\n",
                                        "\n",
                                        "**æ ¸å¿ƒé—®é¢˜ï¼š** å¦‚ä½•æŠŠ Attention å’Œ MLP ç»„åˆæˆå¼ºå¤§çš„æ¨¡å‹ï¼Ÿ\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## æœ¬ç« å†…å®¹\n",
                                        "\n",
                                        "1. **æ®‹å·®è¿æ¥ (Residual)**ï¼šæ¢¯åº¦çš„é«˜é€Ÿå…¬è·¯\n",
                                        "2. **LayerNorm**ï¼šæ•°æ®çš„å½’ä¸€åŒ–å¤„ç†\n",
                                        "3. **FFN/MLP**ï¼šæ¨¡å‹çš„\"è®°å¿†åŒº\"\n",
                                        "4. **å®Œæ•´ Block**ï¼šç»„è£… Transformer ç§¯æœ¨"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "prereq_ch4_001",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## ğŸ“ å‰ç½®çŸ¥è¯†ï¼šä» Attention åˆ°å®Œæ•´çš„ Transformer\n",
                                        "\n",
                                        "### å›é¡¾ï¼šSelf-Attention èƒ½åšä»€ä¹ˆï¼Ÿ\n",
                                        "\n",
                                        "ä¸Šä¸€ç« æˆ‘ä»¬å­¦ä¹ äº† Self-Attentionï¼š\n",
                                        "\n",
                                        "```\n",
                                        "è¾“å…¥åºåˆ— â†’ Self-Attention â†’ æ¯ä¸ªè¯éƒ½\"çœ‹è¿‡\"å…¶ä»–è¯çš„è¾“å‡º\n",
                                        "```\n",
                                        "\n",
                                        "ä½†åªæœ‰ Attention æ˜¯ä¸å¤Ÿçš„ï¼è¿˜éœ€è¦ï¼š\n",
                                        "\n",
                                        "1. **éçº¿æ€§å˜æ¢**ï¼šAttention æœ¬è´¨æ˜¯åŠ æƒæ±‚å’Œï¼ˆçº¿æ€§çš„ï¼‰ï¼Œéœ€è¦éçº¿æ€§æ‰èƒ½å­¦ä¹ å¤æ‚æ¨¡å¼\n",
                                        "2. **ç¨³å®šè®­ç»ƒ**ï¼šæ·±å±‚ç½‘ç»œå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸\n",
                                        "3. **ä¿¡æ¯ä¿ç•™**ï¼šæ·±å±‚ç½‘ç»œå¯èƒ½ä¸¢å¤±åŸå§‹ä¿¡æ¯\n",
                                        "\n",
                                        "### Transformer Block çš„ç»„ä»¶\n",
                                        "\n",
                                        "ä¸€ä¸ªå®Œæ•´çš„ Transformer Block åŒ…å«ï¼š\n",
                                        "\n",
                                        "```\n",
                                        "è¾“å…¥\n",
                                        "  â†“\n",
                                        "LayerNorm â†’ å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ\n",
                                        "  â†“\n",
                                        "Self-Attention â†’ å­¦ä¹ è¯ä¹‹é—´çš„å…³ç³»\n",
                                        "  â†“\n",
                                        "æ®‹å·®è¿æ¥ â†’ ä¿ç•™åŸå§‹ä¿¡æ¯\n",
                                        "  â†“\n",
                                        "LayerNorm â†’ å†æ¬¡å½’ä¸€åŒ–\n",
                                        "  â†“\n",
                                        "FFN (å‰é¦ˆç½‘ç»œ) â†’ å¼•å…¥éçº¿æ€§\n",
                                        "  â†“\n",
                                        "æ®‹å·®è¿æ¥ â†’ å†æ¬¡ä¿ç•™ä¿¡æ¯\n",
                                        "  â†“\n",
                                        "è¾“å‡º\n",
                                        "```\n",
                                        "\n",
                                        "### ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ\n",
                                        "\n",
                                        "**é—®é¢˜**ï¼šæ·±å±‚ç½‘ç»œï¼ˆæ¯”å¦‚100å±‚ï¼‰è®­ç»ƒæ—¶ï¼Œæ¢¯åº¦ä¼šé€å±‚å˜å°ï¼Œæœ€ç»ˆæ¶ˆå¤±\n",
                                        "\n",
                                        "**è§£å†³**ï¼šæ®‹å·®è¿æ¥è®©æ¢¯åº¦æœ‰\"ç›´é€šé“\"\n",
                                        "\n",
                                        "```python\n",
                                        "# æ²¡æœ‰æ®‹å·®ï¼šæ¢¯åº¦è¦ç»è¿‡æ¯ä¸€å±‚å˜æ¢\n",
                                        "output = layer(x)\n",
                                        "\n",
                                        "# æœ‰æ®‹å·®ï¼šæ¢¯åº¦å¯ä»¥ç›´æ¥æµè¿‡\n",
                                        "output = layer(x) + x  # è¿™ä¸ª +x å°±æ˜¯æ®‹å·®è¿æ¥\n",
                                        "```\n",
                                        "\n",
                                        "### æœ¬ç« ç›®æ ‡\n",
                                        "\n",
                                        "- ç†è§£æ®‹å·®è¿æ¥å¦‚ä½•è§£å†³æ¢¯åº¦æ¶ˆå¤±\n",
                                        "- ç†è§£ LayerNorm çš„ä½œç”¨\n",
                                        "- ç†è§£ FFN/MLP çš„ä½œç”¨\n",
                                        "- ç»„è£…å®Œæ•´çš„ Transformer Block"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "7ad8ad1f",
                              "metadata": {},
                              "source": [
                                        "## 0. ç¯å¢ƒå‡†å¤‡"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "c2072d52",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "\n",
                                        "torch.manual_seed(42)\n",
                                        "print(f\"PyTorch version: {torch.__version__}\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "7c0a2e18",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 1. æ®‹å·®è¿æ¥ (Residual Connection)\n",
                                        "\n",
                                        "### ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ\n",
                                        "\n",
                                        "æ·±åº¦ç½‘ç»œçš„é—®é¢˜ï¼š\n",
                                        "- æ¢¯åº¦æ¶ˆå¤±ï¼šæ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶è¶Šæ¥è¶Šå°\n",
                                        "- é€€åŒ–é—®é¢˜ï¼šç½‘ç»œè¶Šæ·±ï¼Œè®­ç»ƒæ•ˆæœåè€Œå˜å·®\n",
                                        "\n",
                                        "### è§£å†³æ–¹æ¡ˆ\n",
                                        "\n",
                                        "```\n",
                                        "output = layer(x) + x  # æ®‹å·®è¿æ¥\n",
                                        "```\n",
                                        "\n",
                                        "è¿™æ ·æ¢¯åº¦å¯ä»¥\"ç›´é€š\"åˆ°åº•å±‚ï¼Œä¸ç»è¿‡ä»»ä½•å˜æ¢ï¼"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "cb314eff",
                              "metadata": {},
                              "source": [
                                        "![resnet_kaiming](resnet_kaiming.png)\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "23b36eb4",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class ResidualBlock(nn.Module):\n",
                                        "    \"\"\"æ¼”ç¤ºæ®‹å·®è¿æ¥\"\"\"\n",
                                        "    def __init__(self, d_model):\n",
                                        "        super().__init__()\n",
                                        "        self.linear = nn.Linear(d_model, d_model)\n",
                                        "    \n",
                                        "    def forward(self, x):\n",
                                        "        # æ²¡æœ‰æ®‹å·®è¿æ¥\n",
                                        "        # return self.linear(x)\n",
                                        "        \n",
                                        "        # æœ‰æ®‹å·®è¿æ¥\n",
                                        "        return self.linear(x) + x  # å…³é”®ï¼\n",
                                        "\n",
                                        "# å¯¹æ¯”æ¢¯åº¦æµåŠ¨\n",
                                        "d_model = 64\n",
                                        "x = torch.randn(1, 10, d_model, requires_grad=True)\n",
                                        "\n",
                                        "# å †å å¤šå±‚\n",
                                        "layers_no_residual = nn.Sequential(*[nn.Linear(d_model, d_model) for _ in range(10)])\n",
                                        "layers_with_residual = nn.ModuleList([ResidualBlock(d_model) for _ in range(10)])\n",
                                        "\n",
                                        "# æ— æ®‹å·®\n",
                                        "out_no_res = x.clone()\n",
                                        "for layer in layers_no_residual:\n",
                                        "    out_no_res = layer(out_no_res)\n",
                                        "loss_no_res = out_no_res.sum()\n",
                                        "loss_no_res.backward()\n",
                                        "\n",
                                        "print(\"æ— æ®‹å·®è¿æ¥:\")\n",
                                        "print(f\"  è¾“å‡ºèŒƒå›´: [{out_no_res.min().item():.2f}, {out_no_res.max().item():.2f}]\")\n",
                                        "\n",
                                        "# æœ‰æ®‹å·®\n",
                                        "x2 = torch.randn(1, 10, d_model, requires_grad=True)\n",
                                        "out_res = x2.clone()\n",
                                        "for layer in layers_with_residual:\n",
                                        "    out_res = layer(out_res)\n",
                                        "loss_res = out_res.sum()\n",
                                        "loss_res.backward()\n",
                                        "\n",
                                        "print(\"\\næœ‰æ®‹å·®è¿æ¥:\")\n",
                                        "print(f\"  è¾“å‡ºèŒƒå›´: [{out_res.min().item():.2f}, {out_res.max().item():.2f}]\")\n",
                                        "print(\"\\næ®‹å·®è¿æ¥è®©ä¿¡å·æ›´ç¨³å®šï¼\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "3a1f42c4",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ–æ®‹å·®è¿æ¥\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                                        "\n",
                                        "n_layers = 30\n",
                                        "decay = 0.7\n",
                                        "residual_gain = 0.1\n",
                                        "x = np.linspace(-2, 2, 100)\n",
                                        "\n",
                                        "# æ— æ®‹å·®ï¼šä¿¡å·å¯èƒ½çˆ†ç‚¸æˆ–æ¶ˆå¤±\n",
                                        "for i in range(n_layers):\n",
                                        "    y = x * (decay ** i)  # æ¯å±‚ç¼©å°\n",
                                        "    axes[0].plot(x, y, label=f'Layer {i+1}', alpha=0.6)\n",
                                        "axes[0].set_title('Without Residual: Signal Vanishes')\n",
                                        "axes[0].set_xlabel('Input')\n",
                                        "axes[0].set_ylabel('Output')\n",
                                        "axes[0].legend(ncol=2, fontsize=8)\n",
                                        "axes[0].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "# æœ‰æ®‹å·®ï¼šä¿¡å·ä¿æŒ\n",
                                        "for i in range(n_layers):\n",
                                        "    y = x * (1 + residual_gain * i)  # æ®‹å·®å åŠ \n",
                                        "    axes[1].plot(x, y, label=f'Layer {i+1}', alpha=0.6)\n",
                                        "axes[1].set_title('With Residual: Signal Preserved')\n",
                                        "axes[1].set_xlabel('Input')\n",
                                        "axes[1].set_ylabel('Output')\n",
                                        "axes[1].legend(ncol=2, fontsize=8)\n",
                                        "axes[1].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "max_abs = np.max(np.abs(x)) * (1 + residual_gain * (n_layers - 1))\n",
                                        "for ax in axes:\n",
                                        "    ax.set_ylim(-max_abs, max_abs)\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "116d4780",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 2. Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰\n",
                                        "\n",
                                        "### ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ\n",
                                        "\n",
                                        "- è®©æ•°æ®åˆ†å¸ƒæ›´ç¨³å®š\n",
                                        "- åŠ é€Ÿè®­ç»ƒæ”¶æ•›\n",
                                        "- å‡å°‘å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿæ€§\n",
                                        "\n",
                                        "### LayerNorm vs BatchNorm\n",
                                        "\n",
                                        "```\n",
                                        "BatchNorm: åœ¨ batch ç»´åº¦å½’ä¸€åŒ–ï¼ˆéœ€è¦å¤§ batchï¼‰\n",
                                        "LayerNorm: åœ¨ feature ç»´åº¦å½’ä¸€åŒ–ï¼ˆé€‚åˆåºåˆ—æ¨¡å‹ï¼‰\n",
                                        "```\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "fc1e5149",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class LayerNorm(nn.Module):\n",
                                        "    \"\"\"æ‰‹å†™ LayerNorm\"\"\"\n",
                                        "    def __init__(self, d_model, eps=1e-5):\n",
                                        "        super().__init__()\n",
                                        "        self.eps = eps\n",
                                        "        # å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°\n",
                                        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
                                        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
                                        "    \n",
                                        "    def forward(self, x):\n",
                                        "        # x: [batch, seq_len, d_model]\n",
                                        "        \n",
                                        "        # åœ¨æœ€åä¸€ä¸ªç»´åº¦è®¡ç®—å‡å€¼å’Œæ–¹å·®\n",
                                        "        mean = x.mean(dim=-1, keepdim=True)\n",
                                        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
                                        "        \n",
                                        "        # å½’ä¸€åŒ–\n",
                                        "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
                                        "        \n",
                                        "        # ç¼©æ”¾å’Œåç§»\n",
                                        "        return self.gamma * x_norm + self.beta\n",
                                        "\n",
                                        "# æµ‹è¯•\n",
                                        "x = torch.randn(2, 5, 64) * 10 + 5  # å‡å€¼ä¸ä¸º0ï¼Œæ–¹å·®ä¸ä¸º1\n",
                                        "\n",
                                        "layer_norm = LayerNorm(64)\n",
                                        "x_normed = layer_norm(x)\n",
                                        "\n",
                                        "print(\"å½’ä¸€åŒ–å‰:\")\n",
                                        "print(f\"  å‡å€¼: {x.mean(dim=-1)[0]}\")\n",
                                        "print(f\"  æ–¹å·®: {x.var(dim=-1)[0]}\")\n",
                                        "\n",
                                        "print(\"\\nå½’ä¸€åŒ–å:\")\n",
                                        "print(f\"  å‡å€¼: {x_normed.mean(dim=-1)[0]}\")  # æ¥è¿‘0\n",
                                        "print(f\"  æ–¹å·®: {x_normed.var(dim=-1)[0]}\")   # æ¥è¿‘1"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "7631936b",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ– LayerNorm æ•ˆæœ\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                                        "\n",
                                        "raw_vals = x.detach().numpy().ravel()\n",
                                        "norm_vals = x_normed.detach().numpy().ravel()\n",
                                        "raw_mean = raw_vals.mean()\n",
                                        "raw_std = raw_vals.std()\n",
                                        "norm_mean = norm_vals.mean()\n",
                                        "norm_std = norm_vals.std()\n",
                                        "raw_min, raw_max = raw_vals.min(), raw_vals.max()\n",
                                        "norm_min, norm_max = norm_vals.min(), norm_vals.max()\n",
                                        "\n",
                                        "# å½’ä¸€åŒ–å‰çš„åˆ†å¸ƒ\n",
                                        "axes[0].hist(raw_vals, bins=50, alpha=0.7, color='blue', range=(raw_min, raw_max))\n",
                                        "axes[0].set_title(f'Before LayerNorm (mean={raw_mean:.2f}, std={raw_std:.2f})')\n",
                                        "axes[0].set_xlabel('Value')\n",
                                        "axes[0].set_ylabel('Count')\n",
                                        "axes[0].axvline(x=raw_mean, color='red', linestyle='--', label='Mean')\n",
                                        "\n",
                                        "# å½’ä¸€åŒ–åçš„åˆ†å¸ƒ\n",
                                        "axes[1].hist(norm_vals, bins=50, alpha=0.7, color='green', range=(norm_min, norm_max))\n",
                                        "axes[1].set_title(f'After LayerNorm (mean={norm_mean:.2f}, std={norm_std:.2f})')\n",
                                        "axes[1].set_xlabel('Value')\n",
                                        "axes[1].set_ylabel('Count')\n",
                                        "axes[1].axvline(x=norm_mean, color='red', linestyle='--', label='Mean')\n",
                                        "\n",
                                        "axes[0].set_xlim(raw_min, raw_max)\n",
                                        "axes[1].set_xlim(norm_min, norm_max)\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"LayerNorm è®©æ•°æ®åˆ†å¸ƒæ ‡å‡†åŒ–ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "77d1875a",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 3. å‰é¦ˆç½‘ç»œï¼ˆFFN/MLPï¼‰\n",
                                        "\n",
                                        "Transformer ä¸­çš„ FFN å°±æ˜¯ä¸€ä¸ªç®€å•çš„ä¸¤å±‚ MLPï¼š\n",
                                        "\n",
                                        "```\n",
                                        "FFN(x) = GELU(xW? + b?)W? + b?\n",
                                        "```\n",
                                        "\n",
                                        "ç‰¹ç‚¹ï¼š\n",
                                        "- å…ˆå‡ç»´ï¼ˆé€šå¸¸ 4xï¼‰\n",
                                        "- æ¿€æ´»å‡½æ•°\n",
                                        "- å†é™ç»´\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "b17aeb97",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class FeedForward(nn.Module):\n",
                                        "    \"\"\"Transformer ä¸­çš„ FFN\"\"\"\n",
                                        "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
                                        "        super().__init__()\n",
                                        "        d_ff = d_ff or 4 * d_model  # é»˜è®¤4å€æ‰©å±•\n",
                                        "        \n",
                                        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
                                        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
                                        "        self.dropout = nn.Dropout(dropout)\n",
                                        "    \n",
                                        "    def forward(self, x):\n",
                                        "        # å‡ç»´ -> æ¿€æ´» -> é™ç»´\n",
                                        "        x = self.linear1(x)\n",
                                        "        x = F.gelu(x)  # ç°ä»£LLMå¸¸ç”¨GELU\n",
                                        "        x = self.dropout(x)\n",
                                        "        x = self.linear2(x)\n",
                                        "        return x\n",
                                        "\n",
                                        "# æµ‹è¯•\n",
                                        "ffn = FeedForward(d_model=64, d_ff=256)\n",
                                        "x = torch.randn(2, 10, 64)\n",
                                        "out = ffn(x)\n",
                                        "\n",
                                        "print(f\"è¾“å…¥: {x.shape}\")\n",
                                        "print(f\"ä¸­é—´å±‚: [2, 10, 256]  (4xæ‰©å±•)\")\n",
                                        "print(f\"è¾“å‡º: {out.shape}\")\n",
                                        "print(f\"\\nå‚æ•°é‡: {sum(p.numel() for p in ffn.parameters()):,}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "75eac114",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ– GELU vs ReLU\n",
                                        "x = np.linspace(-4, 4, 100)\n",
                                        "\n",
                                        "plt.figure(figsize=(10, 5))\n",
                                        "plt.plot(x, np.maximum(0, x), 'b-', linewidth=2, label='ReLU')\n",
                                        "plt.plot(x, x * 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3))), \n",
                                        "         'r-', linewidth=2, label='GELU')\n",
                                        "plt.xlabel('x')\n",
                                        "plt.ylabel('f(x)')\n",
                                        "plt.title('ReLU vs GELU Activation Functions')\n",
                                        "plt.legend()\n",
                                        "plt.grid(True, alpha=0.3)\n",
                                        "plt.axhline(y=0, color='k', linewidth=0.5)\n",
                                        "plt.axvline(x=0, color='k', linewidth=0.5)\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"GELU åœ¨è´Ÿæ•°åŒºåŸŸæœ‰å°çš„è´Ÿå€¼ï¼Œæ¯” ReLU æ›´å¹³æ»‘\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "2fd32e12",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 4. å®Œæ•´çš„ Transformer Block\n",
                                        "\n",
                                        "ç°åœ¨æŠŠæ‰€æœ‰ç»„ä»¶æ‹¼è£…åœ¨ä¸€èµ·ï¼\n",
                                        "\n",
                                        "### Pre-LN vs Post-LNï¼ˆå‰å½’ä¸€åŒ– vs åå½’ä¸€åŒ–ï¼‰\n",
                                        "\n",
                                        "```\n",
                                        "Post-LN (åŸå§‹):  x + LayerNorm(Attention(x))\n",
                                        "Pre-LN (ç°ä»£):   x + Attention(LayerNorm(x))\n",
                                        "```\n",
                                        "\n",
                                        "ç°ä»£ LLMï¼ˆGPT-2/3, LLaMAï¼‰éƒ½ç”¨ Pre-LNï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "204fd097",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class MultiHeadAttention(nn.Module):\n",
                                        "    \"\"\"å¤šå¤´æ³¨æ„åŠ›ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
                                        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
                                        "        super().__init__()\n",
                                        "        assert d_model % n_heads == 0\n",
                                        "        self.n_heads = n_heads\n",
                                        "        self.d_k = d_model // n_heads\n",
                                        "        \n",
                                        "        self.W_qkv = nn.Linear(d_model, 3 * d_model)\n",
                                        "        self.W_o = nn.Linear(d_model, d_model)\n",
                                        "        self.dropout = nn.Dropout(dropout)\n",
                                        "    \n",
                                        "    def forward(self, x, mask=None):\n",
                                        "        B, T, C = x.shape\n",
                                        "        \n",
                                        "        # ä¸€æ¬¡æ€§è®¡ç®— Q, K, V\n",
                                        "        qkv = self.W_qkv(x)\n",
                                        "        q, k, v = qkv.chunk(3, dim=-1)\n",
                                        "        \n",
                                        "        # åˆ†å¤´\n",
                                        "        q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
                                        "        k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
                                        "        v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
                                        "        \n",
                                        "        # æ³¨æ„åŠ›\n",
                                        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
                                        "        if mask is not None:\n",
                                        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
                                        "        attn = F.softmax(scores, dim=-1)\n",
                                        "        attn = self.dropout(attn)\n",
                                        "        \n",
                                        "        # è¾“å‡º\n",
                                        "        out = torch.matmul(attn, v)\n",
                                        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
                                        "        return self.W_o(out)"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "5ad97ffb",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class TransformerBlock(nn.Module):\n",
                                        "    \"\"\"\n",
                                        "    å®Œæ•´çš„ Transformer Block (Pre-LN ç‰ˆæœ¬)\n",
                                        "    \n",
                                        "    ç»“æ„:\n",
                                        "    x -> LayerNorm -> Attention -> + -> LayerNorm -> FFN -> +\n",
                                        "         |__________________________|    |___________________|\n",
                                        "                æ®‹å·®è¿æ¥                       æ®‹å·®è¿æ¥\n",
                                        "    \"\"\"\n",
                                        "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):\n",
                                        "        super().__init__()\n",
                                        "        \n",
                                        "        # Layer Norms\n",
                                        "        self.ln1 = nn.LayerNorm(d_model)\n",
                                        "        self.ln2 = nn.LayerNorm(d_model)\n",
                                        "        \n",
                                        "        # Attention\n",
                                        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
                                        "        \n",
                                        "        # Feed Forward\n",
                                        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
                                        "        \n",
                                        "        # Dropout\n",
                                        "        self.dropout = nn.Dropout(dropout)\n",
                                        "    \n",
                                        "    def forward(self, x, mask=None):\n",
                                        "        # Attention block with residual\n",
                                        "        x = x + self.dropout(self.attention(self.ln1(x), mask))\n",
                                        "        \n",
                                        "        # FFN block with residual\n",
                                        "        x = x + self.dropout(self.ffn(self.ln2(x)))\n",
                                        "        \n",
                                        "        return x\n",
                                        "\n",
                                        "# åˆ›å»ºå¹¶æµ‹è¯•\n",
                                        "block = TransformerBlock(d_model=64, n_heads=8, d_ff=256)\n",
                                        "x = torch.randn(2, 10, 64)\n",
                                        "out = block(x)\n",
                                        "\n",
                                        "print(\"Transformer Block:\")\n",
                                        "print(f\"  è¾“å…¥: {x.shape}\")\n",
                                        "print(f\"  è¾“å‡º: {out.shape}\")\n",
                                        "print(f\"  å‚æ•°é‡: {sum(p.numel() for p in block.parameters()):,}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "7dd984bd",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æ‰“å°æ¨¡å‹ç»“æ„\n",
                                        "print(\"TransformerBlock ç»“æ„:\")\n",
                                        "print(\"=\" * 60)\n",
                                        "for name, module in block.named_children():\n",
                                        "    print(f\"{name}: {module.__class__.__name__}\")\n",
                                        "    if hasattr(module, 'weight'):\n",
                                        "        print(f\"    shape: {module.weight.shape}\")\n",
                                        "print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "add3484a",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 5. å †å å¤šä¸ª Block\n",
                                        "\n",
                                        "çœŸæ­£çš„ Transformer æ˜¯å¤šä¸ª Block å †å ã€‚"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "5308d7a7",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class Transformer(nn.Module):\n",
                                        "    \"\"\"å †å å¤šä¸ª Transformer Block\"\"\"\n",
                                        "    def __init__(self, n_layers, d_model, n_heads, d_ff=None, dropout=0.1):\n",
                                        "        super().__init__()\n",
                                        "        self.blocks = nn.ModuleList([\n",
                                        "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
                                        "            for _ in range(n_layers)\n",
                                        "        ])\n",
                                        "        self.ln_final = nn.LayerNorm(d_model)\n",
                                        "    \n",
                                        "    def forward(self, x, mask=None):\n",
                                        "        for block in self.blocks:\n",
                                        "            x = block(x, mask)\n",
                                        "        return self.ln_final(x)\n",
                                        "\n",
                                        "# åˆ›å»ºä¸€ä¸ª 6 å±‚ Transformer\n",
                                        "model = Transformer(\n",
                                        "    n_layers=6,\n",
                                        "    d_model=64,\n",
                                        "    n_heads=8,\n",
                                        "    d_ff=256\n",
                                        ")\n",
                                        "\n",
                                        "x = torch.randn(2, 10, 64)\n",
                                        "out = model(x)\n",
                                        "\n",
                                        "print(f\"6å±‚ Transformer:\")\n",
                                        "print(f\"  è¾“å…¥: {x.shape}\")\n",
                                        "print(f\"  è¾“å‡º: {out.shape}\")\n",
                                        "print(f\"  æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "737d1841",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ– Transformer ç»“æ„\n",
                                        "fig, ax = plt.subplots(figsize=(10, 12))\n",
                                        "ax.axis('off')\n",
                                        "\n",
                                        "# ç»˜åˆ¶ç»“æ„å›¾\n",
                                        "structure = '''\n",
                                        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                                        "â”‚              Input Embeddings            â”‚\n",
                                        "â”‚              [batch, seq, d_model]       â”‚\n",
                                        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                                        "                     â”‚\n",
                                        "        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
                                        "        â•‘                         â•‘ Ã— N layers\n",
                                        "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
                                        "        â•‘  â”‚    Layer Norm     â”‚  â•‘\n",
                                        "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
                                        "        â•‘            â”‚            â•‘\n",
                                        "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
                                        "        â•‘  â”‚  Multi-Head Attn  â”‚  â•‘\n",
                                        "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
                                        "        â•‘            â”‚            â•‘\n",
                                        "        â•‘      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”      â•‘\n",
                                        "        â•‘      â”‚  Add (âŠ•)  â”‚â—„â”€â”€â”€â”€â”€â•«â”€â”€â”\n",
                                        "        â•‘      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â•‘  â”‚ Residual\n",
                                        "        â•‘            â”‚            â•‘  â”‚\n",
                                        "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘  â”‚\n",
                                        "        â•‘  â”‚    Layer Norm     â”‚  â•‘  â”‚\n",
                                        "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘  â”‚\n",
                                        "        â•‘            â”‚            â•‘  â”‚\n",
                                        "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘  â”‚\n",
                                        "        â•‘  â”‚   Feed Forward    â”‚  â•‘  â”‚\n",
                                        "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘  â”‚\n",
                                        "        â•‘            â”‚            â•‘  â”‚\n",
                                        "        â•‘      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”      â•‘  â”‚\n",
                                        "        â•‘      â”‚  Add (âŠ•)  â”‚â—„â”€â”€â”€â”€â”€â•«â”€â”€â”˜\n",
                                        "        â•‘      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â•‘\n",
                                        "        â•‘            â”‚            â•‘\n",
                                        "        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                                        "                     â”‚\n",
                                        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                                        "        â”‚    Final Layer Norm     â”‚\n",
                                        "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                                        "                     â”‚\n",
                                        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                                        "        â”‚         Output          â”‚\n",
                                        "        â”‚    [batch, seq, d_model]â”‚\n",
                                        "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                                        "'''\n",
                                        "ax.text(0.5, 0.5, structure, fontsize=10, family='monospace',\n",
                                        "        ha='center', va='center', transform=ax.transAxes)\n",
                                        "plt.title('Transformer Architecture (Pre-LN)', fontsize=14, fontweight='bold')\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "315bc4e4",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## æœ¬ç« æ€»ç»“\n",
                                        "\n",
                                        "\n",
                                        "1. **æ®‹å·®è¿æ¥**\n",
                                        "   - `output = layer(x) + x`\n",
                                        "   - è§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œé€€åŒ–é—®é¢˜\n",
                                        "   - è®©æ·±åº¦ç½‘ç»œæˆä¸ºå¯èƒ½\n",
                                        "\n",
                                        "2. **LayerNorm**\n",
                                        "   - åœ¨ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–\n",
                                        "   - ç¨³å®šè®­ç»ƒè¿‡ç¨‹\n",
                                        "   - Pre-LN æ¯” Post-LN æ›´ç¨³å®š\n",
                                        "\n",
                                        "3. **FFN/MLP**\n",
                                        "   - å…ˆå‡ç»´ï¼ˆ4xï¼‰å†é™ç»´\n",
                                        "   - ä½¿ç”¨ GELU æ¿€æ´»å‡½æ•°\n",
                                        "   - æ¨¡å‹çš„ä¸»è¦å‚æ•°æ‰€åœ¨\n",
                                        "\n",
                                        "4. **Transformer Block**\n",
                                        "   - Attention + FFN + æ®‹å·® + LayerNorm\n",
                                        "   - ç°ä»£ LLM çš„åŸºæœ¬ç§¯æœ¨\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "25058de4",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## æ€è€ƒ\n",
                                        "\n",
                                        "1. **å¯¹æ¯” Pre-LN å’Œ Post-LN**ï¼šå®ç°ä¸¤ç§ç‰ˆæœ¬ï¼Œæ¯”è¾ƒè®­ç»ƒç¨³å®šæ€§\n",
                                        "2. **æ”¹å˜ FFN ç»´åº¦**ï¼šå°è¯•ä¸åŒçš„æ‰©å±•æ¯”ä¾‹ï¼ˆ2x, 4x, 8xï¼‰\n",
                                        "3. **æ€è€ƒé¢˜**ï¼šä¸ºä»€ä¹ˆ Attention å‚æ•°é‡æ¯” FFN å°‘ï¼Œä½†åŒæ ·é‡è¦ï¼Ÿ"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "3b2691bf",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ç»ƒä¹ ç©ºé—´\n",
                                        "\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
