{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "id":  "7ce648b1",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Bonus B | Evaluation：你怎么知道模型变强了？\n",
                                     "\n",
                                     "---\n",
                                     "\n",
                                     "**目标：** 建立科学的模型评估体系\n",
                                     "\n",
                                     "**核心问题：** 如何量化模型的能力？\n",
                                     "\n",
                                     "---\n",
                                     "\n",
                                     "## 本章内容\n",
                                     "\n",
                                     "1. **Perplexity (PPL)**：语言模型的基础指标\n",
                                     "2. **下游任务评测**：MMLU, GSM8K 等\n",
                                     "3. **LLM-as-a-Judge**：用 GPT-4 评估\n",
                                     "4. **实战**：构建评测脚本"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "prereq_bonus_b_001",
                      "metadata":  {

                                   },
                      "source":  [
                                     "---\n",
                                     "\n",
                                     "## 🎓 前置知识：如何评价一个 LLM？\n",
                                     "\n",
                                     "### 评估的重要性\n",
                                     "\n",
                                     "训练了模型后，怎么知道它好不好？\n",
                                     "\n",
                                     "**不能只看 Loss！** Loss 低不代表模型实际表现好。\n",
                                     "\n",
                                     "### 评估的挑战\n",
                                     "\n",
                                     "LLM 的输出是**开放式**的，没有标准答案：\n",
                                     "\n",
                                     "```\n",
                                     "问题: \"写一首关于春天的诗\"\n",
                                     "回答A: \"春风送暖入屠苏...\"  好吗？\n",
                                     "回答B: \"Spring is coming...\"  好吗？\n",
                                     "```\n",
                                     "\n",
                                     "怎么比较？怎么打分？\n",
                                     "\n",
                                     "**评估拆解：** 先明确四件事\n",
                                     "1. **评什么能力**：知识/推理/对话/安全等\n",
                                     "2. **用什么数据**：公开基准 + 真实场景样本\n",
                                     "3. **怎么打分**：准确率、pass@k、胜率、PPL 等\n",
                                     "4. **如何控变量**：固定 prompt、温度、token budget\n",
                                     "\n",
                                     "**注意：** 防止数据污染与提示差异导致结果不可比。\n",
                                     "\n",
                                     "### 常见评估方法\n",
                                     "\n",
                                     "| 方法 | 适用场景 | 特点 |\n",
                                     "|:---|:---|:---|\n",
                                     "| Perplexity | 语言模型质量 | 自动，但与实际表现有差距 |\n",
                                     "| 基准测试 (Benchmark) | 特定能力 | 标准化，可比较 |\n",
                                     "| 人工评估 | 真实场景 | 准确，但昂贵 |\n",
                                     "| LLM-as-Judge | 大规模评估 | 用 GPT-4 打分 |\n",
                                     "\n",
                                     "### 常用 Benchmark\n",
                                     "\n",
                                     "- **MMLU**: 多学科知识\n",
                                     "- **HumanEval**: 代码能力\n",
                                     "- **GSM8K**: 数学推理\n",
                                     "- **MT-Bench**: 对话能力\n",
                                     "\n",
                                     "### 本章目标\n",
                                     "\n",
                                     "- 理解各种评估方法的优缺点\n",
                                     "- 学习如何使用常见 Benchmark\n",
                                     "- 了解 LLM-as-Judge 的实现\n",
                                     "- 设计自己的评估流程\n",
                                     "\n",
                                     "**注：** 本章代码与图表使用模拟数据，仅用于演示评测流程。"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "b172386d",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 0. 环境准备"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "03bf8c33",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import torch\n",
                                     "import torch.nn.functional as F\n",
                                     "import numpy as np\n",
                                     "import matplotlib.pyplot as plt\n",
                                     "from collections import defaultdict\n",
                                     "import json\n",
                                     "import re\n",
                                     "\n",
                                     "print(\"环境准备完成！\")"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "64247f67",
                      "metadata":  {

                                   },
                      "source":  [
                                     "---\n",
                                     "\n",
                                     "## 1. Perplexity (PPL)：困惑度\n",
                                     "\n",
                                     "### 什么是 PPL？\n",
                                     "\n",
                                     "PPL 衡量模型对测试数据的\"惊讶程度\"：\n",
                                     "\n",
                                     "$$PPL = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^N \\log P(w_i|w_{\u003ci})\\right)$$\n",
                                     "\n",
                                     "- **PPL 越低越好**：模型对数据越不\"惊讶\"\n",
                                     "- 直觉：如果下一个词很难预测，PPL 就高\n",
                                     "\n",
                                     "**局限性：** PPL 主要衡量语言建模能力，对指令遵循/推理不敏感；不同领域或 tokenizer 间不可直接横比。\n",
                                     "**注：** 下方 PPL 数值与曲线为模拟示例，用于展示趋势。"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "5339b076",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def calculate_perplexity(logits, targets):\n",
                                     "    \"\"\"\n",
                                     "    计算困惑度\n",
                                     "    \n",
                                     "    参数:\n",
                                     "        logits: [batch, seq_len, vocab_size] 模型输出\n",
                                     "        targets: [batch, seq_len] 目标 token\n",
                                     "    \n",
                                     "    返回:\n",
                                     "        PPL 值\n",
                                     "    \"\"\"\n",
                                     "    # 计算交叉熵损失\n",
                                     "    loss = F.cross_entropy(\n",
                                     "        logits.view(-1, logits.size(-1)),\n",
                                     "        targets.view(-1),\n",
                                     "        reduction=\u0027mean\u0027\n",
                                     "    )\n",
                                     "    \n",
                                     "    # PPL = exp(loss)\n",
                                     "    ppl = torch.exp(loss)\n",
                                     "    return ppl.item()\n",
                                     "\n",
                                     "# 模拟测试\n",
                                     "vocab_size = 1000\n",
                                     "batch_size = 2\n",
                                     "seq_len = 100\n",
                                     "\n",
                                     "# 好模型：预测较准确\n",
                                     "good_logits = torch.randn(batch_size, seq_len, vocab_size)\n",
                                     "targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
                                     "# 让正确 token 有较高概率\n",
                                     "good_logits.scatter_(-1, targets.unsqueeze(-1), 5.0)\n",
                                     "\n",
                                     "# 差模型：随机预测\n",
                                     "bad_logits = torch.randn(batch_size, seq_len, vocab_size)\n",
                                     "\n",
                                     "ppl_good = calculate_perplexity(good_logits, targets)\n",
                                     "ppl_bad = calculate_perplexity(bad_logits, targets)\n",
                                     "\n",
                                     "print(f\"好模型 PPL: {ppl_good:.2f}\")\n",
                                     "print(f\"差模型 PPL: {ppl_bad:.2f}\")\n",
                                     "print(f\"\\n注意: 随机预测的 PPL ≈ vocab_size = {vocab_size}\")"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "970e5f71",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# 可视化 PPL 与模型质量的关系（示意）\n",
                                     "ppls = [1000, 500, 200, 100, 50, 20, 10, 5]\n",
                                     "quality = [10, 20, 40, 55, 70, 85, 92, 98]\n",
                                     "\n",
                                     "plt.figure(figsize=(10, 5))\n",
                                     "plt.plot(ppls, quality, \u0027bo-\u0027, linewidth=2, markersize=10)\n",
                                     "plt.xscale(\u0027log\u0027)\n",
                                     "plt.xlabel(\u0027Perplexity (log scale)\u0027)\n",
                                     "plt.ylabel(\u0027Model Quality (%)\u0027)\n",
                                     "plt.title(\u0027Perplexity vs Model Quality (illustrative)\u0027)\n",
                                     "plt.grid(True, alpha=0.3)\n",
                                     "\n",
                                     "# 标注\n",
                                     "for p, q in zip(ppls, quality):\n",
                                     "    plt.annotate(f\u0027PPL={p}\u0027, (p, q), textcoords=\"offset points\", xytext=(0,10), ha=\u0027center\u0027)\n",
                                     "\n",
                                     "plt.show()\n",
                                     "\n",
                                     "print(\"PPL 越低，模型越好\")\n",
                                     "print(\"但 PPL 有局限性：只衡量语言建模能力，不反映其他能力\")"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "ab3611a8",
                      "metadata":  {

                                   },
                      "source":  [
                                     "---\n",
                                     "\n",
                                     "## 2. 下游任务评测\n",
                                     "\n",
                                     "### 常见基准测试\n",
                                     "\n",
                                     "| 基准 | 评测内容 | 示例 |\n",
                                     "|:---|:---|:---|\n",
                                     "| MMLU | 知识问答 | \"法国的首都是哪里？\" |\n",
                                     "| GSM8K | 数学推理 | \"如果小明有 5 个苹果...\" |\n",
                                     "| HumanEval | 代码生成 | \"写一个函数来...\" |\n",
                                     "| TruthfulQA | 真实性 | 避免生成虚假信息 |\n",
                                     "\n",
                                     "### 评测协议要点\n",
                                     "\n",
                                     "- 明确 zero-shot / few-shot / CoT 设置\n",
                                     "- 固定 prompt、温度、最大 token 等生成参数\n",
                                     "- 使用官方数据拆分，避免训练污染\n",
                                     "\n",
                                     "**注：** 下方 MMLU/GSM8K 代码与结果为模拟示例。\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "c4f3ce85",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# 模拟 MMLU 评测\n",
                                     "def evaluate_mmlu_sample(model_answer, correct_answer):\n",
                                     "    \"\"\"评估单个 MMLU 样本\"\"\"\n",
                                     "    return model_answer.strip().upper() == correct_answer.strip().upper()\n",
                                     "\n",
                                     "# 模拟 MMLU 数据\n",
                                     "mmlu_samples = [\n",
                                     "    {\n",
                                     "        \"question\": \"What is the capital of France?\",\n",
                                     "        \"choices\": [\"A. London\", \"B. Paris\", \"C. Berlin\", \"D. Madrid\"],\n",
                                     "        \"answer\": \"B\"\n",
                                     "    },\n",
                                     "    {\n",
                                     "        \"question\": \"What is 2 + 2?\",\n",
                                     "        \"choices\": [\"A. 3\", \"B. 4\", \"C. 5\", \"D. 6\"],\n",
                                     "        \"answer\": \"B\"\n",
                                     "    },\n",
                                     "    {\n",
                                     "        \"question\": \"What is the chemical symbol for water?\",\n",
                                     "        \"choices\": [\"A. O2\", \"B. CO2\", \"C. H2O\", \"D. NaCl\"],\n",
                                     "        \"answer\": \"C\"\n",
                                     "    },\n",
                                     "]\n",
                                     "\n",
                                     "# 模拟模型回答\n",
                                     "model_answers = [\"B\", \"B\", \"C\"]  # 假设模型全对\n",
                                     "\n",
                                     "# 评估\n",
                                     "correct = sum(evaluate_mmlu_sample(ma, s[\"answer\"]) \n",
                                     "              for ma, s in zip(model_answers, mmlu_samples))\n",
                                     "accuracy = correct / len(mmlu_samples) * 100\n",
                                     "\n",
                                     "print(\"MMLU 评测结果:\")\n",
                                     "print(f\"  正确: {correct}/{len(mmlu_samples)}\")\n",
                                     "print(f\"  准确率: {accuracy:.1f}%\")"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "577058b2",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# 模拟 GSM8K 评测\n",
                                     "def evaluate_gsm8k(model_answer, correct_answer):\n",
                                     "    \"\"\"\n",
                                     "    评估 GSM8K 样本\n",
                                     "    提取数字答案进行比较\n",
                                     "    \"\"\"\n",
                                     "    def extract_number(text):\n",
                                     "        numbers = re.findall(r\u0027-?\\d+\\.?\\d*\u0027, text)\n",
                                     "        return float(numbers[-1]) if numbers else None\n",
                                     "    \n",
                                     "    model_num = extract_number(model_answer)\n",
                                     "    correct_num = extract_number(correct_answer)\n",
                                     "    \n",
                                     "    if model_num is None or correct_num is None:\n",
                                     "        return False\n",
                                     "    return abs(model_num - correct_num) \u003c 0.01\n",
                                     "\n",
                                     "# 模拟数据\n",
                                     "gsm8k_samples = [\n",
                                     "    {\n",
                                     "        \"question\": \"John has 5 apples. He gives 2 to Mary. How many does he have?\",\n",
                                     "        \"answer\": \"3\"\n",
                                     "    },\n",
                                     "    {\n",
                                     "        \"question\": \"A car travels 60 km in 1 hour. How far in 3 hours?\",\n",
                                     "        \"answer\": \"180\"\n",
                                     "    },\n",
                                     "]\n",
                                     "\n",
                                     "# 模拟模型回答\n",
                                     "model_answers = [\n",
                                     "    \"John has 5 - 2 = 3 apples left.\",\n",
                                     "    \"60 * 3 = 180 km\"\n",
                                     "]\n",
                                     "\n",
                                     "correct = sum(evaluate_gsm8k(ma, s[\"answer\"]) \n",
                                     "              for ma, s in zip(model_answers, gsm8k_samples))\n",
                                     "accuracy = correct / len(gsm8k_samples) * 100\n",
                                     "\n",
                                     "print(\"GSM8K 评测结果:\")\n",
                                     "print(f\"  正确: {correct}/{len(gsm8k_samples)}\")\n",
                                     "print(f\"  准确率: {accuracy:.1f}%\")"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "9225138d",
                      "metadata":  {

                                   },
                      "source":  [
                                     "---\n",
                                     "\n",
                                     "## 3. LLM 评审（LLM-as-a-Judge）\n",
                                     "\n",
                                     "### 思想\n",
                                     "\n",
                                     "用更强的模型（如 GPT-4）来评估其他模型的输出。\n",
                                     "\n",
                                     "### 评估维度\n",
                                     "\n",
                                     "- 有帮助性（Helpfulness）\n",
                                     "- 无害性（Harmlessness）\n",
                                     "- 诚实性（Honesty）\n",
                                     "\n",
                                     "### 常见偏差与对策\n",
                                     "\n",
                                     "- 位置/啰嗦偏差：随机打乱顺序，控制长度\n",
                                     "- 评审一致性：多评审/多次投票，计算一致率\n",
                                     "- 任务相关性：提供评分 rubric 或参考答案\n",
                                     "\n",
                                     "**注：** 下方 LLM-as-a-Judge 结果统计为模拟示例。\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "9cdf8474",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def llm_judge_prompt(question, response_a, response_b):\n",
                                     "    \"\"\"\n",
                                     "    构建 LLM-as-a-Judge 的 prompt\n",
                                     "    \"\"\"\n",
                                     "    prompt = f\"\"\"Please evaluate the following two responses to the question.\n",
                                     "\n",
                                     "Question: {question}\n",
                                     "\n",
                                     "Response A: {response_a}\n",
                                     "\n",
                                     "Response B: {response_b}\n",
                                     "\n",
                                     "Please compare the responses based on:\n",
                                     "1. Helpfulness: Does it answer the question well?\n",
                                     "2. Accuracy: Is the information correct?\n",
                                     "3. Clarity: Is it easy to understand?\n",
                                     "\n",
                                     "Output your judgment as:\n",
                                     "- \"A\" if Response A is better\n",
                                     "- \"B\" if Response B is better\n",
                                     "- \"Tie\" if they are equally good\n",
                                     "\n",
                                     "Your judgment:\"\"\"\n",
                                     "    return prompt\n",
                                     "\n",
                                     "# 示例\n",
                                     "question = \"How do I learn Python?\"\n",
                                     "response_a = \"Read the docs.\"\n",
                                     "response_b = \"Start with basics like variables and loops. Practice with small projects. Use online resources like Codecademy or freeCodeCamp.\"\n",
                                     "\n",
                                     "prompt = llm_judge_prompt(question, response_a, response_b)\n",
                                     "print(\"LLM Judge Prompt:\")\n",
                                     "print(\"=\" * 60)\n",
                                     "print(prompt)\n",
                                     "\n",
                                     "print(\"\\n\" + \"=\" * 60)\n",
                                     "print(\"(在实际中，将此 prompt 发送给 GPT-4)\")"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "503a60b3",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# 模拟评测结果统计\n",
                                     "def analyze_judgments(judgments):\n",
                                     "    \"\"\"分析评测结果\"\"\"\n",
                                     "    counts = defaultdict(int)\n",
                                     "    for j in judgments:\n",
                                     "        counts[j] += 1\n",
                                     "    \n",
                                     "    total = len(judgments)\n",
                                     "    return {k: v/total*100 for k, v in counts.items()}\n",
                                     "\n",
                                     "# 模拟多次评测结果\n",
                                     "judgments = [\"B\", \"B\", \"A\", \"B\", \"Tie\", \"B\", \"B\", \"A\", \"B\", \"B\"]\n",
                                     "results = analyze_judgments(judgments)\n",
                                     "\n",
                                     "print(\"LLM-as-a-Judge 结果统计:\")\n",
                                     "for option, percentage in sorted(results.items()):\n",
                                     "    print(f\"  {option}: {percentage:.1f}%\")\n",
                                     "\n",
                                     "# 可视化\n",
                                     "plt.figure(figsize=(8, 5))\n",
                                     "plt.bar(results.keys(), results.values(), color=[\u0027coral\u0027, \u0027green\u0027, \u0027gray\u0027])\n",
                                     "plt.ylabel(\u0027Percentage (%)\u0027)\n",
                                     "plt.title(\u0027LLM-as-a-Judge Results\u0027)\n",
                                     "plt.ylim(0, 100)\n",
                                     "plt.show()"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "08991e4a",
                      "metadata":  {

                                   },
                      "source":  [
                                     "---\n",
                                     "\n",
                                     "## 4. 综合评测框架\n",
                                     "\n",
                                     "把多维指标汇总成 scorecard，形成可追踪的版本对比：\n",
                                     "- 统一输入/生成配置，记录版本与数据\n",
                                     "- 指标归一化后做雷达图或排行榜\n",
                                     "- 与关键业务指标（如留存、满意度）对齐\n",
                                     "\n",
                                     "**注：** 下方雷达图示例使用随机数模拟。"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "106a349e",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "class ModelEvaluator:\n",
                                     "    \"\"\"\n",
                                     "    综合模型评测器\n",
                                     "    \"\"\"\n",
                                     "    def __init__(self):\n",
                                     "        self.results = {}\n",
                                     "    \n",
                                     "    def evaluate_perplexity(self, model, test_data):\n",
                                     "        \"\"\"评估 PPL\"\"\"\n",
                                     "        # 模拟\n",
                                     "        ppl = np.random.uniform(10, 100)\n",
                                     "        self.results[\u0027perplexity\u0027] = ppl\n",
                                     "        return ppl\n",
                                     "    \n",
                                     "    def evaluate_mmlu(self, model, mmlu_data):\n",
                                     "        \"\"\"评估 MMLU\"\"\"\n",
                                     "        # 模拟\n",
                                     "        accuracy = np.random.uniform(0.6, 0.9)\n",
                                     "        self.results[\u0027mmlu\u0027] = accuracy\n",
                                     "        return accuracy\n",
                                     "    \n",
                                     "    def evaluate_gsm8k(self, model, gsm8k_data):\n",
                                     "        \"\"\"评估 GSM8K\"\"\"\n",
                                     "        # 模拟\n",
                                     "        accuracy = np.random.uniform(0.4, 0.8)\n",
                                     "        self.results[\u0027gsm8k\u0027] = accuracy\n",
                                     "        return accuracy\n",
                                     "    \n",
                                     "    def evaluate_humaneval(self, model, humaneval_data):\n",
                                     "        \"\"\"评估代码生成\"\"\"\n",
                                     "        # 模拟\n",
                                     "        pass_rate = np.random.uniform(0.3, 0.7)\n",
                                     "        self.results[\u0027humaneval\u0027] = pass_rate\n",
                                     "        return pass_rate\n",
                                     "    \n",
                                     "    def get_summary(self):\n",
                                     "        \"\"\"获取评测摘要\"\"\"\n",
                                     "        return self.results\n",
                                     "    \n",
                                     "    def plot_radar(self):\n",
                                     "        \"\"\"雷达图可视化\"\"\"\n",
                                     "        categories = list(self.results.keys())\n",
                                     "        values = list(self.results.values())\n",
                                     "        \n",
                                     "        # 归一化\n",
                                     "        normalized = []\n",
                                     "        for i, (cat, val) in enumerate(zip(categories, values)):\n",
                                     "            if cat == \u0027perplexity\u0027:\n",
                                     "                normalized.append(1 - val/100)  # PPL 越低越好\n",
                                     "            else:\n",
                                     "                normalized.append(val)\n",
                                     "        \n",
                                     "        # 雷达图\n",
                                     "        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
                                     "        normalized += normalized[:1]\n",
                                     "        angles += angles[:1]\n",
                                     "        \n",
                                     "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
                                     "        ax.plot(angles, normalized, \u0027o-\u0027, linewidth=2)\n",
                                     "        ax.fill(angles, normalized, alpha=0.25)\n",
                                     "        ax.set_xticks(angles[:-1])\n",
                                     "        ax.set_xticklabels(categories)\n",
                                     "        ax.set_ylim(0, 1)\n",
                                     "        plt.title(\u0027Model Evaluation Radar Chart\u0027)\n",
                                     "        plt.show()\n",
                                     "\n",
                                     "# 运行评测\n",
                                     "evaluator = ModelEvaluator()\n",
                                     "evaluator.evaluate_perplexity(None, None)\n",
                                     "evaluator.evaluate_mmlu(None, None)\n",
                                     "evaluator.evaluate_gsm8k(None, None)\n",
                                     "evaluator.evaluate_humaneval(None, None)\n",
                                     "\n",
                                     "print(\"评测结果:\")\n",
                                     "for metric, value in evaluator.get_summary().items():\n",
                                     "    if metric == \u0027perplexity\u0027:\n",
                                     "        print(f\"  {metric}: {value:.1f}\")\n",
                                     "    else:\n",
                                     "        print(f\"  {metric}: {value*100:.1f}%\")\n",
                                     "\n",
                                     "evaluator.plot_radar()"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "db91118c",
                      "metadata":  {

                                   },
                      "source":  [
                                     "---\n",
                                     "\n",
                                     "## 本章总结\n",
                                     "\n",
                                     "### 你学到了什么？\n",
                                     "\n",
                                     "1. **Perplexity**\n",
                                     "   - 衡量语言建模能力\n",
                                     "   - 越低越好，但有局限性\n",
                                     "\n",
                                     "2. **下游任务评测**\n",
                                     "   - MMLU: 知识问答\n",
                                     "   - GSM8K: 数学推理\n",
                                     "   - HumanEval: 代码生成\n",
                                     "\n",
                                     "3. **LLM-as-a-Judge**\n",
                                     "   - 用强模型评估弱模型\n",
                                     "   - 可以评估主观质量\n",
                                     "\n",
                                     "### 评测最佳实践\n",
                                     "\n",
                                     "1. **多维度评估**：不要只看一个指标\n",
                                     "2. **统一协议**：固定 prompt/温度/采样参数\n",
                                     "3. **定期评测**：监控模型变化\n",
                                     "4. **公平对比**：同数据同预算，避免污染\n",
                                     "5. **人工抽检**：自动化不能替代人工\n",
                                     "\n",
                                     "**备注：** 本章所有数值/图表为模拟示例。"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "9813b89c",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# 练习空间\n",
                                     "\n"
                                 ]
                  }
              ],
    "metadata":  {

                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
