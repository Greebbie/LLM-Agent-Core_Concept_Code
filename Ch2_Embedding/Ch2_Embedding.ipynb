{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a344bb",
   "metadata": {},
   "source": [
    "# Ch2 | Embeddingï¼šè¯­è¨€æ¨¡å‹çš„çµé­‚\n",
    "\n",
    "---\n",
    "\n",
    "**ç›®æ ‡ï¼š** ç†è§£è¯å‘é‡æ˜¯å¦‚ä½•å·¥ä½œçš„\n",
    "\n",
    "**æ ¸å¿ƒé—®é¢˜ï¼š** æœºå™¨å¦‚ä½•\"ç†è§£\"æ–‡å­—ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« å†…å®¹\n",
    "\n",
    "1. **Lookup Table**ï¼šæ­ç§˜ `nn.Embedding` çš„æœ¬è´¨\n",
    "2. **è®­ç»ƒæ¼”ç¤º**ï¼šç”¨ Bigram æ¨¡å‹è®­ç»ƒè¯å‘é‡\n",
    "3. **å¯è§†åŒ–**ï¼šçœ‹è¯å‘é‡åœ¨ç©ºé—´ä¸­çš„åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_ch2_001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ å‰ç½®çŸ¥è¯†ï¼šä¸ºä»€ä¹ˆæœºå™¨éœ€è¦\"ç†è§£\"æ–‡å­—ï¼Ÿ\n",
    "\n",
    "### æœºå™¨çš„å›°å¢ƒ\n",
    "\n",
    "è®¡ç®—æœºåªèƒ½å¤„ç†**æ•°å­—**ï¼Œä¸èƒ½ç›´æ¥å¤„ç†æ–‡å­—ï¼š\n",
    "\n",
    "```python\n",
    "# è¿™æ ·ä¸è¡Œï¼\n",
    "model(\"ä½ å¥½\")  # âŒ æœºå™¨ä¸æ‡‚\n",
    "\n",
    "# éœ€è¦è½¬æ¢æˆæ•°å­—\n",
    "model([1, 2, 3])  # âœ“ æœºå™¨èƒ½å¤„ç†\n",
    "```\n",
    "\n",
    "### æœ€ç®€å•çš„æ–¹æ³•ï¼šOne-Hot ç¼–ç \n",
    "\n",
    "ç»™æ¯ä¸ªè¯ä¸€ä¸ªç‹¬ç‰¹çš„\"ç¼–å·å‘é‡\"ï¼š\n",
    "\n",
    "```\n",
    "è¯è¡¨: [çŒ«, ç‹—, é¸Ÿ]\n",
    "çŒ« â†’ [1, 0, 0]\n",
    "ç‹— â†’ [0, 1, 0]  \n",
    "é¸Ÿ â†’ [0, 0, 1]\n",
    "```\n",
    "\n",
    "**é—®é¢˜**ï¼š\n",
    "- ç»´åº¦å¤ªé«˜ï¼ˆè¯è¡¨10ä¸‡ â†’ 10ä¸‡ç»´å‘é‡ï¼‰\n",
    "- æ— æ³•è¡¨è¾¾è¯­ä¹‰å…³ç³»ï¼ˆ\"çŒ«\"å’Œ\"ç‹—\"çš„è·ç¦» = \"çŒ«\"å’Œ\"æ±½è½¦\"çš„è·ç¦»ï¼‰\n",
    "\n",
    "### è§£å†³æ–¹æ¡ˆï¼šEmbeddingï¼ˆè¯åµŒå…¥ï¼‰\n",
    "\n",
    "æŠŠæ¯ä¸ªè¯æ˜ å°„åˆ°ä¸€ä¸ª**ä½ç»´ã€ç¨ å¯†**çš„å‘é‡ï¼š\n",
    "\n",
    "```\n",
    "çŒ« â†’ [0.2, 0.8, -0.3, 0.5]  (4ç»´)\n",
    "ç‹— â†’ [0.3, 0.7, -0.2, 0.4]  (ç›¸ä¼¼ï¼)\n",
    "æ±½è½¦ â†’ [-0.8, 0.1, 0.9, -0.2]  (ä¸ç›¸ä¼¼)\n",
    "```\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šè¯­ä¹‰ç›¸è¿‘çš„è¯ï¼Œå‘é‡ä¹Ÿç›¸è¿‘ï¼\n",
    "\n",
    "### æœ¬ç« ç›®æ ‡\n",
    "\n",
    "- ç†è§£ `nn.Embedding` çš„æœ¬è´¨ï¼ˆå°±æ˜¯æŸ¥è¡¨ï¼ï¼‰\n",
    "- äº²æ‰‹è®­ç»ƒè¯å‘é‡\n",
    "- å¯è§†åŒ–è¯å‘é‡ç©ºé—´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8517bcb",
   "metadata": {},
   "source": [
    "## 0. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aeea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42) \n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa5fc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Embedding çš„æœ¬è´¨ï¼šä¸€ä¸ªæŸ¥è¡¨çŸ©é˜µ\n",
    "\n",
    "å¿˜æ‰å¤æ‚çš„æ•°å­¦å®šä¹‰ï¼`nn.Embedding` å°±æ˜¯ï¼š\n",
    "\n",
    "```\n",
    "ä¸€ä¸ªå½¢çŠ¶ä¸º [è¯è¡¨å¤§å°, å‘é‡ç»´åº¦] çš„çŸ©é˜µ\n",
    "```\n",
    "\n",
    "å½“ä½ è¾“å…¥ä¸€ä¸ªè¯çš„ç´¢å¼•ï¼Œå®ƒå°±è¿”å›å¯¹åº”çš„é‚£ä¸€è¡Œå‘é‡ã€‚å°±è¿™ä¹ˆç®€å•ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªç®€å•çš„ Embedding å±‚\n",
    "vocab_size = 10  # è¯è¡¨å¤§å°\n",
    "embed_dim = 4    # æ¯ä¸ªè¯ç”¨4ç»´å‘é‡è¡¨ç¤º\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "print(\"Embedding çŸ©é˜µå½¢çŠ¶:\", embedding.weight.shape)\n",
    "print(\"\\nEmbedding çŸ©é˜µå†…å®¹ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰:\")\n",
    "print(embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb385c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding å°±æ˜¯æŸ¥è¡¨ï¼\n",
    "word_idx = torch.tensor([0, 3, 5])  # è¦æŸ¥çš„è¯ç´¢å¼•\n",
    "\n",
    "# æ–¹æ³•1: ä½¿ç”¨ Embedding å±‚\n",
    "result1 = embedding(word_idx)\n",
    "\n",
    "# æ–¹æ³•2: ç›´æ¥ç´¢å¼•çŸ©é˜µï¼ˆå®Œå…¨ç­‰ä»·ï¼ï¼‰\n",
    "result2 = embedding.weight[word_idx]\n",
    "\n",
    "print(\"æŸ¥è¯¢è¯ç´¢å¼•:\", word_idx.tolist())\n",
    "print(\"\\næ–¹æ³•1 - embedding(idx):\")\n",
    "print(result1)\n",
    "print(\"\\næ–¹æ³•2 - weight[idx]:\")\n",
    "print(result2)\n",
    "print(\"\\nä¸¤ç§æ–¹æ³•å®Œå…¨ç­‰ä»·:\", torch.allclose(result1, result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58004201",
   "metadata": {},
   "source": [
    "### å…³é”®æ´å¯Ÿ\n",
    "\n",
    "```\n",
    "nn.Embedding â‰¡ å¯å­¦ä¹ çš„æŸ¥è¡¨çŸ©é˜µ\n",
    "```\n",
    "\n",
    "- è¾“å…¥ï¼šè¯çš„ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰\n",
    "- è¾“å‡ºï¼šå¯¹åº”çš„å‘é‡\n",
    "- è®­ç»ƒï¼šé€šè¿‡åå‘ä¼ æ’­æ›´æ–°çŸ©é˜µä¸­çš„å€¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af04aab8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. è®­ç»ƒè¯å‘é‡ï¼šBigram è¯­è¨€æ¨¡å‹\n",
    "\n",
    "æˆ‘ä»¬ç”¨æœ€ç®€å•çš„è¯­è¨€æ¨¡å‹æ¥è®­ç»ƒè¯å‘é‡ï¼š\n",
    "\n",
    "**Bigram æ¨¡å‹ï¼š** æ ¹æ®å½“å‰è¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯\n",
    "\n",
    "```\n",
    "P(next_word | current_word)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "text = \"\"\"\n",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒ\n",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„åˆ†æ”¯\n",
    "ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€\n",
    "äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ\n",
    "æœºå™¨å­¦ä¹ æ”¹å˜ç”Ÿæ´»\n",
    "\"\"\"\n",
    "\n",
    "# ç®€å•åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰\n",
    "words = list(text.replace('\\n', '').replace(' ', ''))\n",
    "print(\"è¯è¡¨:\", words[:20], \"...\")\n",
    "print(f\"æ€»è¯æ•°: {len(words)}\")\n",
    "\n",
    "# æ„å»ºè¯è¡¨\n",
    "vocab = sorted(set(words))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"\\nè¯è¡¨å¤§å°: {vocab_size}\")\n",
    "print(\"è¯è¡¨:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e2704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºè®­ç»ƒæ•°æ®ï¼š(å½“å‰è¯, ä¸‹ä¸€ä¸ªè¯) å¯¹\n",
    "X, Y = [], []\n",
    "for i in range(len(words) - 1):\n",
    "    X.append(word2idx[words[i]])\n",
    "    Y.append(word2idx[words[i+1]])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(X)}\")\n",
    "print(f\"\\nå‰5ä¸ªæ ·æœ¬:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {idx2word[X[i].item()]} -> {idx2word[Y[i].item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21714f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ Bigram æ¨¡å‹\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size] è¯ç´¢å¼•\n",
    "        emb = self.embedding(x)       # [batch_size, embed_dim]\n",
    "        logits = self.linear(emb)     # [batch_size, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "embed_dim = 16\n",
    "model = BigramModel(vocab_size, embed_dim)\n",
    "\n",
    "print(\"æ¨¡å‹ç»“æ„:\")\n",
    "print(model)\n",
    "print(f\"\\næ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(500):\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, Y)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # åå‘ä¼ æ’­\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"è®­ç»ƒå®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187126c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss æ›²çº¿\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', linewidth=1.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Bigram Model Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135ec04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. å¯è§†åŒ–è¯å‘é‡ç©ºé—´\n",
    "\n",
    "è¿™æ˜¯æœ€æ¿€åŠ¨äººå¿ƒçš„éƒ¨åˆ†ï¼è®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒåçš„è¯å‘é‡åœ¨ç©ºé—´ä¸­æ˜¯å¦‚ä½•åˆ†å¸ƒçš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–è®­ç»ƒåçš„è¯å‘é‡\n",
    "word_vectors = model.embedding.weight.detach().numpy()\n",
    "\n",
    "print(\"è¯å‘é‡å½¢çŠ¶:\", word_vectors.shape)\n",
    "print(f\"æ¯ä¸ªè¯ç”¨ {word_vectors.shape[1]} ç»´å‘é‡è¡¨ç¤º\")\n",
    "\n",
    "# ç”¨ PCA é™åˆ°2ç»´è¿›è¡Œå¯è§†åŒ–\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_2d = pca.fit_transform(word_vectors)\n",
    "\n",
    "print(f\"\\nPCA è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è¯å‘é‡\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# ç”»ç‚¹\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], \n",
    "            c='steelblue', s=100, alpha=0.7)\n",
    "\n",
    "# æ ‡æ³¨æ¯ä¸ªè¯\n",
    "for i, word in enumerate(vocab):\n",
    "    plt.annotate(word, (word_vectors_2d[i, 0], word_vectors_2d[i, 1]),\n",
    "                 fontsize=14, ha='center', va='bottom',\n",
    "                 fontproperties={'family': 'SimHei'})\n",
    "\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Word Embedding Visualization', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"è§‚å¯Ÿï¼šè¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b753098",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. è¯å‘é‡çš„æ•°å­¦æ“ä½œ\n",
    "\n",
    "è¯å‘é‡æœ‰ä¸€ä¸ªç¥å¥‡çš„æ€§è´¨ï¼š**è¯­ä¹‰å…³ç³»å¯ä»¥ç”¨å‘é‡è¿ç®—è¡¨ç¤º**\n",
    "\n",
    "ç»å…¸ä¾‹å­ï¼š\n",
    "```\n",
    "king - man + woman â‰ˆ queen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def find_similar(word, top_k=5):\n",
    "    if word not in word2idx:\n",
    "        print(f\"è¯ '{word}' ä¸åœ¨è¯è¡¨ä¸­\")\n",
    "        return\n",
    "    \n",
    "    word_vec = word_vectors[word2idx[word]]\n",
    "    similarities = []\n",
    "    \n",
    "    for w in vocab:\n",
    "        if w != word:\n",
    "            sim = cosine_similarity(word_vec, word_vectors[word2idx[w]])\n",
    "            similarities.append((w, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"ä¸ '{word}' æœ€ç›¸ä¼¼çš„è¯:\")\n",
    "    for w, sim in similarities[:top_k]:\n",
    "        print(f\"  {w}: {sim:.4f}\")\n",
    "\n",
    "# æŸ¥æ‰¾ç›¸ä¼¼è¯\n",
    "find_similar('å­¦')\n",
    "print()\n",
    "find_similar('æœº')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è¯å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "similarity_matrix = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        similarity_matrix[i, j] = cosine_similarity(\n",
    "            word_vectors[i], word_vectors[j]\n",
    "        )\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(similarity_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xticks(range(vocab_size), vocab, rotation=45, ha='right', fontproperties={'family': 'SimHei'})\n",
    "plt.yticks(range(vocab_size), vocab, fontproperties={'family': 'SimHei'})\n",
    "plt.title('Word Similarity Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a1ad3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. One-Hot vs Embedding\n",
    "\n",
    "è®©æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹ä¸¤ç§è¯è¡¨ç¤ºæ–¹å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a32f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot ç¼–ç \n",
    "def one_hot(idx, vocab_size):\n",
    "    vec = torch.zeros(vocab_size)\n",
    "    vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "word_idx = 3\n",
    "print(f\"è¯ '{idx2word[word_idx]}' çš„è¡¨ç¤º:\")\n",
    "print(f\"\\nOne-Hot ({vocab_size}ç»´ï¼Œç¨€ç–):\")\n",
    "print(one_hot(word_idx, vocab_size))\n",
    "print(f\"\\nEmbedding ({embed_dim}ç»´ï¼Œç¨ å¯†):\")\n",
    "print(model.embedding.weight[word_idx].data)\n",
    "\n",
    "print(f\"\\nå¯¹æ¯”:\")\n",
    "print(f\"  One-Hot ç»´åº¦:  {vocab_size}\")\n",
    "print(f\"  Embedding ç»´åº¦: {embed_dim}\")\n",
    "print(f\"  ç»´åº¦å‹ç¼©:      {vocab_size / embed_dim:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b39ef0",
   "metadata": {},
   "source": [
    "### One-Hot vs Embedding å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | One-Hot | Embedding |\n",
    "|:---|:---|:---|\n",
    "| ç»´åº¦ | è¯è¡¨å¤§å°ï¼ˆå¯èƒ½å¾ˆå¤§ï¼‰ | å›ºå®šç»´åº¦ï¼ˆé€šå¸¸256-1024ï¼‰ |\n",
    "| ç¨€ç–æ€§ | æåº¦ç¨€ç–ï¼ˆåªæœ‰1ä¸ª1ï¼‰ | ç¨ å¯† |\n",
    "| è¯­ä¹‰å…³ç³» | æ— æ³•è¡¨è¾¾ | ç›¸ä¼¼è¯å‘é‡ç›¸è¿‘ |\n",
    "| å¯å­¦ä¹  | å¦ | æ˜¯ |\n",
    "| å†…å­˜å ç”¨ | O(è¯è¡¨å¤§å°) | O(åµŒå…¥ç»´åº¦) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae7570",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "\n",
    "1. **Embedding çš„æœ¬è´¨**\n",
    "   - å°±æ˜¯ä¸€ä¸ªæŸ¥è¡¨çŸ©é˜µ `[vocab_size, embed_dim]`\n",
    "   - è¾“å…¥è¯ç´¢å¼•ï¼Œè¾“å‡ºå¯¹åº”å‘é‡\n",
    "   - é€šè¿‡è®­ç»ƒå­¦ä¹ è¯­ä¹‰å…³ç³»\n",
    "\n",
    "2. **è¯å‘é‡çš„é­”åŠ›**\n",
    "   - æŠŠç¦»æ•£çš„è¯å˜æˆè¿ç»­çš„å‘é‡\n",
    "   - è¯­ä¹‰ç›¸ä¼¼çš„è¯ï¼Œå‘é‡ä¹Ÿç›¸è¿‘\n",
    "   - æ”¯æŒå‘é‡è¿ç®—è¡¨è¾¾è¯­ä¹‰å…³ç³»\n",
    "\n",
    "3. **æ ¸å¿ƒæ´å¯Ÿ**\n",
    "   - æœºå™¨ä¸æ‡‚ä¸­æ–‡ï¼Œä½†å®ƒæ‡‚å‘é‡ç©ºé—´é‡Œçš„å‡ ä½•ä½ç½®\n",
    "   - Embedding æ˜¯æ‰€æœ‰è¯­è¨€æ¨¡å‹çš„åŸºç¡€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202109ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€è€ƒ\n",
    "\n",
    "1. **å¢åŠ è®­ç»ƒæ•°æ®**ï¼šç”¨æ›´å¤šæ–‡æœ¬è®­ç»ƒï¼Œè§‚å¯Ÿè¯å‘é‡è´¨é‡çš„å˜åŒ–\n",
    "2. **æ”¹å˜åµŒå…¥ç»´åº¦**ï¼šå°è¯•ä¸åŒçš„ `embed_dim`ï¼Œè§‚å¯Ÿå¯¹ç»“æœçš„å½±å“\n",
    "3. **å®ç°è¯ç±»æ¯”**ï¼šå®ç° `word1 - word2 + word3 â‰ˆ word4` çš„åŠŸèƒ½\n",
    "4. **æ€è€ƒé¢˜**ï¼šä¸ºä»€ä¹ˆéœ€è¦è®­ç»ƒ Embeddingï¼Œè€Œä¸æ˜¯ç”¨å›ºå®šçš„å‘é‡ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55864b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ ç©ºé—´\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
