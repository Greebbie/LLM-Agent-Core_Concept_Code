{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "8331eea2",
                              "metadata": {},
                              "source": [
                                        "# Ch5 | GPT ç»„è£…ï¼šä»æ¦‚ç‡é¢„æµ‹åˆ°æ–‡æœ¬ç”Ÿæˆ\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "**ç›®æ ‡ï¼š** ä»é›¶æ„å»ºä¸€ä¸ªå®Œæ•´çš„ GPT æ¨¡å‹\n",
                                        "\n",
                                        "**æ ¸å¿ƒé—®é¢˜ï¼š** å¦‚ä½•è®©æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Ÿ\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## æœ¬ç« å†…å®¹\n",
                                        "\n",
                                        "1. **ä½ç½®ç¼–ç  (Positional Encoding)**ï¼šæ³¨å…¥ä½ç½®ä¿¡æ¯\n",
                                        "2. **RoPE (æ—‹è½¬ä½ç½®ç¼–ç )**ï¼šç°ä»£ LLM çš„æ ‡é…\n",
                                        "3. **GPT æ¶æ„**ï¼šå®Œæ•´æ¨¡å‹ç»„è£…\n",
                                        "4. **æ–‡æœ¬ç”Ÿæˆ**ï¼šTemperature, Top-k, Top-p é‡‡æ ·"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "prereq_ch5_001",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## å‰ç½®çŸ¥è¯†ï¼šä»ç§¯æœ¨åˆ°å®Œæ•´å»ºç­‘\n",
                                        "\n",
                                        "### æˆ‘ä»¬å·²ç»å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ\n",
                                        "\n",
                                        "å‰é¢å‡ ç« æˆ‘ä»¬å­¦ä¹ äº†æ„å»º LLM çš„\"ç§¯æœ¨\"ï¼š\n",
                                        "\n",
                                        "| ç« èŠ‚ | å†…å®¹ | ä½œç”¨ |\n",
                                        "|:---|:---|:---|\n",
                                        "| Ch1 | Autograd | è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ |\n",
                                        "| Ch2 | Embedding | æŠŠè¯å˜æˆå‘é‡ |\n",
                                        "| Ch3 | Self-Attention | è®©è¯\"çœ‹åˆ°\"ä¸Šä¸‹æ–‡ |\n",
                                        "| Ch4 | Transformer Block | å®Œæ•´çš„å¤„ç†å•å…ƒ |\n",
                                        "\n",
                                        "### ç°åœ¨ï¼šç»„è£… GPTï¼\n",
                                        "\n",
                                        "GPT (Generative Pre-trained Transformer) çš„ç»“æ„ï¼š\n",
                                        "\n",
                                        "```\n",
                                        "è¾“å…¥æ–‡æœ¬\n",
                                        "   â†“\n",
                                        "Token Embedding (Ch2) + Position Embedding\n",
                                        "   â†“\n",
                                        "N Ã— Transformer Block (Ch4)\n",
                                        "   â†“\n",
                                        "Language Model Head (é¢„æµ‹ä¸‹ä¸€ä¸ªè¯)\n",
                                        "   â†“\n",
                                        "è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ\n",
                                        "```\n",
                                        "\n",
                                        "### ä»€ä¹ˆæ˜¯ä½ç½®ç¼–ç ï¼Ÿ\n",
                                        "\n",
                                        "Self-Attention æ˜¯**æ— åºçš„**â€”â€”å®ƒåªçœ‹è¯ä¹‹é—´çš„å…³ç³»ï¼Œä¸çŸ¥é“è¯çš„é¡ºåºï¼\n",
                                        "\n",
                                        "```\n",
                                        "\"ç‹—å’¬äºº\" å’Œ \"äººå’¬ç‹—\" å¯¹ Attention æ¥è¯´ä¸€æ ·ï¼\n",
                                        "```\n",
                                        "\n",
                                        "**è§£å†³æ–¹æ¡ˆ**ï¼šç»™æ¯ä¸ªä½ç½®ä¸€ä¸ªç‹¬ç‰¹çš„ç¼–ç ï¼ŒåŠ åˆ°è¯å‘é‡ä¸Š\n",
                                        "\n",
                                        "```\n",
                                        "è¯å‘é‡ + ä½ç½®å‘é‡ = å¸¦ä½ç½®ä¿¡æ¯çš„å‘é‡\n",
                                        "```\n",
                                        "\n",
                                        "### ä»€ä¹ˆæ˜¯ Language Model Headï¼Ÿ\n",
                                        "\n",
                                        "æŠŠ Transformer çš„è¾“å‡ºå˜æˆ**ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ**ï¼š\n",
                                        "\n",
                                        "```\n",
                                        "éšè—çŠ¶æ€ [batch, seq, hidden] \n",
                                        "    â†“ Linearå±‚\n",
                                        "Logits [batch, seq, vocab_size]\n",
                                        "    â†“ Softmax\n",
                                        "æ¦‚ç‡åˆ†å¸ƒ (å“ªä¸ªè¯æœ€å¯èƒ½æ˜¯ä¸‹ä¸€ä¸ªï¼Ÿ)\n",
                                        "```\n",
                                        "\n",
                                        "### æœ¬ç« ç›®æ ‡\n",
                                        "\n",
                                        "- å®ç°ä½ç½®ç¼–ç ï¼ˆSinusoidal å’Œ RoPEï¼‰\n",
                                        "- ç»„è£…å®Œæ•´çš„ GPT æ¨¡å‹\n",
                                        "- ç†è§£æ–‡æœ¬ç”Ÿæˆçš„é‡‡æ ·ç­–ç•¥ï¼ˆTemperature, Top-k, Top-pï¼‰\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "f90dc131",
                              "metadata": {},
                              "source": [
                                        "## 0. ç¯å¢ƒå‡†å¤‡"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "13a96c98",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "\n",
                                        "torch.manual_seed(42)\n",
                                        "print(f\"PyTorch version: {torch.__version__}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "hb8a0d8naxd",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ğŸ¨ GPT æ¶æ„å…¨æ™¯å›¾\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "import matplotlib.patches as patches\n",
                                        "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
                                        "import numpy as np\n",
                                        "import os\n",
                                        "from matplotlib import font_manager as fm\n",
                                        "import matplotlib as mpl\n",
                                        "font_path = \"../assets/fonts/NotoSansCJKsc-Regular.otf\"\n",
                                        "if os.path.exists(font_path):\n",
                                        "    fm.fontManager.addfont(font_path)\n",
                                        "\n",
                                        "mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
                                        "mpl.rcParams[\"font.sans-serif\"] = [\"Noto Sans CJK SC\", \"DejaVu Sans\"]\n",
                                        "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
                                        "\n",
                                        "def draw_gpt_architecture():\n",
                                        "    \"\"\"\n",
                                        "    ç»˜åˆ¶ GPT æ¶æ„çš„å®Œæ•´ç¤ºæ„å›¾\n",
                                        "    å±•ç¤ºä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´æ•°æ®æµ\n",
                                        "    \"\"\"\n",
                                        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
                                        "    ax.set_xlim(0, 16)\n",
                                        "    ax.set_ylim(0, 12)\n",
                                        "    ax.axis('off')\n",
                                        "    ax.set_title('GPT Architecture', fontsize=18, fontweight='bold', pad=20)\n",
                                        "    \n",
                                        "    # é¢œè‰²å®šä¹‰\n",
                                        "    colors = {\n",
                                        "        'input': '#E8F6F3',\n",
                                        "        'embedding': '#D5F5E3',\n",
                                        "        'attention': '#FCF3CF',\n",
                                        "        'ffn': '#FADBD8',\n",
                                        "        'norm': '#E8DAEF',\n",
                                        "        'output': '#D6EAF8',\n",
                                        "        'arrow': '#566573'\n",
                                        "    }\n",
                                        "    \n",
                                        "    # ===== è¾“å…¥å±‚ =====\n",
                                        "    ax.add_patch(FancyBboxPatch((1, 0.5), 14, 1.2, boxstyle='round,pad=0.1',\n",
                                        "                                facecolor=colors['input'], edgecolor='#1ABC9C', linewidth=2))\n",
                                        "    ax.text(8, 1.1, 'Output Tokens: [\"The\", \"cat\", \"sat\", \"on\"]', fontsize=12, ha='center', fontweight='bold')\n",
                                        "    \n",
                                        "    # ç®­å¤´\n",
                                        "    ax.annotate('', xy=(8, 2.2), xytext=(8, 1.8),\n",
                                        "               arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=2))\n",
                                        "    \n",
                                        "    # ===== Embedding å±‚ =====\n",
                                        "    ax.add_patch(FancyBboxPatch((2, 2.2), 5, 1.5, boxstyle='round,pad=0.1',\n",
                                        "                                facecolor=colors['embedding'], edgecolor='#27AE60', linewidth=2))\n",
                                        "    ax.text(4.5, 3.2, 'Token Embedding', fontsize=11, ha='center', fontweight='bold')\n",
                                        "    ax.text(4.5, 2.7, 'nn.Embedding(vocab, d_model)', fontsize=9, ha='center', family='monospace', color='#666')\n",
                                        "    \n",
                                        "    ax.add_patch(FancyBboxPatch((9, 2.2), 5, 1.5, boxstyle='round,pad=0.1',\n",
                                        "                                facecolor=colors['embedding'], edgecolor='#27AE60', linewidth=2))\n",
                                        "    ax.text(11.5, 3.2, 'Position Embedding', fontsize=11, ha='center', fontweight='bold')\n",
                                        "    ax.text(11.5, 2.7, 'Sinusoidal / RoPE', fontsize=9, ha='center', family='monospace', color='#666')\n",
                                        "    \n",
                                        "    # + å·\n",
                                        "    ax.text(7.5, 3, '+', fontsize=20, ha='center', fontweight='bold', color='#E74C3C')\n",
                                        "    \n",
                                        "    ax.annotate('', xy=(8, 4.3), xytext=(8, 3.8),\n",
                                        "               arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=2))\n",
                                        "    \n",
                                        "    # ===== Transformer Block (å±•å¼€ä¸€ä¸ª) =====\n",
                                        "    # å¤–æ¡†\n",
                                        "    ax.add_patch(FancyBboxPatch((1.5, 4.3), 13, 5.5, boxstyle='round,pad=0.1',\n",
                                        "                                facecolor='white', edgecolor='#3498DB', linewidth=3, linestyle='--'))\n",
                                        "    ax.text(8, 9.5, 'Transformer Block Ã— N', fontsize=13, ha='center', fontweight='bold', color='#3498DB')\n",
                                        "    \n",
                                        "    # LayerNorm 1\n",
                                        "    ax.add_patch(FancyBboxPatch((2.5, 4.8), 4, 0.8, boxstyle='round,pad=0.05',\n",
                                        "                                facecolor=colors['norm'], edgecolor='#8E44AD', linewidth=1.5))\n",
                                        "    ax.text(4.5, 5.2, 'LayerNorm', fontsize=10, ha='center')\n",
                                        "    \n",
                                        "    # Multi-Head Attention\n",
                                        "    ax.add_patch(FancyBboxPatch((2.5, 6), 4, 1.5, boxstyle='round,pad=0.1',\n",
                                        "                                facecolor=colors['attention'], edgecolor='#F39C12', linewidth=2))\n",
                                        "    ax.text(4.5, 7, 'Multi-Head', fontsize=11, ha='center', fontweight='bold')\n",
                                        "    ax.text(4.5, 6.4, 'Self-Attention', fontsize=11, ha='center', fontweight='bold')\n",
                                        "    \n",
                                        "    # æ®‹å·®è¿æ¥ 1\n",
                                        "    ax.annotate('', xy=(4.5, 8), xytext=(4.5, 7.6),\n",
                                        "               arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=1.5))\n",
                                        "    ax.plot([1.8, 1.8, 4.5], [5.2, 8, 8], color='#E74C3C', linewidth=2, linestyle='--')\n",
                                        "    ax.text(1.5, 6.5, '+', fontsize=16, color='#E74C3C', fontweight='bold')\n",
                                        "    \n",
                                        "    # LayerNorm 2\n",
                                        "    ax.add_patch(FancyBboxPatch((9.5, 4.8), 4, 0.8, boxstyle='round,pad=0.05',\n",
                                        "                                facecolor=colors['norm'], edgecolor='#8E44AD', linewidth=1.5))\n",
                                        "    ax.text(11.5, 5.2, 'LayerNorm', fontsize=10, ha='center')\n",
                                        "    \n",
                                        "    # FFN\n",
                                        "    ax.add_patch(FancyBboxPatch((9.5, 6), 4, 1.5, boxstyle='round,pad=0.1',\n",
                                        "                                facecolor=colors['ffn'], edgecolor='#E74C3C', linewidth=2))\n",
                                        "    ax.text(11.5, 7, 'Feed-Forward', fontsize=11, ha='center', fontweight='bold')\n",
                                        "    ax.text(11.5, 6.4, 'Network (MLP)', fontsize=11, ha='center', fontweight='bold')\n",
                                        "    \n",
                                        "    # æ®‹å·®è¿æ¥ 2\n",
                                        "    ax.annotate('', xy=(11.5, 8), xytext=(11.5, 7.6),\n",
                                        "               arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=1.5))\n",
                                        "    ax.plot([14.2, 14.2, 11.5], [5.2, 8, 8], color='#E74C3C', linewidth=2, linestyle='--')\n",
                                        "    ax.text(14.5, 6.5, '+', fontsize=16, color='#E74C3C', fontweight='bold')\n",
                                        "    \n",
                                        "    # è¿æ¥ä¸¤éƒ¨åˆ†\n",
                                        "    ax.annotate('', xy=(9.3, 5.2), xytext=(6.7, 5.2),\n",
                                        "               arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=1.5))\n",
                                        "    \n",
                                        "    # ===== è¾“å‡ºå±‚ =====\n",
                                        "    ax.annotate('', xy=(8, 10.2), xytext=(8, 9.8),\n",
                                        "               arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=2))\n",
                                        "    \n",
                                        "    ax.add_patch(FancyBboxPatch((4, 10.2), 8, 0.8, boxstyle='round,pad=0.05',\n",
                                        "                                facecolor=colors['norm'], edgecolor='#8E44AD', linewidth=1.5))\n",
                                        "    ax.text(8, 10.6, 'Final LayerNorm', fontsize=10, ha='center')\n",
                                        "    \n",
                                        "    ax.annotate('', xy=(8, 11.3), xytext=(8, 11),\n",
                                        "               arrowprops=dict(arrowstyle='->', color=colors['arrow'], lw=2))\n",
                                        "    \n",
                                        "    ax.add_patch(FancyBboxPatch((4, 11.3), 8, 0.6, boxstyle='round,pad=0.05',\n",
                                        "                                facecolor=colors['output'], edgecolor='#2980B9', linewidth=2))\n",
                                        "    ax.text(8, 11.6, 'LM Head â†’ Logits â†’ Predicted Next Token', fontsize=10, ha='center', fontweight='bold')\n",
                                        "    \n",
                                        "    # ===== å›¾ä¾‹ =====\n",
                                        "    legend_items = [\n",
                                        "        ('Residual Connection', '#E74C3C', '--'),\n",
                                        "        ('Data Flow', '#566573', '-'),\n",
                                        "    ]\n",
                                        "    for i, (label, color, style) in enumerate(legend_items):\n",
                                        "        ax.plot([0.5, 1.2], [11.5 - i*0.4, 11.5 - i*0.4], color=color, linewidth=2, linestyle=style)\n",
                                        "        ax.text(1.4, 11.5 - i*0.4, label, fontsize=9, va='center')\n",
                                        "    \n",
                                        "    plt.tight_layout()\n",
                                        "    plt.show()\n",
                                        "    \n",
                                        "    print(\"\\nğŸ“ GPT å…³é”®ç»„ä»¶ï¼š\")\n",
                                        "    print(\"   1. Token + Position Embedding â†’ æ³¨å…¥ä½ç½®ä¿¡æ¯\")\n",
                                        "    print(\"   2. N Ã— Transformer Block â†’ æå–è¯­ä¹‰ç‰¹å¾\")\n",
                                        "    print(\"   3. æ¯ä¸ªBlock: LayerNorm â†’ Attention â†’ æ®‹å·® â†’ LayerNorm â†’ FFN â†’ æ®‹å·®\")\n",
                                        "    print(\"   4. LM Head â†’ é¢„æµ‹è¯è¡¨ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡\")\n",
                                        "\n",
                                        "draw_gpt_architecture()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "f9d8ede8",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 1. ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡å‹\"è¿™æ˜¯ç¬¬å‡ ä¸ªè¯\"\n",
                                        "\n",
                                        "### ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ\n",
                                        "\n",
                                        "Attention æœºåˆ¶æ˜¯**æ— åºçš„**ï¼å®ƒåªçœ‹è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä¸çŸ¥é“è¯çš„é¡ºåºã€‚\n",
                                        "\n",
                                        "ä½†è¯­è¨€æ˜¯æœ‰é¡ºåºçš„ï¼š\"ç‹—å’¬äºº\" â‰  \"äººå’¬ç‹—\"\n",
                                        "\n",
                                        "### è§£å†³æ–¹æ¡ˆ\n",
                                        "\n",
                                        "ç»™æ¯ä¸ªä½ç½®ä¸€ä¸ªç‹¬ç‰¹çš„å‘é‡ï¼ŒåŠ åˆ° word embedding ä¸Šã€‚"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "bec6ff47",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class SinusoidalPositionalEncoding(nn.Module):\n",
                                        "    \"\"\"\n",
                                        "    æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆTransformer åŸè®ºæ–‡ï¼‰\n",
                                        "    \n",
                                        "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
                                        "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
                                        "    \"\"\"\n",
                                        "    def __init__(self, d_model, max_len=5000):\n",
                                        "        super().__init__()\n",
                                        "        \n",
                                        "        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ\n",
                                        "        pe = torch.zeros(max_len, d_model)\n",
                                        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
                                        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
                                        "        \n",
                                        "        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ç»´åº¦\n",
                                        "        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ç»´åº¦\n",
                                        "        \n",
                                        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
                                        "        self.register_buffer('pe', pe)\n",
                                        "    \n",
                                        "    def forward(self, x):\n",
                                        "        # x: [batch, seq_len, d_model]\n",
                                        "        return x + self.pe[:, :x.size(1), :]\n",
                                        "\n",
                                        "# å¯è§†åŒ–ä½ç½®ç¼–ç \n",
                                        "pos_enc = SinusoidalPositionalEncoding(d_model=64)\n",
                                        "pe = pos_enc.pe[0, :100, :].numpy()\n",
                                        "\n",
                                        "plt.figure(figsize=(15, 5))\n",
                                        "plt.imshow(pe.T, aspect='auto', cmap='RdBu_r')\n",
                                        "plt.colorbar()\n",
                                        "plt.xlabel('Position')\n",
                                        "plt.ylabel('Dimension')\n",
                                        "plt.title('Sinusoidal Positional Encoding')\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"æ¯ä¸ªä½ç½®æœ‰å”¯ä¸€çš„ç¼–ç æ¨¡å¼ï¼\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "f1258503",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 2. RoPEï¼šæ—‹è½¬ä½ç½®ç¼–ç \n",
                                        "\n",
                                        "ç°ä»£ LLMï¼ˆLLaMA, Mistralï¼‰éƒ½ç”¨ RoPEï¼Œä¼˜ç‚¹ï¼š\n",
                                        "- å¯ä»¥å¤–æ¨åˆ°æ›´é•¿çš„åºåˆ—\n",
                                        "- ç›¸å¯¹ä½ç½®ä¿¡æ¯æ›´è‡ªç„¶"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "02e77ac4",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def precompute_freqs_cis(dim, max_seq_len, theta=10000.0):\n",
                                        "    \"\"\"é¢„è®¡ç®— RoPE çš„é¢‘ç‡\"\"\"\n",
                                        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
                                        "    t = torch.arange(max_seq_len)\n",
                                        "    freqs = torch.outer(t, freqs)\n",
                                        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # e^(i*theta)\n",
                                        "    return freqs_cis\n",
                                        "\n",
                                        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
                                        "    \"\"\"åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç \"\"\"\n",
                                        "    # å°† real tensor è½¬æ¢ä¸º complex\n",
                                        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
                                        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
                                        "    \n",
                                        "    # æ—‹è½¬\n",
                                        "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2)  # [1, seq, 1, dim/2]\n",
                                        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(-2)\n",
                                        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(-2)\n",
                                        "    \n",
                                        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
                                        "\n",
                                        "# æµ‹è¯•\n",
                                        "dim = 64\n",
                                        "max_seq_len = 100\n",
                                        "freqs_cis = precompute_freqs_cis(dim, max_seq_len)\n",
                                        "\n",
                                        "print(f\"é¢‘ç‡å½¢çŠ¶: {freqs_cis.shape}\")\n",
                                        "print(\"RoPE æœ¬è´¨æ˜¯åœ¨å¤æ•°ç©ºé—´ä¸­æ—‹è½¬å‘é‡ï¼\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "8ade104f",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ– RoPE\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                                        "\n",
                                        "# ä¸åŒä½ç½®çš„ç›¸ä½\n",
                                        "phases = torch.angle(freqs_cis).numpy()\n",
                                        "axes[0].imshow(phases.T, aspect='auto', cmap='twilight')\n",
                                        "axes[0].set_xlabel('Position')\n",
                                        "axes[0].set_ylabel('Dimension')\n",
                                        "axes[0].set_title('RoPE Phases at Different Positions')\n",
                                        "\n",
                                        "# ç›¸é‚»ä½ç½®çš„å·®å¼‚\n",
                                        "pos_diff = phases[1:] - phases[:-1]\n",
                                        "axes[1].imshow(pos_diff.T, aspect='auto', cmap='RdBu_r')\n",
                                        "axes[1].set_xlabel('Position')\n",
                                        "axes[1].set_ylabel('Dimension')\n",
                                        "axes[1].set_title('Phase Difference Between Adjacent Positions')\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "8e727d0f",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 3. å®Œæ•´ GPT æ¨¡å‹"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "439632ae",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class CausalSelfAttention(nn.Module):\n",
                                        "    \"\"\"å¸¦æœ‰ causal mask çš„è‡ªæ³¨æ„åŠ›\"\"\"\n",
                                        "    def __init__(self, config):\n",
                                        "        super().__init__()\n",
                                        "        assert config['n_embd'] % config['n_head'] == 0\n",
                                        "        \n",
                                        "        self.n_head = config['n_head']\n",
                                        "        self.n_embd = config['n_embd']\n",
                                        "        self.head_dim = config['n_embd'] // config['n_head']\n",
                                        "        \n",
                                        "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
                                        "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
                                        "        self.dropout = nn.Dropout(config['dropout'])\n",
                                        "        \n",
                                        "        # Causal mask\n",
                                        "        self.register_buffer(\"mask\", torch.tril(\n",
                                        "            torch.ones(config['block_size'], config['block_size'])\n",
                                        "        ).view(1, 1, config['block_size'], config['block_size']))\n",
                                        "    \n",
                                        "    def forward(self, x):\n",
                                        "        B, T, C = x.size()\n",
                                        "        \n",
                                        "        # Q, K, V\n",
                                        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
                                        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
                                        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
                                        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
                                        "        \n",
                                        "        # Attention\n",
                                        "        att = (q @ k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
                                        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
                                        "        att = F.softmax(att, dim=-1)\n",
                                        "        att = self.dropout(att)\n",
                                        "        \n",
                                        "        y = att @ v\n",
                                        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
                                        "        return self.c_proj(y)\n",
                                        "\n",
                                        "class MLP(nn.Module):\n",
                                        "    def __init__(self, config):\n",
                                        "        super().__init__()\n",
                                        "        self.c_fc = nn.Linear(config['n_embd'], 4 * config['n_embd'])\n",
                                        "        self.c_proj = nn.Linear(4 * config['n_embd'], config['n_embd'])\n",
                                        "        self.dropout = nn.Dropout(config['dropout'])\n",
                                        "    \n",
                                        "    def forward(self, x):\n",
                                        "        x = F.gelu(self.c_fc(x))\n",
                                        "        x = self.dropout(self.c_proj(x))\n",
                                        "        return x\n",
                                        "\n",
                                        "class Block(nn.Module):\n",
                                        "    def __init__(self, config):\n",
                                        "        super().__init__()\n",
                                        "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
                                        "        self.attn = CausalSelfAttention(config)\n",
                                        "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
                                        "        self.mlp = MLP(config)\n",
                                        "    \n",
                                        "    def forward(self, x):\n",
                                        "        x = x + self.attn(self.ln_1(x))\n",
                                        "        x = x + self.mlp(self.ln_2(x))\n",
                                        "        return x"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "b132a501",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class GPT(nn.Module):\n",
                                        "    \"\"\"\n",
                                        "    GPT è¯­è¨€æ¨¡å‹\n",
                                        "    \"\"\"\n",
                                        "    def __init__(self, config):\n",
                                        "        super().__init__()\n",
                                        "        self.config = config\n",
                                        "        \n",
                                        "        self.transformer = nn.ModuleDict(dict(\n",
                                        "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),  # token embedding\n",
                                        "            wpe = nn.Embedding(config['block_size'], config['n_embd']), # position embedding\n",
                                        "            drop = nn.Dropout(config['dropout']),\n",
                                        "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
                                        "            ln_f = nn.LayerNorm(config['n_embd']),\n",
                                        "        ))\n",
                                        "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
                                        "        \n",
                                        "        # æƒé‡å…±äº«ï¼ˆå‡å°‘å‚æ•°ï¼‰\n",
                                        "        self.transformer.wte.weight = self.lm_head.weight\n",
                                        "        \n",
                                        "        # åˆå§‹åŒ–\n",
                                        "        self.apply(self._init_weights)\n",
                                        "        print(f\"GPT å‚æ•°é‡: {self.get_num_params():,}\")\n",
                                        "    \n",
                                        "    def _init_weights(self, module):\n",
                                        "        if isinstance(module, nn.Linear):\n",
                                        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                                        "            if module.bias is not None:\n",
                                        "                torch.nn.init.zeros_(module.bias)\n",
                                        "        elif isinstance(module, nn.Embedding):\n",
                                        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                                        "    \n",
                                        "    def get_num_params(self):\n",
                                        "        return sum(p.numel() for p in self.parameters())\n",
                                        "    \n",
                                        "    def forward(self, idx, targets=None):\n",
                                        "        B, T = idx.size()\n",
                                        "        assert T <= self.config['block_size']\n",
                                        "        \n",
                                        "        # Embeddings\n",
                                        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
                                        "        tok_emb = self.transformer.wte(idx)  # [B, T, n_embd]\n",
                                        "        pos_emb = self.transformer.wpe(pos)  # [T, n_embd]\n",
                                        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
                                        "        \n",
                                        "        # Transformer blocks\n",
                                        "        for block in self.transformer.h:\n",
                                        "            x = block(x)\n",
                                        "        x = self.transformer.ln_f(x)\n",
                                        "        \n",
                                        "        # è¾“å‡º logits\n",
                                        "        logits = self.lm_head(x)  # [B, T, vocab_size]\n",
                                        "        \n",
                                        "        # è®¡ç®— lossï¼ˆå¦‚æœæœ‰ç›®æ ‡ï¼‰\n",
                                        "        loss = None\n",
                                        "        if targets is not None:\n",
                                        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
                                        "        \n",
                                        "        return logits, loss\n",
                                        "\n",
                                        "# åˆ›å»ºä¸€ä¸ªå°å‹ GPT\n",
                                        "config = {\n",
                                        "    'vocab_size': 1000,\n",
                                        "    'block_size': 128,   # æœ€å¤§åºåˆ—é•¿åº¦\n",
                                        "    'n_layer': 4,        # Transformer å±‚æ•°\n",
                                        "    'n_head': 4,         # æ³¨æ„åŠ›å¤´æ•°\n",
                                        "    'n_embd': 128,       # åµŒå…¥ç»´åº¦\n",
                                        "    'dropout': 0.1,\n",
                                        "}\n",
                                        "\n",
                                        "model = GPT(config)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "e6fde0fc",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 4. æ–‡æœ¬ç”Ÿæˆï¼šé‡‡æ ·ç­–ç•¥\n",
                                        "\n",
                                        "æœ‰äº†æ¨¡å‹åï¼Œå¦‚ä½•ç”Ÿæˆæ–‡æœ¬ï¼Ÿ\n",
                                        "\n",
                                        "å…³é”®åœ¨äº**å¦‚ä½•ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·**ã€‚"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "5c41532f",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "@torch.no_grad()\n",
                                        "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None):\n",
                                        "    \"\"\"\n",
                                        "    è‡ªå›å½’ç”Ÿæˆ\n",
                                        "    \n",
                                        "    idx: [B, T] åˆå§‹ token åºåˆ—\n",
                                        "    temperature: æ§åˆ¶éšæœºæ€§ï¼ˆè¶Šå¤§è¶Šéšæœºï¼‰\n",
                                        "    top_k: åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ªä¸­é‡‡æ ·\n",
                                        "    top_p: åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„ token ä¸­é‡‡æ ·\n",
                                        "    \"\"\"\n",
                                        "    model.eval()\n",
                                        "    \n",
                                        "    for _ in range(max_new_tokens):\n",
                                        "        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦\n",
                                        "        idx_cond = idx if idx.size(1) <= model.config['block_size'] else idx[:, -model.config['block_size']:]\n",
                                        "        \n",
                                        "        # å‰å‘ä¼ æ’­å¾—åˆ° logits\n",
                                        "        logits, _ = model(idx_cond)\n",
                                        "        logits = logits[:, -1, :]  # åªå–æœ€åä¸€ä¸ªä½ç½® [B, vocab_size]\n",
                                        "        \n",
                                        "        # Temperature scaling\n",
                                        "        logits = logits / temperature\n",
                                        "        \n",
                                        "        # Top-k è¿‡æ»¤\n",
                                        "        if top_k is not None:\n",
                                        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
                                        "            logits[logits < v[:, [-1]]] = float('-inf')\n",
                                        "        \n",
                                        "        # Top-p (nucleus) è¿‡æ»¤\n",
                                        "        if top_p is not None:\n",
                                        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
                                        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
                                        "            \n",
                                        "            # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ token\n",
                                        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
                                        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
                                        "            sorted_indices_to_remove[..., 0] = 0\n",
                                        "            \n",
                                        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
                                        "            logits[indices_to_remove] = float('-inf')\n",
                                        "        \n",
                                        "        # é‡‡æ ·\n",
                                        "        probs = F.softmax(logits, dim=-1)\n",
                                        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
                                        "        \n",
                                        "        # æ‹¼æ¥\n",
                                        "        idx = torch.cat((idx, idx_next), dim=1)\n",
                                        "    \n",
                                        "    return idx\n",
                                        "\n",
                                        "print(\"ç”Ÿæˆå‡½æ•°å®šä¹‰å®Œæˆï¼\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "efb4b16b",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ–ä¸åŒ temperature çš„æ•ˆæœ\n",
                                        "logits = torch.tensor([2.0, 1.5, 1.0, 0.5, 0.0, -0.5, -1.0])\n",
                                        "temperatures = [0.5, 1.0, 1.5, 2.0]\n",
                                        "\n",
                                        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
                                        "\n",
                                        "for i, temp in enumerate(temperatures):\n",
                                        "    probs = F.softmax(logits / temp, dim=-1).numpy()\n",
                                        "    axes[i].bar(range(len(probs)), probs, color='steelblue')\n",
                                        "    axes[i].set_title(f'Temperature = {temp}')\n",
                                        "    axes[i].set_xlabel('Token')\n",
                                        "    axes[i].set_ylabel('Probability')\n",
                                        "    axes[i].set_ylim(0, 1)\n",
                                        "\n",
                                        "plt.suptitle('Temperature å¯¹æ¦‚ç‡åˆ†å¸ƒçš„å½±å“', fontsize=14)\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"Temperature:\")\n",
                                        "print(\"  ä½ (0.5): æ›´ç¡®å®šæ€§ï¼Œé€‰æ‹©é«˜æ¦‚ç‡ token\")\n",
                                        "print(\"  é«˜ (2.0): æ›´éšæœºï¼Œæ¦‚ç‡åˆ†å¸ƒæ›´å¹³å¦\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "f5d34b13",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ– Top-k å’Œ Top-p\n",
                                        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                                        "\n",
                                        "# åŸå§‹åˆ†å¸ƒ\n",
                                        "probs = F.softmax(logits, dim=-1).numpy()\n",
                                        "axes[0].bar(range(len(probs)), probs, color='steelblue')\n",
                                        "axes[0].set_title('Original Distribution')\n",
                                        "axes[0].set_xlabel('Token')\n",
                                        "axes[0].set_ylabel('Probability')\n",
                                        "\n",
                                        "# Top-k = 3\n",
                                        "top_k_probs = probs.copy()\n",
                                        "top_k_probs[np.argsort(top_k_probs)[:-3]] = 0\n",
                                        "top_k_probs = top_k_probs / top_k_probs.sum()\n",
                                        "axes[1].bar(range(len(top_k_probs)), top_k_probs, color='coral')\n",
                                        "axes[1].set_title('Top-k = 3')\n",
                                        "axes[1].set_xlabel('Token')\n",
                                        "axes[1].set_ylabel('Probability')\n",
                                        "\n",
                                        "# Top-p = 0.8\n",
                                        "sorted_probs = np.sort(probs)[::-1]\n",
                                        "cumsum = np.cumsum(sorted_probs)\n",
                                        "cutoff_idx = np.searchsorted(cumsum, 0.8) + 1\n",
                                        "top_p_probs = probs.copy()\n",
                                        "threshold = sorted_probs[min(cutoff_idx, len(sorted_probs)-1)]\n",
                                        "top_p_probs[probs < threshold] = 0\n",
                                        "top_p_probs = top_p_probs / top_p_probs.sum()\n",
                                        "axes[2].bar(range(len(top_p_probs)), top_p_probs, color='forestgreen')\n",
                                        "axes[2].set_title('Top-p = 0.8')\n",
                                        "axes[2].set_xlabel('Token')\n",
                                        "axes[2].set_ylabel('Probability')\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"Top-k: åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„ k ä¸ª\")\n",
                                        "print(\"Top-p: ä¿ç•™ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„ token\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "3346650a",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# æµ‹è¯•ç”Ÿæˆï¼ˆéšæœºè¾“å…¥ï¼‰\n",
                                        "model.eval()\n",
                                        "idx = torch.randint(0, config['vocab_size'], (1, 5))  # 5ä¸ªåˆå§‹token\n",
                                        "\n",
                                        "print(\"åˆå§‹åºåˆ—:\", idx[0].tolist())\n",
                                        "\n",
                                        "# ä¸åŒé‡‡æ ·ç­–ç•¥\n",
                                        "generated_greedy = generate(model, idx.clone(), max_new_tokens=10, temperature=0.1)\n",
                                        "generated_random = generate(model, idx.clone(), max_new_tokens=10, temperature=1.5)\n",
                                        "generated_topk = generate(model, idx.clone(), max_new_tokens=10, temperature=1.0, top_k=5)\n",
                                        "generated_topp = generate(model, idx.clone(), max_new_tokens=10, temperature=1.0, top_p=0.9)\n",
                                        "\n",
                                        "print(f\"\\nGreedy (temp=0.1): {generated_greedy[0].tolist()}\")\n",
                                        "print(f\"Random (temp=1.5): {generated_random[0].tolist()}\")\n",
                                        "print(f\"Top-k (k=5):       {generated_topk[0].tolist()}\")\n",
                                        "print(f\"Top-p (p=0.9):     {generated_topp[0].tolist()}\")\n",
                                        "\n",
                                        "print(\"\\næ³¨æ„ï¼šæ¨¡å‹æœªè®­ç»ƒï¼Œç”Ÿæˆçš„æ˜¯éšæœºæ•°å­—\")\n",
                                        "print(\"è®­ç»ƒåæ‰èƒ½ç”Ÿæˆæœ‰æ„ä¹‰çš„æ–‡æœ¬ï¼\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "3b7bc397",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## æœ¬ç« æ€»ç»“\n",
                                        "\n",
                                        "\n",
                                        "1. **ä½ç½®ç¼–ç **\n",
                                        "   - æ­£å¼¦ä½ç½®ç¼–ç ï¼šå›ºå®šçš„ï¼ŒåŸºäºä¸‰è§’å‡½æ•°\n",
                                        "   - RoPEï¼šå¯å­¦ä¹ çš„ï¼Œæ›´å¥½çš„é•¿åº¦å¤–æ¨\n",
                                        "\n",
                                        "2. **GPT æ¶æ„**\n",
                                        "   - Token + Position Embedding\n",
                                        "   - N ä¸ª Transformer Block\n",
                                        "   - LM Head é¢„æµ‹ä¸‹ä¸€ä¸ª token\n",
                                        "\n",
                                        "3. **é‡‡æ ·ç­–ç•¥**\n",
                                        "   - Temperatureï¼šæ§åˆ¶éšæœºæ€§\n",
                                        "   - Top-kï¼šåªä» k ä¸ªæœ€é«˜æ¦‚ç‡ä¸­é€‰\n",
                                        "   - Top-pï¼šä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„é›†åˆä¸­é€‰\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "095bda23",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## æ€è€ƒ\n",
                                        "\n",
                                        "1. **å¢åŠ æ¨¡å‹è§„æ¨¡**ï¼šå°è¯•æ›´å¤šå±‚ã€æ›´å¤§çš„ n_embd\n",
                                        "2. **å¯¹æ¯”é‡‡æ ·ç­–ç•¥**ï¼šåœ¨åŒä¸€ä¸ª prompt ä¸Šæµ‹è¯•ä¸åŒç­–ç•¥\n",
                                        "3. **å®ç° RoPE**ï¼šæŠŠä½ç½®ç¼–ç æ¢æˆ RoPE"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "8f65d6f7",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ç»ƒä¹ ç©ºé—´\n",
                                        "\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
