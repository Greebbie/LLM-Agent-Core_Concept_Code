{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "# Ch9 | LoRA 与量化：面向消费级 GPU 的高效微调\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "**目标：** 在有限 GPU 显存下学习如何微调大模型\n",
                                        "\n",
                                        "**本 notebook 使用 PEFT 和 GPT-2 演示真实的 LoRA 微调。**\n",
                                        "\n",
                                        "**设备建议：** GPU；CPU 仅建议阅读/演示。\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## 内容\n",
                                        "\n",
                                        "1. **LoRA 原理**：低秩矩阵分解\n",
                                        "2. **从零实现**：手动实现 LoRA\n",
                                        "3. **量化**：4-bit 与 8-bit 精度\n",
                                        "4. **真实训练**：使用 PEFT 进行 LoRA 微调\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 挑战：微调大模型\n",
                                        "\n",
                                        "### 全量微调的显存需求\n",
                                        "\n",
                                        "以 7B 模型为例：\n",
                                        "```\n",
                                        "模型参数: 7B × 2 bytes (FP16) = 14 GB\n",
                                        "梯度:     7B × 2 bytes = 14 GB  \n",
                                        "优化器(Adam): 7B × 8 bytes = 56 GB\n",
                                        "----------------------------------------\n",
                                        "总计: 约 84 GB GPU 显存\n",
                                        "```\n",
                                        "\n",
                                        "**消费级 GPU（16-24GB）无法承受！**\n",
                                        "\n",
                                        "### 解决方案\n",
                                        "\n",
                                        "| 技术 | 显存节省 | 原理 |\n",
                                        "|-----------|---------------|-------------|\n",
                                        "| **LoRA** | 可训练参数减少约 100x | 低秩分解 |\n",
                                        "| **量化** | 2-8x | 降低精度 |\n",
                                        "| **QLoRA** | 组合方案 | 两者结合 |\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 环境准备\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "from torch.utils.data import Dataset, DataLoader\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "from typing import Dict, List, Optional\n",
                                        "import warnings\n",
                                        "warnings.filterwarnings('ignore')\n",
                                        "\n",
                                        "torch.manual_seed(42)\n",
                                        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                                        "print(f\"Using device: {device}\")\n",
                                        "\n",
                                        "if device == 'cuda':\n",
                                        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                                        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Install required packages\n",
                                        "try:\n",
                                        "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
                                        "    print(\"Transformers loaded!\")\n",
                                        "except ImportError:\n",
                                        "    !pip install transformers accelerate -q\n",
                                        "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
                                        "\n",
                                        "try:\n",
                                        "    from peft import LoraConfig, get_peft_model, TaskType, PeftModel, prepare_model_for_kbit_training\n",
                                        "    PEFT_AVAILABLE = True\n",
                                        "    print(\"PEFT library loaded!\")\n",
                                        "except ImportError:\n",
                                        "    PEFT_AVAILABLE = False\n",
                                        "    print(\"PEFT not available - install with: pip install peft\")\n",
                                        "\n",
                                        "try:\n",
                                        "    import bitsandbytes as bnb\n",
                                        "    BNB_AVAILABLE = True\n",
                                        "    print(\"bitsandbytes loaded (for quantization)!\")\n",
                                        "except Exception as e:\n",
                                        "    import sys\n",
                                        "    BNB_AVAILABLE = False\n",
                                        "    print(f\"bitsandbytes import failed: {type(e).__name__}: {e}\")\n",
                                        "    print(f\"Python: {sys.executable}\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 1：理解 LoRA\n",
                                        "\n",
                                        "### 核心思想\n",
                                        "\n",
                                        "不训练所有权重时，LoRA 会：\n",
                                        "1. **冻结** 原始的预训练权重\n",
                                        "2. **新增** 一个小的可训练“增量”（delta）\n",
                                        "\n",
                                        "$$W_{new} = W_{frozen} + \\Delta W$$\n",
                                        "\n",
                                        "### 低秩分解\n",
                                        "\n",
                                        "关键洞见：$\\Delta W$ 可以表示为：\n",
                                        "\n",
                                        "$$\\Delta W = B \\times A$$\n",
                                        "\n",
                                        "其中：\n",
                                        "- $A$：[r, input_dim] —— “下投影”\n",
                                        "- $B$：[output_dim, r] —— “上投影”\n",
                                        "- $r$：秩（通常 4-64，远小于原维度）\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Visualize LoRA parameter savings\n",
                                        "\n",
                                        "def calculate_params(d_in, d_out, rank):\n",
                                        "    \"\"\"Calculate parameters for full vs LoRA\"\"\"\n",
                                        "    full_params = d_in * d_out\n",
                                        "    lora_params = d_in * rank + rank * d_out\n",
                                        "    return full_params, lora_params\n",
                                        "\n",
                                        "dimensions = [768, 1024, 2048, 4096, 8192]\n",
                                        "ranks = [4, 8, 16, 32]\n",
                                        "\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                                        "\n",
                                        "# Plot 1: Parameter comparison for rank=16\n",
                                        "rank = 16\n",
                                        "full_params = [d * d for d in dimensions]\n",
                                        "lora_params = [d * rank + rank * d for d in dimensions]\n",
                                        "\n",
                                        "x = np.arange(len(dimensions))\n",
                                        "width = 0.35\n",
                                        "\n",
                                        "axes[0].bar(x - width/2, np.array(full_params)/1e6, width, label='Full Fine-tuning', color='coral')\n",
                                        "axes[0].bar(x + width/2, np.array(lora_params)/1e6, width, label=f'LoRA (rank={rank})', color='green')\n",
                                        "axes[0].set_xlabel('Hidden Dimension')\n",
                                        "axes[0].set_ylabel('Parameters (Millions)')\n",
                                        "axes[0].set_xticks(x)\n",
                                        "axes[0].set_xticklabels(dimensions)\n",
                                        "axes[0].set_title('Trainable Parameters Comparison')\n",
                                        "axes[0].legend()\n",
                                        "axes[0].set_yscale('log')\n",
                                        "\n",
                                        "# Plot 2: Compression ratio vs rank\n",
                                        "d = 4096  # GPT-2 large dimension\n",
                                        "compression_ratios = []\n",
                                        "for r in range(1, 65):\n",
                                        "    full, lora = calculate_params(d, d, r)\n",
                                        "    compression_ratios.append(full / lora)\n",
                                        "\n",
                                        "axes[1].plot(range(1, 65), compression_ratios, 'b-', linewidth=2)\n",
                                        "axes[1].axhline(y=128, color='r', linestyle='--', alpha=0.5, label='128x (rank=16)')\n",
                                        "axes[1].axhline(y=64, color='orange', linestyle='--', alpha=0.5, label='64x (rank=32)')\n",
                                        "axes[1].set_xlabel('LoRA Rank')\n",
                                        "axes[1].set_ylabel('Compression Ratio')\n",
                                        "axes[1].set_title(f'Parameter Reduction vs Rank (d={d})')\n",
                                        "axes[1].legend()\n",
                                        "axes[1].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "# Print specific examples\n",
                                        "print(\"\\nParameter comparison for d=4096 (typical LLM hidden size):\")\n",
                                        "for r in [4, 8, 16, 32]:\n",
                                        "    full, lora = calculate_params(4096, 4096, r)\n",
                                        "    print(f\"  rank={r:2d}: Full={full/1e6:.1f}M, LoRA={lora/1e3:.0f}K, Compression={full/lora:.0f}x\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 2：从零实现 LoRA\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class LoRALinear(nn.Module):\n",
                                        "    \"\"\"\n",
                                        "    LoRA implementation for a Linear layer.\n",
                                        "    \n",
                                        "    Mathematical formulation:\n",
                                        "        y = Wx + (BA)x * scaling\n",
                                        "        \n",
                                        "    Where:\n",
                                        "        W: Original frozen weights [out_features, in_features]\n",
                                        "        A: Down projection [rank, in_features]\n",
                                        "        B: Up projection [out_features, rank]\n",
                                        "        scaling: alpha / rank (controls magnitude of LoRA contribution)\n",
                                        "    \"\"\"\n",
                                        "    \n",
                                        "    def __init__(\n",
                                        "        self, \n",
                                        "        in_features: int, \n",
                                        "        out_features: int, \n",
                                        "        rank: int = 4,\n",
                                        "        alpha: float = 1.0,\n",
                                        "        dropout: float = 0.0\n",
                                        "    ):\n",
                                        "        super().__init__()\n",
                                        "        \n",
                                        "        self.in_features = in_features\n",
                                        "        self.out_features = out_features\n",
                                        "        self.rank = rank\n",
                                        "        self.alpha = alpha\n",
                                        "        self.scaling = alpha / rank\n",
                                        "        \n",
                                        "        # Original frozen linear layer\n",
                                        "        self.linear = nn.Linear(in_features, out_features)\n",
                                        "        self.linear.weight.requires_grad = False\n",
                                        "        if self.linear.bias is not None:\n",
                                        "            self.linear.bias.requires_grad = False\n",
                                        "        \n",
                                        "        # LoRA matrices\n",
                                        "        # A: initialized with small random values (Kaiming)\n",
                                        "        # B: initialized to zero (ensures ΔW = 0 at start)\n",
                                        "        self.lora_A = nn.Parameter(torch.empty(rank, in_features))\n",
                                        "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
                                        "        \n",
                                        "        # Initialize A with Kaiming uniform\n",
                                        "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
                                        "        \n",
                                        "        # Optional dropout\n",
                                        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
                                        "    \n",
                                        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                                        "        \"\"\"\n",
                                        "        Forward pass: y = Wx + (BA)x * scaling\n",
                                        "        \"\"\"\n",
                                        "        # Original output (frozen)\n",
                                        "        original_output = self.linear(x)\n",
                                        "        \n",
                                        "        # LoRA contribution: x @ A^T @ B^T * scaling\n",
                                        "        lora_output = self.dropout(x)\n",
                                        "        lora_output = F.linear(lora_output, self.lora_A)  # x @ A^T\n",
                                        "        lora_output = F.linear(lora_output, self.lora_B)  # (xA^T) @ B^T\n",
                                        "        lora_output = lora_output * self.scaling\n",
                                        "        \n",
                                        "        return original_output + lora_output\n",
                                        "    \n",
                                        "    def merge_weights(self):\n",
                                        "        \"\"\"\n",
                                        "        Merge LoRA weights into the original weights for efficient inference.\n",
                                        "        After merging, no additional computation is needed.\n",
                                        "        \"\"\"\n",
                                        "        with torch.no_grad():\n",
                                        "            # W_new = W + (BA) * scaling\n",
                                        "            delta_w = (self.lora_B @ self.lora_A) * self.scaling\n",
                                        "            self.linear.weight.add_(delta_w)\n",
                                        "    \n",
                                        "    def get_trainable_params(self) -> int:\n",
                                        "        return self.lora_A.numel() + self.lora_B.numel()\n",
                                        "    \n",
                                        "    def get_frozen_params(self) -> int:\n",
                                        "        params = self.linear.weight.numel()\n",
                                        "        if self.linear.bias is not None:\n",
                                        "            params += self.linear.bias.numel()\n",
                                        "        return params\n",
                                        "    \n",
                                        "    def __repr__(self):\n",
                                        "        return (\n",
                                        "            f\"LoRALinear(in={self.in_features}, out={self.out_features}, \"\n",
                                        "            f\"rank={self.rank}, alpha={self.alpha})\"\n",
                                        "        )"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Test our LoRA implementation\n",
                                        "print(\"Testing LoRA Linear Layer\")\n",
                                        "print(\"=\" * 50)\n",
                                        "\n",
                                        "lora_layer = LoRALinear(1024, 1024, rank=16, alpha=32)\n",
                                        "print(f\"Layer: {lora_layer}\")\n",
                                        "print(f\"\\nParameter counts:\")\n",
                                        "print(f\"  Frozen params: {lora_layer.get_frozen_params():,}\")\n",
                                        "print(f\"  Trainable params: {lora_layer.get_trainable_params():,}\")\n",
                                        "print(f\"  Compression: {lora_layer.get_frozen_params() / lora_layer.get_trainable_params():.1f}x\")\n",
                                        "\n",
                                        "# Test forward pass\n",
                                        "x = torch.randn(2, 10, 1024)\n",
                                        "y = lora_layer(x)\n",
                                        "print(f\"\\nForward pass:\")\n",
                                        "print(f\"  Input shape: {x.shape}\")\n",
                                        "print(f\"  Output shape: {y.shape}\")\n",
                                        "\n",
                                        "# Verify initialization (B=0 means ΔW=0 initially)\n",
                                        "original_out = lora_layer.linear(x)\n",
                                        "diff = (y - original_out).abs().max().item()\n",
                                        "print(f\"\\nInitialization check:\")\n",
                                        "print(f\"  Max difference from original: {diff:.6f}\")\n",
                                        "print(f\"  (Should be ~0 because B is initialized to zero)\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Verify gradient flow\n",
                                        "print(\"Gradient Flow Verification\")\n",
                                        "print(\"=\" * 50)\n",
                                        "\n",
                                        "lora_layer = LoRALinear(512, 512, rank=8)\n",
                                        "x = torch.randn(4, 8, 512, requires_grad=True)\n",
                                        "y = lora_layer(x)\n",
                                        "loss = y.sum()\n",
                                        "loss.backward()\n",
                                        "\n",
                                        "print(f\"Frozen weight gradient: {lora_layer.linear.weight.grad}\")\n",
                                        "print(f\"LoRA A gradient shape: {lora_layer.lora_A.grad.shape}\")\n",
                                        "print(f\"LoRA B gradient shape: {lora_layer.lora_B.grad.shape}\")\n",
                                        "print(f\"\\nGradients flow to LoRA params but NOT to frozen weights!\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 3：量化基础\n",
                                        "\n",
                                        "量化通过使用更低精度来降低显存占用：\n",
                                        "\n",
                                        "| 精度 | 字节/参数 | 7B 模型大小 |\n",
                                        "|-----------|-------------|---------------|\n",
                                        "| FP32 | 4 | 28 GB |\n",
                                        "| FP16 | 2 | 14 GB |\n",
                                        "| INT8 | 1 | 7 GB |\n",
                                        "| INT4 | 0.5 | 3.5 GB |\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def quantize_tensor_symmetric(tensor: torch.Tensor, bits: int = 8) -> tuple:\n",
                                        "    \"\"\"\n",
                                        "    Symmetric quantization: maps values to [-2^(bits-1), 2^(bits-1) - 1]\n",
                                        "    \n",
                                        "    Returns:\n",
                                        "        quantized tensor, scale factor\n",
                                        "    \"\"\"\n",
                                        "    # Find the maximum absolute value\n",
                                        "    max_val = tensor.abs().max()\n",
                                        "    \n",
                                        "    # Calculate scale\n",
                                        "    q_max = 2 ** (bits - 1) - 1\n",
                                        "    scale = max_val / q_max\n",
                                        "    \n",
                                        "    # Quantize\n",
                                        "    quantized = (tensor / scale).round().clamp(-q_max, q_max)\n",
                                        "    \n",
                                        "    return quantized.to(torch.int8), scale\n",
                                        "\n",
                                        "\n",
                                        "def dequantize_tensor(quantized: torch.Tensor, scale: float) -> torch.Tensor:\n",
                                        "    \"\"\"Dequantize back to float\"\"\"\n",
                                        "    return quantized.float() * scale\n",
                                        "\n",
                                        "\n",
                                        "def quantize_per_channel(tensor: torch.Tensor, bits: int = 8) -> tuple:\n",
                                        "    \"\"\"\n",
                                        "    Per-channel quantization: each output channel has its own scale.\n",
                                        "    More accurate than per-tensor quantization.\n",
                                        "    \"\"\"\n",
                                        "    # For weight matrix [out_features, in_features], quantize per output channel\n",
                                        "    max_vals = tensor.abs().max(dim=1, keepdim=True)[0]\n",
                                        "    \n",
                                        "    q_max = 2 ** (bits - 1) - 1\n",
                                        "    scales = max_vals / q_max\n",
                                        "    scales = scales.clamp(min=1e-8)  # Avoid division by zero\n",
                                        "    \n",
                                        "    quantized = (tensor / scales).round().clamp(-q_max, q_max)\n",
                                        "    \n",
                                        "    return quantized.to(torch.int8), scales.squeeze()\n",
                                        "\n",
                                        "\n",
                                        "# Test quantization\n",
                                        "print(\"Quantization Test\")\n",
                                        "print(\"=\" * 50)\n",
                                        "\n",
                                        "# Create a sample weight matrix\n",
                                        "original = torch.randn(512, 512)\n",
                                        "\n",
                                        "# Test different quantization methods\n",
                                        "for bits in [8, 4]:\n",
                                        "    q_tensor, scale = quantize_tensor_symmetric(original, bits)\n",
                                        "    reconstructed = dequantize_tensor(q_tensor, scale)\n",
                                        "    error = (original - reconstructed).abs()\n",
                                        "    \n",
                                        "    print(f\"\\n{bits}-bit symmetric quantization:\")\n",
                                        "    print(f\"  Original size: {original.numel() * 4 / 1024:.1f} KB (FP32)\")\n",
                                        "    print(f\"  Quantized size: {q_tensor.numel() * (bits/8) / 1024:.1f} KB\")\n",
                                        "    print(f\"  Compression: {32/bits:.1f}x\")\n",
                                        "    print(f\"  Mean error: {error.mean():.6f}\")\n",
                                        "    print(f\"  Max error: {error.max():.6f}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Visualize quantization effects\n",
                                        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                                        "\n",
                                        "original = torch.randn(1000, 1000)\n",
                                        "\n",
                                        "# Original distribution\n",
                                        "axes[0, 0].hist(original.flatten().numpy(), bins=100, color='blue', alpha=0.7)\n",
                                        "axes[0, 0].set_title('Original Weight Distribution (FP32)')\n",
                                        "axes[0, 0].set_xlabel('Value')\n",
                                        "\n",
                                        "# 8-bit quantization\n",
                                        "q8, s8 = quantize_tensor_symmetric(original, 8)\n",
                                        "r8 = dequantize_tensor(q8, s8)\n",
                                        "axes[0, 1].hist(r8.flatten().numpy(), bins=100, color='green', alpha=0.7)\n",
                                        "axes[0, 1].set_title('After 8-bit Quantization')\n",
                                        "axes[0, 1].set_xlabel('Value')\n",
                                        "\n",
                                        "# 4-bit quantization\n",
                                        "q4, s4 = quantize_tensor_symmetric(original, 4)\n",
                                        "r4 = dequantize_tensor(q4, s4)\n",
                                        "axes[1, 0].hist(r4.flatten().numpy(), bins=100, color='orange', alpha=0.7)\n",
                                        "axes[1, 0].set_title('After 4-bit Quantization')\n",
                                        "axes[1, 0].set_xlabel('Value')\n",
                                        "\n",
                                        "# Error comparison\n",
                                        "error_8 = (original - r8).abs().flatten().numpy()\n",
                                        "error_4 = (original - r4).abs().flatten().numpy()\n",
                                        "axes[1, 1].hist(error_8, bins=50, alpha=0.5, label='8-bit error', color='green')\n",
                                        "axes[1, 1].hist(error_4, bins=50, alpha=0.5, label='4-bit error', color='orange')\n",
                                        "axes[1, 1].set_title('Quantization Error Distribution')\n",
                                        "axes[1, 1].set_xlabel('Absolute Error')\n",
                                        "axes[1, 1].legend()\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 4：使用 PEFT 进行真实 LoRA 微调\n",
                                        "\n",
                                        "现在我们在 GPT-2 上进行实际的 LoRA 微调！\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Load Chinese GPT-2 model and tokenizer\n",
                                        "model_name = \"uer/gpt2-chinese-cluecorpussmall\"  # Chinese GPT-2 base\n",
                                        "\n",
                                        "print(f\"Loading {model_name}...\")\n",
                                        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                                        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
                                        "\n",
                                        "# Ensure the tokenizer has a pad token\n",
                                        "if tokenizer.pad_token is None:\n",
                                        "    if tokenizer.eos_token is not None:\n",
                                        "        tokenizer.pad_token = tokenizer.eos_token\n",
                                        "    else:\n",
                                        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
                                        "        model.resize_token_embeddings(len(tokenizer))\n",
                                        "\n",
                                        "model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "total_params = sum(p.numel() for p in model.parameters())\n",
                                        "print(f\"Model loaded: {total_params:,} parameters\")\n",
                                        "print(f\"Memory: {total_params * 4 / 1e9:.2f} GB (FP32)\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "if PEFT_AVAILABLE:\n",
                                        "    # Configure LoRA\n",
                                        "    lora_config = LoraConfig(\n",
                                        "        task_type=TaskType.CAUSAL_LM,\n",
                                        "        r=16,  # Rank\n",
                                        "        lora_alpha=32,  # Scaling factor\n",
                                        "        lora_dropout=0.1,\n",
                                        "        # Which layers to apply LoRA to\n",
                                        "        target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 attention layers\n",
                                        "        bias=\"none\",\n",
                                        "    )\n",
                                        "    \n",
                                        "    print(\"LoRA Configuration:\")\n",
                                        "    print(f\"  Rank (r): {lora_config.r}\")\n",
                                        "    print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
                                        "    print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
                                        "    print(f\"  Target modules: {lora_config.target_modules}\")\n",
                                        "else:\n",
                                        "    print(\"PEFT not available. Install with: pip install peft\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "if PEFT_AVAILABLE:\n",
                                        "    # Create LoRA model\n",
                                        "    lora_model = get_peft_model(model, lora_config)\n",
                                        "    \n",
                                        "    # Print trainable parameters\n",
                                        "    lora_model.print_trainable_parameters()\n",
                                        "    \n",
                                        "    # Get detailed parameter info\n",
                                        "    trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
                                        "    all_params = sum(p.numel() for p in lora_model.parameters())\n",
                                        "    \n",
                                        "    print(f\"\\nDetailed breakdown:\")\n",
                                        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
                                        "    print(f\"  Total parameters: {all_params:,}\")\n",
                                        "    print(f\"  Trainable %: {100 * trainable_params / all_params:.4f}%\")\n",
                                        "    print(f\"  Memory for LoRA params: {trainable_params * 4 / 1e6:.2f} MB\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Prepare training data\n",
                                        "import random\n",
                                        "\n",
                                        "ISSUE_LABELS = [\"延迟发货\", \"退款申请\", \"地址修改\", \"物流异常\", \"售后咨询\", \"发票问题\"]\n",
                                        "ISSUE_PHRASES = {\n",
                                        "    \"延迟发货\": [\"迟迟未发货\", \"发货很久没消息\", \"订单一直待发货\"],\n",
                                        "    \"退款申请\": [\"想申请退款\", \"希望退货退款\", \"申请退款处理\"],\n",
                                        "    \"地址修改\": [\"想修改收货地址\", \"地址填错需要改\", \"收货地址需要调整\"],\n",
                                        "    \"物流异常\": [\"物流一直不更新\", \"包裹疑似丢件\", \"物流状态异常\"],\n",
                                        "    \"售后咨询\": [\"咨询售后流程\", \"售后政策想了解\", \"需要售后支持\"],\n",
                                        "    \"发票问题\": [\"发票怎么开\", \"需要补开发票\", \"发票信息有误\"],\n",
                                        "}\n",
                                        "NAME_POOL = [\"张伟\", \"王芳\", \"李娜\", \"刘洋\", \"陈磊\", \"赵敏\", \"周凯\", \"吴磊\", \"郑强\", \"孙婷\", \"马超\", \"胡静\"]\n",
                                        "EMAIL_DOMAINS = [\"example.com\", \"mail.com\", \"email.cn\"]\n",
                                        "TEXT_TEMPLATES = [\n",
                                        "    \"订单{order_id}相关用户{name}邮箱{email}，反馈：{detail}。\",\n",
                                        "    \"客户{name}（{email}）反馈订单{order_id}，情况：{detail}。\",\n",
                                        "    \"用户{name}邮箱{email}，订单号{order_id}需要处理：{detail}。\",\n",
                                        "    \"{name}关于订单{order_id}咨询：{detail}，联系方式{email}。\",\n",
                                        "]\n",
                                        "\n",
                                        "def _random_email(rng: random.Random) -> str:\n",
                                        "    user = \"\".join(rng.choice(\"abcdefghijklmnopqrstuvwxyz\") for _ in range(6))\n",
                                        "    return f\"{user}@{rng.choice(EMAIL_DOMAINS)}\"\n",
                                        "\n",
                                        "def _random_order_id(rng: random.Random) -> str:\n",
                                        "    return f\"OD{rng.randint(20240101, 20241228)}-{rng.randint(1000, 9999)}\"\n",
                                        "\n",
                                        "def _generate_record(rng: random.Random) -> dict:\n",
                                        "    issue = rng.choice(ISSUE_LABELS)\n",
                                        "    name = rng.choice(NAME_POOL)\n",
                                        "    email = _random_email(rng)\n",
                                        "    order_id = _random_order_id(rng)\n",
                                        "    detail = rng.choice(ISSUE_PHRASES[issue])\n",
                                        "    text = rng.choice(TEXT_TEMPLATES).format(\n",
                                        "        order_id=order_id,\n",
                                        "        name=name,\n",
                                        "        email=email,\n",
                                        "        detail=detail,\n",
                                        "    )\n",
                                        "    return {\"text\": text, \"label\": issue}\n",
                                        "\n",
                                        "def _build_records(count: int, seed: int) -> list:\n",
                                        "    rng = random.Random(seed)\n",
                                        "    return [_generate_record(rng) for _ in range(count)]\n",
                                        "\n",
                                        "def format_prompt(text: str) -> str:\n",
                                        "    return f\"文本：{text}\\n类别：\"\n",
                                        "\n",
                                        "TRAINING_DATA = _build_records(1200, seed=42)\n",
                                        "\n",
                                        "class TextDataset(Dataset):\n",
                                        "    def __init__(self, samples, tokenizer, max_length=128):\n",
                                        "        self.samples = samples\n",
                                        "        self.tokenizer = tokenizer\n",
                                        "        self.max_length = max_length\n",
                                        "    \n",
                                        "    def __len__(self):\n",
                                        "        return len(self.samples)\n",
                                        "    \n",
                                        "    def __getitem__(self, idx):\n",
                                        "        sample = self.samples[idx]\n",
                                        "        prompt = format_prompt(sample['text'])\n",
                                        "        answer = sample['label']\n",
                                        "\n",
                                        "        prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
                                        "        answer_ids = self.tokenizer.encode(answer, add_special_tokens=False)\n",
                                        "\n",
                                        "        max_prompt_len = self.max_length - len(answer_ids)\n",
                                        "        if max_prompt_len < 1:\n",
                                        "            prompt_ids = prompt_ids[-max(1, self.max_length - 1):]\n",
                                        "            answer_ids = answer_ids[:1]\n",
                                        "        elif len(prompt_ids) > max_prompt_len:\n",
                                        "            prompt_ids = prompt_ids[-max_prompt_len:]\n",
                                        "\n",
                                        "        input_ids = prompt_ids + answer_ids\n",
                                        "        attention_mask = [1] * len(input_ids)\n",
                                        "        labels = [-100] * len(prompt_ids) + answer_ids\n",
                                        "\n",
                                        "        pad_len = self.max_length - len(input_ids)\n",
                                        "        if pad_len > 0:\n",
                                        "            input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
                                        "            attention_mask += [0] * pad_len\n",
                                        "            labels += [-100] * pad_len\n",
                                        "\n",
                                        "        return {\n",
                                        "            'input_ids': torch.tensor(input_ids),\n",
                                        "            'attention_mask': torch.tensor(attention_mask),\n",
                                        "            'labels': torch.tensor(labels),\n",
                                        "        }\n",
                                        "\n",
                                        "dataset = TextDataset(TRAINING_DATA, tokenizer)\n",
                                        "print(f\"Dataset size: {len(dataset)} examples\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "if PEFT_AVAILABLE:\n",
                                        "    # Training loop\n",
                                        "    print(\"\\n\" + \"=\"*60)\n",
                                        "    print(\"Starting LoRA Fine-tuning\")\n",
                                        "    print(\"=\"*60)\n",
                                        "    \n",
                                        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
                                        "    optimizer = torch.optim.AdamW(lora_model.parameters(), lr=1e-3)\n",
                                        "    \n",
                                        "    lora_model.train()\n",
                                        "    losses = []\n",
                                        "    \n",
                                        "    NUM_EPOCHS = 6\n",
                                        "    for epoch in range(NUM_EPOCHS):\n",
                                        "        epoch_loss = 0\n",
                                        "        for batch_idx, batch in enumerate(dataloader):\n",
                                        "            input_ids = batch['input_ids'].to(device)\n",
                                        "            attention_mask = batch['attention_mask'].to(device)\n",
                                        "            labels = batch['labels'].to(device)\n",
                                        "            \n",
                                        "            outputs = lora_model(\n",
                                        "                input_ids=input_ids,\n",
                                        "                attention_mask=attention_mask,\n",
                                        "                labels=labels\n",
                                        "            )\n",
                                        "            loss = outputs.loss\n",
                                        "            \n",
                                        "            optimizer.zero_grad()\n",
                                        "            loss.backward()\n",
                                        "            optimizer.step()\n",
                                        "            \n",
                                        "            epoch_loss += loss.item()\n",
                                        "            losses.append(loss.item())\n",
                                        "        \n",
                                        "        avg_loss = epoch_loss / len(dataloader)\n",
                                        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {avg_loss:.4f}\")\n",
                                        "    \n",
                                        "    print(\"\\nTraining completed!\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "if PEFT_AVAILABLE and losses:\n",
                                        "    # Visualize training\n",
                                        "    plt.figure(figsize=(10, 4))\n",
                                        "    plt.plot(losses, 'b-', alpha=0.7)\n",
                                        "    plt.xlabel('Step')\n",
                                        "    plt.ylabel('Loss')\n",
                                        "    plt.title('LoRA Fine-tuning Loss')\n",
                                        "    plt.grid(True, alpha=0.3)\n",
                                        "    plt.show()"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Test the fine-tuned model\n",
                                        "def _clean_text(text: str) -> str:\n",
                                        "    text = text.replace(\"##\", \"\")\n",
                                        "    return text.replace(\" \", \"\").strip()\n",
                                        "\n",
                                        "def _normalize_text(text: str) -> str:\n",
                                        "    table = str.maketrans({\n",
                                        "        \"發\": \"发\",\n",
                                        "        \"貨\": \"货\",\n",
                                        "        \"異\": \"异\",\n",
                                        "        \"後\": \"后\",\n",
                                        "        \"諮\": \"咨\",\n",
                                        "        \"詢\": \"询\",\n",
                                        "        \"請\": \"请\",\n",
                                        "    })\n",
                                        "    return text.translate(table)\n",
                                        "\n",
                                        "def _trim_at_stop(text: str) -> str:\n",
                                        "    for token in [\"\\n\", \"。\", \"！\", \"？\", \"，\", \"；\", \"、\", \"/\", \"／\", \" \"]:\n",
                                        "        idx = text.find(token)\n",
                                        "        if idx != -1:\n",
                                        "            return text[:idx]\n",
                                        "    return text\n",
                                        "\n",
                                        "def _extract_label(text: str, labels) -> str:\n",
                                        "    normalized = _normalize_text(text)\n",
                                        "    for label in labels:\n",
                                        "        if label in normalized:\n",
                                        "            return label\n",
                                        "    return \"未识别\"\n",
                                        "\n",
                                        "def generate_answer(model, tokenizer, text, max_new_tokens=8, temperature=0.7, do_sample=False, top_p=0.9):\n",
                                        "    model.eval()\n",
                                        "    prompt = format_prompt(text)\n",
                                        "    model_device = next(model.parameters()).device\n",
                                        "    inputs = tokenizer(prompt, return_tensors='pt').to(model_device)\n",
                                        "\n",
                                        "    if tokenizer.pad_token_id is None:\n",
                                        "        tokenizer.pad_token = tokenizer.eos_token\n",
                                        "    if model.config.pad_token_id is None:\n",
                                        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "    if model.generation_config.pad_token_id is None:\n",
                                        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "    gen_kwargs = {\n",
                                        "        \"max_new_tokens\": max_new_tokens,\n",
                                        "        \"do_sample\": do_sample,\n",
                                        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
                                        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
                                        "        \"repetition_penalty\": 1.05,\n",
                                        "        \"no_repeat_ngram_size\": 3,\n",
                                        "        \"min_new_tokens\": 2,\n",
                                        "    }\n",
                                        "    if do_sample:\n",
                                        "        gen_kwargs[\"temperature\"] = temperature\n",
                                        "        gen_kwargs[\"top_p\"] = top_p\n",
                                        "\n",
                                        "    input_len = inputs['input_ids'].shape[-1]\n",
                                        "    with torch.no_grad():\n",
                                        "        try:\n",
                                        "            outputs = model.generate(\n",
                                        "                **inputs,\n",
                                        "                **gen_kwargs,\n",
                                        "            )\n",
                                        "        except TypeError:\n",
                                        "            gen_kwargs.pop(\"min_new_tokens\", None)\n",
                                        "            outputs = model.generate(\n",
                                        "                **inputs,\n",
                                        "                **gen_kwargs,\n",
                                        "            )\n",
                                        "\n",
                                        "    gen_ids = outputs[0][input_len:]\n",
                                        "    response = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
                                        "    response = _trim_at_stop(_clean_text(response))\n",
                                        "    return response if response else \"（空）\"\n",
                                        "\n",
                                        "if PEFT_AVAILABLE:\n",
                                        "    print(\"\\n\" + \"=\"*60)\n",
                                        "    print(\"对比：LoRA 训练前 vs 训练后\")\n",
                                        "    print(\"=\"*60)\n",
                                        "    \n",
                                        "    test_samples = _build_records(5, seed=123)\n",
                                        "    test_texts = [s[\"text\"] for s in test_samples]\n",
                                        "    test_answers = [s[\"label\"] for s in test_samples]\n",
                                        "    \n",
                                        "    # Reload base model for a clean before/after comparison\n",
                                        "    print(\"Loading base model for comparison...\")\n",
                                        "    base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
                                        "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "    \n",
                                        "    base_answers = [generate_answer(base_model, tokenizer, t) for t in test_texts]\n",
                                        "    lora_answers = [generate_answer(lora_model, tokenizer, t) for t in test_texts]\n",
                                        "    \n",
                                        "    for text, gold, base_a, lora_a in zip(test_texts, test_answers, base_answers, lora_answers):\n",
                                        "        base_label = _extract_label(base_a, ISSUE_LABELS)\n",
                                        "        lora_label = _extract_label(lora_a, ISSUE_LABELS)\n",
                                        "        print(f\"\\nText: {text}\")\n",
                                        "        print(f\"Gold: {gold}\")\n",
                                        "        print(f\"Base: {base_a}\")\n",
                                        "        print(f\"Base label: {base_label} | Exact: {base_label == gold}\")\n",
                                        "        print(f\"LoRA: {lora_a}\")\n",
                                        "        print(f\"LoRA label: {lora_label} | Exact: {lora_label == gold}\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 5：QLoRA - 量化与 LoRA 的结合\n",
                                        "\n",
                                        "QLoRA = 4-bit 量化 + LoRA（只训练低秩适配器，基座权重保持量化冻结）。\n",
                                        "\n",
                                        "### bitsandbytes 在里面扮演什么角色？\n",
                                        "- 提供 8-bit/4-bit 权重量化与反量化的 CUDA kernel\n",
                                        "- 支持 NF4（更贴近权重分布）和 Double Quant（量化量化参数本身）\n",
                                        "- 常与 `device_map='auto'` 配合做分片加载，进一步省显存\n",
                                        "\n",
                                        "### 关键配置项（对应 BitsAndBytesConfig）\n",
                                        "- `load_in_4bit=True`：以 4-bit 权重加载模型\n",
                                        "- `bnb_4bit_quant_type='nf4'`：NF4 量化，适合正态分布权重\n",
                                        "- `bnb_4bit_use_double_quant=True`：双重量化，进一步压缩量化参数\n",
                                        "- `bnb_4bit_compute_dtype=torch.bfloat16/float16`：计算精度\n",
                                        "\n",
                                        "### 训练时发生了什么？\n",
                                        "1. 基座模型权重保持 4-bit 冻结\n",
                                        "2. LoRA 在指定层插入 A/B 低秩矩阵（通常只占 0.1%~2% 参数）\n",
                                        "3. 反向传播只更新 LoRA 参数（与少量 LayerNorm）\n",
                                        "4. 推理时 LoRA 权重与基座权重“合并”或“并行生效”\n",
                                        "\n",
                                        "### 适用与限制\n",
                                        "- 适合 7B+ 模型在消费级 GPU 上微调\n",
                                        "- Windows 下 bitsandbytes 兼容性可能较弱，必要时用 WSL2\n",
                                        "- 量化会带来一定精度损失，但 LoRA 可部分补偿\n",
                                        "\n",
                                        "下面给出**可运行的小型 QLoRA 演示**（如果 CUDA + bitsandbytes 可用）。\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Memory requirements comparison\n",
                                        "model_sizes = ['7B', '13B', '30B', '70B']\n",
                                        "params_b = [7, 13, 30, 70]\n",
                                        "\n",
                                        "# Memory estimates (GB)\n",
                                        "fp32_mem = [p * 4 for p in params_b]  # FP32: 4 bytes/param\n",
                                        "fp16_mem = [p * 2 for p in params_b]  # FP16: 2 bytes/param\n",
                                        "int8_mem = [p * 1 for p in params_b]  # INT8: 1 byte/param\n",
                                        "int4_mem = [p * 0.5 for p in params_b]  # INT4: 0.5 bytes/param\n",
                                        "qlora_mem = [p * 0.5 + 0.1 for p in params_b]  # INT4 + LoRA overhead\n",
                                        "\n",
                                        "fig, ax = plt.subplots(figsize=(12, 6))\n",
                                        "\n",
                                        "x = np.arange(len(model_sizes))\n",
                                        "width = 0.15\n",
                                        "\n",
                                        "ax.bar(x - 2*width, fp32_mem, width, label='FP32', color='darkred')\n",
                                        "ax.bar(x - width, fp16_mem, width, label='FP16', color='coral')\n",
                                        "ax.bar(x, int8_mem, width, label='INT8', color='gold')\n",
                                        "ax.bar(x + width, int4_mem, width, label='INT4', color='yellowgreen')\n",
                                        "ax.bar(x + 2*width, qlora_mem, width, label='QLoRA', color='green')\n",
                                        "\n",
                                        "# Add GPU memory lines\n",
                                        "ax.axhline(y=24, color='blue', linestyle='--', alpha=0.7, label='RTX 3090 (24GB)')\n",
                                        "ax.axhline(y=16, color='purple', linestyle='--', alpha=0.7, label='RTX 4080 (16GB)')\n",
                                        "ax.axhline(y=8, color='orange', linestyle='--', alpha=0.7, label='RTX 3070 (8GB)')\n",
                                        "\n",
                                        "ax.set_xlabel('Model Size')\n",
                                        "ax.set_ylabel('GPU Memory (GB)')\n",
                                        "ax.set_title('Memory Requirements by Precision')\n",
                                        "ax.set_xticks(x)\n",
                                        "ax.set_xticklabels(model_sizes)\n",
                                        "ax.legend(loc='upper left')\n",
                                        "ax.set_yscale('log')\n",
                                        "ax.grid(True, alpha=0.3, axis='y')\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"\\nQLoRA enables fine-tuning on consumer GPUs:\")\n",
                                        "print(f\"  7B model with QLoRA: ~{qlora_mem[0]:.1f} GB (fits on 8GB GPU!)\")\n",
                                        "print(f\"  13B model with QLoRA: ~{qlora_mem[1]:.1f} GB (fits on 16GB GPU!)\")\n",
                                        "print(f\"  30B model with QLoRA: ~{qlora_mem[2]:.1f} GB (fits on 24GB GPU!)\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# QLoRA configuration example (requires bitsandbytes)\n",
                                        "qlora_code = '''\n",
                                        "# QLoRA Configuration Example\n",
                                        "# This requires: pip install bitsandbytes\n",
                                        "\n",
                                        "from transformers import BitsAndBytesConfig\n",
                                        "from peft import prepare_model_for_kbit_training\n",
                                        "\n",
                                        "# 4-bit quantization config\n",
                                        "bnb_config = BitsAndBytesConfig(\n",
                                        "    load_in_4bit=True,\n",
                                        "    bnb_4bit_use_double_quant=True,  # Nested quantization\n",
                                        "    bnb_4bit_quant_type=\"nf4\",       # NormalFloat4\n",
                                        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                                        ")\n",
                                        "\n",
                                        "# Load model with quantization\n",
                                        "model = AutoModelForCausalLM.from_pretrained(\n",
                                        "    \"meta-llama/Llama-2-7b-hf\",\n",
                                        "    quantization_config=bnb_config,\n",
                                        "    device_map=\"auto\"\n",
                                        ")\n",
                                        "\n",
                                        "model = prepare_model_for_kbit_training(model)\n",
                                        "\n",
                                        "# Apply LoRA on top of quantized model\n",
                                        "lora_config = LoraConfig(\n",
                                        "    r=64,\n",
                                        "    lora_alpha=16,\n",
                                        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                                        "    lora_dropout=0.05,\n",
                                        "    bias=\"none\",\n",
                                        "    task_type=\"CAUSAL_LM\"\n",
                                        ")\n",
                                        "\n",
                                        "model = get_peft_model(model, lora_config)\n",
                                        "'''\n",
                                        "\n",
                                        "print(\"QLoRA Example Code:\")\n",
                                        "print(\"=\" * 60)\n",
                                        "print(qlora_code)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "### 可选：QLoRA 迷你实操（CUDA + bitsandbytes）\n",
                                        "\n",
                                        "下面是一个**小模型**（OPT-125M）的 QLoRA 迷你训练示例，跑 20~40 step 方便观察。\n",
                                        "如果没有安装 bitsandbytes，先执行 `pip install bitsandbytes`。\n",
                                        "Windows 若遇到安装失败，可考虑 WSL2/Linux。\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Optional: runnable QLoRA mini demo (requires CUDA + bitsandbytes)\n",
                                        "if not PEFT_AVAILABLE:\n",
                                        "    print(\"PEFT not available. Install with: pip install peft\")\n",
                                        "elif not BNB_AVAILABLE:\n",
                                        "    print(\"bitsandbytes not available. Install with: pip install bitsandbytes\")\n",
                                        "elif not torch.cuda.is_available():\n",
                                        "    print(\"CUDA not available. QLoRA demo skipped.\")\n",
                                        "else:\n",
                                        "    from transformers import BitsAndBytesConfig\n",
                                        "\n",
                                        "    qlora_model_name = \"uer/gpt2-chinese-cluecorpussmall\"\n",
                                        "    compute_dtype = torch.bfloat16 if hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported() else torch.float16\n",
                                        "\n",
                                        "    bnb_config = BitsAndBytesConfig(\n",
                                        "        load_in_4bit=True,\n",
                                        "        bnb_4bit_use_double_quant=True,\n",
                                        "        bnb_4bit_quant_type=\"nf4\",\n",
                                        "        bnb_4bit_compute_dtype=compute_dtype,\n",
                                        "    )\n",
                                        "\n",
                                        "    qlora_tokenizer = AutoTokenizer.from_pretrained(qlora_model_name)\n",
                                        "    if qlora_tokenizer.pad_token is None:\n",
                                        "        qlora_tokenizer.pad_token = qlora_tokenizer.eos_token or qlora_tokenizer.unk_token\n",
                                        "\n",
                                        "    qlora_base = AutoModelForCausalLM.from_pretrained(\n",
                                        "        qlora_model_name,\n",
                                        "        quantization_config=bnb_config,\n",
                                        "        device_map=\"auto\"\n",
                                        "    )\n",
                                        "    qlora_base = prepare_model_for_kbit_training(qlora_base)\n",
                                        "    qlora_base.config.use_cache = False\n",
                                        "\n",
                                        "    qlora_config = LoraConfig(\n",
                                        "        r=16,\n",
                                        "        lora_alpha=32,\n",
                                        "        lora_dropout=0.05,\n",
                                        "        target_modules=[\"c_attn\", \"c_proj\"],\n",
                                        "        bias=\"none\",\n",
                                        "        task_type=TaskType.CAUSAL_LM,\n",
                                        "    )\n",
                                        "\n",
                                        "    qlora_model = get_peft_model(qlora_base, qlora_config)\n",
                                        "    qlora_model.print_trainable_parameters()\n",
                                        "\n",
                                        "    qlora_train_samples = _build_records(800, seed=7)\n",
                                        "    qlora_dataset = TextDataset(qlora_train_samples, qlora_tokenizer, max_length=128)\n",
                                        "    qlora_loader = DataLoader(qlora_dataset, batch_size=8, shuffle=True)\n",
                                        "\n",
                                        "    optimizer = torch.optim.AdamW(qlora_model.parameters(), lr=2e-4)\n",
                                        "    qlora_model.train()\n",
                                        "\n",
                                        "    max_steps = 80\n",
                                        "    step = 0\n",
                                        "    for epoch in range(2):\n",
                                        "        for batch in qlora_loader:\n",
                                        "            device = next(qlora_model.parameters()).device\n",
                                        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
                                        "\n",
                                        "            outputs = qlora_model(**batch)\n",
                                        "            loss = outputs.loss\n",
                                        "            loss.backward()\n",
                                        "            optimizer.step()\n",
                                        "            optimizer.zero_grad()\n",
                                        "\n",
                                        "            if step % 10 == 0:\n",
                                        "                print(f\"step {step} | loss {loss.item():.4f}\")\n",
                                        "            step += 1\n",
                                        "            if step >= max_steps:\n",
                                        "                break\n",
                                        "        if step >= max_steps:\n",
                                        "            break\n",
                                        "\n",
                                        "    qlora_model.eval()\n",
                                        "    demo_samples = _build_records(3, seed=123)\n",
                                        "    for sample in demo_samples:\n",
                                        "        pred_raw = generate_answer(qlora_model, qlora_tokenizer, sample[\"text\"])\n",
                                        "        pred_label = _extract_label(pred_raw, ISSUE_LABELS)\n",
                                        "        print(f\"Text: {sample['text']}\")\n",
                                        "        print(f\"Gold: {sample['label']}\")\n",
                                        "        print(f\"Pred raw: {pred_raw}\")\n",
                                        "        print(f\"Pred: {pred_label}\")\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 总结\n",
                                        "\n",
                                        "1. **LoRA 原理**：低秩分解使可训练参数减少 100x+\n",
                                        "   - 只训练小矩阵 A 和 B\n",
                                        "   - 原始权重保持冻结\n",
                                        "\n",
                                        "2. **量化**：降低精度以节省显存\n",
                                        "   - FP32 → FP16 → INT8 → INT4\n",
                                        "   - 权衡：显存节省 vs. 精度\n",
                                        "\n",
                                        "3. **QLoRA**：两者结合\n",
                                        "   - 4-bit 量化的基座模型\n",
                                        "   - LoRA 作为可训练参数\n",
                                        "   - 消费级 GPU 上也能微调 7B+ 模型\n",
                                        "\n",
                                        "### 关键公式\n",
                                        "\n",
                                        "| 概念 | 公式 |\n",
                                        "|---------|--------|\n",
                                        "| LoRA | $W_{new} = W_{frozen} + BA$ |\n",
                                        "| Scaling | $\\alpha / r$ |\n",
                                        "| 压缩比 | $(d_{in} \\times d_{out}) / (d_{in} \\times r + r \\times d_{out})$ |\n",
                                        "\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 练习\n",
                                        "\n",
                                        "1. **秩的对比**：用不同 rank（4、8、16、32）训练并比较效果\n",
                                        "2. **目标模块**：尝试把 LoRA 应用于不同层\n",
                                        "3. **权重合并**：实现权重合并并验证推理是否正常\n",
                                        "4. **4-bit 量化**：如果有 bitsandbytes，尝试在更大模型上做 QLoRA\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Exercise space\n",
                                        "\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 4
}
