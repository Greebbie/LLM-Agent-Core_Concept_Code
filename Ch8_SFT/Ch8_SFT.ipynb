{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "# Ch8 | SFT（Supervised Fine-Tuning）：让模型学会遵循指令\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "**目标：** 将基础模型转变为能够遵循指令的对话模型\n",
                                        "\n",
                                        "**本 notebook 使用 中文 GPT-2 和 HuggingFace Transformers 演示真实的 SFT 训练。**\n",
                                        "\n",
                                        "**设备建议：** GPU；CPU 仅建议阅读/演示。\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## 内容\n",
                                        "\n",
                                        "1. **SFT 原理**：从文本续写到对话\n",
                                        "2. **ChatML 格式**：标准对话格式\n",
                                        "3. **Loss Masking**：仅在 assistant 回复上计算损失\n",
                                        "4. **真实训练**：对 中文 GPT-2 进行指令微调\n",
                                        "\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 预备知识\n",
                                        "\n",
                                        "### 什么是 SFT？\n",
                                        "\n",
                                        "**SFT（Supervised Fine-Tuning）** = 教模型“如何进行对话”\n",
                                        "\n",
                                        "| | 基础模型 | 对话模型（SFT 后） |\n",
                                        "|:---|:---|:---|\n",
                                        "| 训练目标 | 预测下一个 token | 学习对话格式 |\n",
                                        "| 输入 | 任意文本 | 结构化对话 |\n",
                                        "| 输出 | 文本续写 | 有帮助的回复 |\n",
                                        "| 使用场景 | 文本补全 | 聊天、问答、指令 |\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 环境准备\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "from torch.utils.data import Dataset, DataLoader\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "from typing import Dict, List, Optional, Any\n",
                                        "import warnings\n",
                                        "warnings.filterwarnings('ignore')\n",
                                        "\n",
                                        "# Check for GPU\n",
                                        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                                        "print(f\"Using device: {device}\")\n",
                                        "\n",
                                        "# Check available memory\n",
                                        "if device == 'cuda':\n",
                                        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                                        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Install required packages if needed\n",
                                        "try:\n",
                                        "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
                                        "    from datasets import Dataset as HFDataset\n",
                                        "    print(\"HuggingFace Transformers loaded successfully!\")\n",
                                        "except ImportError:\n",
                                        "    print(\"Installing required packages...\")\n",
                                        "    !pip install transformers datasets accelerate -q\n",
                                        "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
                                        "    from datasets import Dataset as HFDataset\n",
                                        "\n",
                                        "# Try to import TRL for SFTTrainer (optional but recommended)\n",
                                        "try:\n",
                                        "    from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
                                        "    USE_TRL = True\n",
                                        "    print(\"TRL library loaded - will use SFTTrainer\")\n",
                                        "except ImportError:\n",
                                        "    USE_TRL = False\n",
                                        "    print(\"TRL not available - will use standard Trainer\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 1：理解基础模型与对话模型\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Load GPT-2 Chinese (base model)\n",
                                        "model_name = \"uer/gpt2-chinese-cluecorpussmall\"  # Chinese GPT-2 base\n",
                                        "\n",
                                        "print(f\"Loading {model_name}...\")\n",
                                        "try:\n",
                                        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
                                        "except Exception:\n",
                                        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
                                        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
                                        "\n",
                                        "# GPT-2 doesn't have a pad token by default\n",
                                        "if tokenizer.pad_token_id is None:\n",
                                        "    tokenizer.pad_token = tokenizer.eos_token\n",
                                        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "print(f\"Model loaded: {sum(p.numel() for p in base_model.parameters()):,} parameters\")\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7, do_sample=True, top_p=0.9):\n",
                                        "    \"\"\"\n",
                                        "    Generate text from a prompt.\n",
                                        "    \"\"\"\n",
                                        "    model.eval()\n",
                                        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                                        "\n",
                                        "    if tokenizer.pad_token_id is None:\n",
                                        "        tokenizer.pad_token = tokenizer.eos_token\n",
                                        "    if model.config.pad_token_id is None:\n",
                                        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "    if model.generation_config.pad_token_id is None:\n",
                                        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "    gen_kwargs = {\n",
                                        "        \"max_new_tokens\": max_new_tokens,\n",
                                        "        \"do_sample\": do_sample,\n",
                                        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
                                        "        \"repetition_penalty\": 1.1,\n",
                                        "        \"no_repeat_ngram_size\": 3,\n",
                                        "    }\n",
                                        "    if do_sample:\n",
                                        "        gen_kwargs[\"temperature\"] = temperature\n",
                                        "        gen_kwargs[\"top_p\"] = top_p\n",
                                        "\n",
                                        "    input_len = inputs['input_ids'].shape[-1]\n",
                                        "    with torch.no_grad():\n",
                                        "        outputs = model.generate(\n",
                                        "            **inputs,\n",
                                        "            **gen_kwargs,\n",
                                        "        )\n",
                                        "\n",
                                        "    gen_ids = outputs[0][input_len:]\n",
                                        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
                                        "\n",
                                        "# Test base model behavior\n",
                                        "print(\"=\" * 60)\n",
                                        "print(\"BASE MODEL BEHAVIOR (before SFT)\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "test_prompts = [\n",
                                        "    \"What is machine learning?\",\n",
                                        "    \"Question: What is 2+2?\\nAnswer:\",\n",
                                        "    \"User: Hello!\\nAssistant:\",\n",
                                        "]\n",
                                        "\n",
                                        "for prompt in test_prompts:\n",
                                        "    print(f\"\\nPrompt: {prompt}\")\n",
                                        "    print(f\"Output: {generate_text(base_model, tokenizer, prompt, max_new_tokens=30)}\")\n",
                                        "    print(\"-\" * 40)\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "如你所见，基础模型并不理解指令格式——它只会沿着输入的风格继续生成文本。\n",
                                        "\n",
                                        "## Part 2：ChatML 格式\n",
                                        "\n",
                                        "ChatML 是一种用于表示对话的标准格式：\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Define special tokens for chat format\n",
                                        "SPECIAL_TOKENS = {\n",
                                        "    \"bos_token\": \"<|startoftext|>\",\n",
                                        "    \"eos_token\": \"<|endoftext|>\",\n",
                                        "    \"pad_token\": \"<|pad|>\",\n",
                                        "    \"additional_special_tokens\": [\n",
                                        "        \"<|im_start|>\",\n",
                                        "        \"<|im_end|>\",\n",
                                        "        \"<|user|>\",\n",
                                        "        \"<|assistant|>\",\n",
                                        "        \"<|system|>\",\n",
                                        "    ]\n",
                                        "}\n",
                                        "\n",
                                        "def format_conversation(messages: List[Dict[str, str]], tokenizer=None) -> str:\n",
                                        "    \"\"\"\n",
                                        "    Convert a conversation to ChatML format.\n",
                                        "    \n",
                                        "    Args:\n",
                                        "        messages: List of {\"role\": str, \"content\": str} dicts\n",
                                        "        tokenizer: Optional tokenizer for encoding\n",
                                        "    \n",
                                        "    Returns:\n",
                                        "        Formatted conversation string\n",
                                        "    \"\"\"\n",
                                        "    formatted = \"\"\n",
                                        "    for msg in messages:\n",
                                        "        role = msg[\"role\"]\n",
                                        "        content = msg[\"content\"]\n",
                                        "        formatted += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
                                        "    return formatted\n",
                                        "\n",
                                        "# Example conversation\n",
                                        "example_conv = [\n",
                                        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
                                        "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
                                        "    {\"role\": \"assistant\", \"content\": \"Python is a high-level programming language known for its simplicity and readability.\"},\n",
                                        "]\n",
                                        "\n",
                                        "print(\"ChatML Format Example:\")\n",
                                        "print(\"=\" * 60)\n",
                                        "print(format_conversation(example_conv))"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 3：SFT 训练数据\n",
                                        "\n",
                                        "默认使用“诉求分类”数据集（gpt2_sft_*.jsonl），也可以切换到 JSON 抽取任务（custom_sft_*.jsonl）。\n",
                                        "JSON 抽取时字段：name / email / order_id（只输出 JSON）。\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Structured SFT training data from data/ (train/val/test splits)\n",
                                        "import json\n",
                                        "from pathlib import Path\n",
                                        "import os\n",
                                        "\n",
                                        "def resolve_data_dir():\n",
                                        "    candidates = [Path.cwd(), Path.cwd().parent]\n",
                                        "    for base in candidates:\n",
                                        "        data_dir = base / \"data\"\n",
                                        "        if data_dir.exists():\n",
                                        "            return str(data_dir)\n",
                                        "    return os.path.join(os.getcwd(), \"data\")\n",
                                        "\n",
                                        "DATA_DIR = resolve_data_dir()\n",
                                        "\n",
                                        "\n",
                                        "def load_jsonl(path):\n",
                                        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
                                        "        return [json.loads(line) for line in f if line.strip()]\n",
                                        "\n",
                                        "# Task selection: classification or JSON extraction\n",
                                        "SFT_TASK = \"classification\"  # \"classification\" or \"json_extraction\"\n",
                                        "USE_PLAIN_EXTRACT = False  # Only for json_extraction\n",
                                        "\n",
                                        "if SFT_TASK == \"classification\":\n",
                                        "    # Optional: regenerate a simpler, Chinese-only classification dataset\n",
                                        "    REGEN_SFT_DATA = True\n",
                                        "    ISSUE_LABELS = [\"延迟发货\", \"退款申请\", \"地址修改\", \"物流异常\", \"售后咨询\", \"发票问题\"]\n",
                                        "    TASK_LABELS = ISSUE_LABELS\n",
                                        "    TRAIN_FILE = \"gpt2_sft_train.jsonl\"\n",
                                        "    VAL_FILE = \"gpt2_sft_val.jsonl\"\n",
                                        "    TEST_FILE = \"gpt2_sft_test.jsonl\"\n",
                                        "else:\n",
                                        "    REGEN_SFT_DATA = False\n",
                                        "    ISSUE_LABELS = []\n",
                                        "    TASK_LABELS = None\n",
                                        "    TRAIN_FILE = \"custom_sft_train.jsonl\"\n",
                                        "    VAL_FILE = \"custom_sft_val.jsonl\"\n",
                                        "    TEST_FILE = \"custom_sft_test.jsonl\"\n",
                                        "\n",
                                        "if SFT_TASK != \"json_extraction\":\n",
                                        "    USE_PLAIN_EXTRACT = False\n",
                                        "\n",
                                        "\n",
                                        "def _rand_order_id(rng):\n",
                                        "    year = rng.choice([2024, 2025])\n",
                                        "    month = rng.randint(1, 12)\n",
                                        "    day = rng.randint(1, 28)\n",
                                        "    return f\"OD{year:04d}{month:02d}{day:02d}-{rng.randint(1000, 9999)}\"\n",
                                        "\n",
                                        "\n",
                                        "def _generate_records(count, seed):\n",
                                        "    import random\n",
                                        "    rng = random.Random(seed)\n",
                                        "    names = [\n",
                                        "        (\"陈洁\", \"chen.jie\"), (\"孙悦\", \"sun.yue\"), (\"张强\", \"zhang.qiang\"), (\"周凯\", \"zhou.kai\"),\n",
                                        "        (\"刘洋\", \"liu.yang\"), (\"王伟\", \"wang.wei\"), (\"吴磊\", \"wu.lei\"), (\"李娜\", \"li.na\"),\n",
                                        "        (\"赵敏\", \"zhao.min\"), (\"黄涛\", \"huang.tao\"), (\"郭婷\", \"guo.ting\"), (\"马超\", \"ma.chao\"),\n",
                                        "    ]\n",
                                        "    templates = [\n",
                                        "        \"客户{name}（{email}）反馈订单{order_id}{issue}。\",\n",
                                        "        \"订单{order_id}相关用户{name}邮箱{email}，问题：{issue}。\",\n",
                                        "        \"请处理：姓名{name}，邮箱{email}，订单号{order_id}，原因：{issue}。\",\n",
                                        "        \"用户{name}邮箱{email}，订单{order_id}需要处理：{issue}。\",\n",
                                        "        \"{name}反馈订单{order_id}{issue}，联系邮箱{email}。\",\n",
                                        "    ]\n",
                                        "\n",
                                        "    records = []\n",
                                        "    for _ in range(count):\n",
                                        "        name, email_user = rng.choice(names)\n",
                                        "        email = f\"{email_user}@example.com\"\n",
                                        "        order_id = _rand_order_id(rng)\n",
                                        "        issue = rng.choice(ISSUE_LABELS)\n",
                                        "        text = rng.choice(templates).format(\n",
                                        "            name=name,\n",
                                        "            email=email,\n",
                                        "            order_id=order_id,\n",
                                        "            issue=issue,\n",
                                        "        )\n",
                                        "        instruction = (\n",
                                        "            \"请判断用户诉求类别，只输出类别（\" + \"/\".join(ISSUE_LABELS) + \"）。\"\n",
                                        "            f\"文本：{text}\"\n",
                                        "        )\n",
                                        "        response = issue\n",
                                        "        records.append({\n",
                                        "            \"instruction\": instruction,\n",
                                        "            \"response\": response,\n",
                                        "            \"category\": \"诉求分类\",\n",
                                        "            \"metric\": \"text\",\n",
                                        "            \"expected\": response,\n",
                                        "        })\n",
                                        "    return records\n",
                                        "\n",
                                        "\n",
                                        "def _write_jsonl(path, records):\n",
                                        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
                                        "        for record in records:\n",
                                        "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
                                        "\n",
                                        "\n",
                                        "if REGEN_SFT_DATA and SFT_TASK == \"classification\":\n",
                                        "    train_records = _generate_records(8000, seed=42)\n",
                                        "    val_records = _generate_records(1000, seed=43)\n",
                                        "    test_records = _generate_records(1000, seed=44)\n",
                                        "    _write_jsonl(os.path.join(DATA_DIR, \"gpt2_sft_train.jsonl\"), train_records)\n",
                                        "    _write_jsonl(os.path.join(DATA_DIR, \"gpt2_sft_val.jsonl\"), val_records)\n",
                                        "    _write_jsonl(os.path.join(DATA_DIR, \"gpt2_sft_test.jsonl\"), test_records)\n",
                                        "    print(\"Regenerated gpt2_sft_*.jsonl with classification labels\")\n",
                                        "\n",
                                        "TRAIN_RECORDS = load_jsonl(os.path.join(DATA_DIR, TRAIN_FILE))\n",
                                        "VAL_RECORDS = load_jsonl(os.path.join(DATA_DIR, VAL_FILE))\n",
                                        "TEST_RECORDS = load_jsonl(os.path.join(DATA_DIR, TEST_FILE))\n",
                                        "\n",
                                        "\n",
                                        "def build_plain_instruction(record):\n",
                                        "    instruction = record[\"instruction\"]\n",
                                        "    text = instruction\n",
                                        "    if \"文本：\" in instruction:\n",
                                        "        text = instruction.split(\"文本：\", 1)[-1]\n",
                                        "    return (\n",
                                        "        \"从文本中抽取姓名、邮箱、订单号，按“姓名：…；邮箱：…；订单号：…”格式回答。\"\n",
                                        "        f\"文本：{text}\"\n",
                                        "    )\n",
                                        "\n",
                                        "\n",
                                        "def build_plain_response(record):\n",
                                        "    try:\n",
                                        "        obj = json.loads(record[\"response\"])\n",
                                        "    except Exception:\n",
                                        "        return record[\"response\"]\n",
                                        "    return f\"姓名：{obj.get('name','')}；邮箱：{obj.get('email','')}；订单号：{obj.get('order_id','')}\"\n",
                                        "\n",
                                        "\n",
                                        "def to_messages(record):\n",
                                        "    if USE_PLAIN_EXTRACT:\n",
                                        "        user_text = build_plain_instruction(record)\n",
                                        "        assistant_text = build_plain_response(record)\n",
                                        "    else:\n",
                                        "        user_text = record[\"instruction\"]\n",
                                        "        assistant_text = record[\"response\"]\n",
                                        "    return {\n",
                                        "        \"messages\": [\n",
                                        "            {\"role\": \"user\", \"content\": user_text},\n",
                                        "            {\"role\": \"assistant\", \"content\": assistant_text},\n",
                                        "        ]\n",
                                        "    }\n",
                                        "\n",
                                        "\n",
                                        "TRAIN_DATA = [to_messages(r) for r in TRAIN_RECORDS]\n",
                                        "VAL_DATA = [to_messages(r) for r in VAL_RECORDS]\n",
                                        "TEST_DATA = [to_messages(r) for r in TEST_RECORDS]\n",
                                        "\n",
                                        "print(f\"SFT task: {SFT_TASK}\")\n",
                                        "print(f\"Train: {len(TRAIN_DATA)} | Val: {len(VAL_DATA)} | Test: {len(TEST_DATA)}\")\n",
                                        "print(f\"Categories: {sorted(set(r['category'] for r in TRAIN_RECORDS))}\")\n",
                                        "print(f\"Example: {TRAIN_DATA[0]['messages']}\")\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 4：Loss Masking\n",
                                        "\n",
                                        "SFT 的关键点：**只在 assistant 回复上计算 loss**，而不是在 user 输入上计算。\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class SFTDataset(torch.utils.data.Dataset):\n",
                                        "    \"\"\"\n",
                                        "    Dataset for Supervised Fine-Tuning with proper loss masking.\n",
                                        "    \n",
                                        "    Key features:\n",
                                        "    - Formats conversations in ChatML style\n",
                                        "    - Creates labels with -100 for non-assistant tokens (loss masking)\n",
                                        "    - Handles variable length sequences\n",
                                        "    \"\"\"\n",
                                        "    \n",
                                        "    def __init__(\n",
                                        "        self,\n",
                                        "        data: List[Dict],\n",
                                        "        tokenizer,\n",
                                        "        max_length: int = 512,\n",
                                        "        mask_prompt: bool = True\n",
                                        "    ):\n",
                                        "        self.data = data\n",
                                        "        self.tokenizer = tokenizer\n",
                                        "        self.max_length = max_length\n",
                                        "        self.mask_prompt = mask_prompt\n",
                                        "        \n",
                                        "        # Pre-process all examples\n",
                                        "        self.processed = []\n",
                                        "        for item in data:\n",
                                        "            processed = self._process_example(item)\n",
                                        "            if processed is not None:\n",
                                        "                self.processed.append(processed)\n",
                                        "        \n",
                                        "        print(f\"Processed {len(self.processed)}/{len(data)} examples\")\n",
                                        "    \n",
                                        "    def _process_example(self, item: Dict) -> Optional[Dict]:\n",
                                        "        \"\"\"\n",
                                        "        Process a single conversation example.\n",
                                        "        \n",
                                        "        Returns:\n",
                                        "            Dict with input_ids, attention_mask, and labels\n",
                                        "        \"\"\"\n",
                                        "        messages = item['messages']\n",
                                        "        \n",
                                        "        # Build the full conversation text and track assistant positions\n",
                                        "        full_text = \"\"\n",
                                        "        assistant_ranges = []  # List of (start, end) character positions for assistant responses\n",
                                        "        \n",
                                        "        for msg in messages:\n",
                                        "            role = msg['role']\n",
                                        "            content = msg['content']\n",
                                        "            \n",
                                        "            msg_text = f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
                                        "            \n",
                                        "            if role == 'assistant':\n",
                                        "                # Track where assistant content starts and ends\n",
                                        "                content_start = len(full_text) + len(f\"<|im_start|>{role}\\n\")\n",
                                        "                content_end = content_start + len(content)\n",
                                        "                assistant_ranges.append((content_start, content_end))\n",
                                        "            \n",
                                        "            full_text += msg_text\n",
                                        "        \n",
                                        "        # Tokenize\n",
                                        "        encoding = self.tokenizer(\n",
                                        "            full_text,\n",
                                        "            truncation=True,\n",
                                        "            max_length=self.max_length,\n",
                                        "            padding='max_length',\n",
                                        "            return_tensors='pt',\n",
                                        "            return_offsets_mapping=getattr(self.tokenizer, \"is_fast\", False)\n",
                                        "        )\n",
                                        "        \n",
                                        "        input_ids = encoding['input_ids'].squeeze(0)\n",
                                        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
                                        "        \n",
                                        "        # Create labels: -100 for tokens we don't want to compute loss on\n",
                                        "        if self.mask_prompt:\n",
                                        "            labels = torch.full_like(input_ids, -100)\n",
                                        "            offsets = encoding.get('offset_mapping')\n",
                                        "            if offsets is not None:\n",
                                        "                offsets = offsets.squeeze(0).tolist()\n",
                                        "                for i, (start_char, end_char) in enumerate(offsets):\n",
                                        "                    if start_char == 0 and end_char == 0:\n",
                                        "                        continue\n",
                                        "                    for a_start, a_end in assistant_ranges:\n",
                                        "                        if start_char < a_end and end_char > a_start:\n",
                                        "                            labels[i] = input_ids[i]\n",
                                        "                            break\n",
                                        "            else:\n",
                                        "                # Fallback approximate mapping for non-fast tokenizers\n",
                                        "                for start_char, end_char in assistant_ranges:\n",
                                        "                    prefix = full_text[:start_char]\n",
                                        "                    prefix_tokens = self.tokenizer(prefix, add_special_tokens=False)['input_ids']\n",
                                        "                    start_token = len(prefix_tokens)\n",
                                        "\n",
                                        "                    content_text = full_text[:end_char]\n",
                                        "                    content_tokens = self.tokenizer(content_text, add_special_tokens=False)['input_ids']\n",
                                        "                    end_token = len(content_tokens)\n",
                                        "\n",
                                        "                    if start_token < self.max_length and end_token <= self.max_length:\n",
                                        "                        labels[start_token:end_token] = input_ids[start_token:end_token]\n",
                                        "        else:\n",
                                        "            labels = input_ids.clone()\n",
                                        "        \n",
                                        "        return {\n",
                                        "            'input_ids': input_ids,\n",
                                        "            'attention_mask': attention_mask,\n",
                                        "            'labels': labels\n",
                                        "        }\n",
                                        "    \n",
                                        "    def __len__(self):\n",
                                        "        return len(self.processed)\n",
                                        "    \n",
                                        "    def __getitem__(self, idx):\n",
                                        "        return self.processed[idx]"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Visualize loss masking\n",
                                        "def visualize_loss_masking(example: Dict, tokenizer):\n",
                                        "    \"\"\"\n",
                                        "    Visualize which tokens have loss computed vs masked.\n",
                                        "    \"\"\"\n",
                                        "    input_ids = example['input_ids']\n",
                                        "    labels = example['labels']\n",
                                        "    \n",
                                        "    # Only show non-padding tokens\n",
                                        "    valid_len = (input_ids != tokenizer.pad_token_id).sum().item()\n",
                                        "    input_ids = input_ids[:valid_len]\n",
                                        "    labels = labels[:valid_len]\n",
                                        "    \n",
                                        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
                                        "    \n",
                                        "    print(\"Loss Masking Visualization:\")\n",
                                        "    print(\"=\" * 60)\n",
                                        "    print(\"Green = Loss computed (assistant)\")\n",
                                        "    print(\"Red = Loss masked (user/system)\")\n",
                                        "    print(\"=\" * 60)\n",
                                        "    \n",
                                        "    # Print tokens with colors\n",
                                        "    for i, (token, label) in enumerate(zip(tokens[:50], labels[:50])):  # First 50 tokens\n",
                                        "        if label == -100:\n",
                                        "            print(f\"\\033[91m{token}\\033[0m\", end=\" \")  # Red\n",
                                        "        else:\n",
                                        "            print(f\"\\033[92m{token}\\033[0m\", end=\" \")  # Green\n",
                                        "    print(\"\\n\")\n",
                                        "    \n",
                                        "    # Statistics\n",
                                        "    masked = (labels == -100).sum().item()\n",
                                        "    total = len(labels)\n",
                                        "    print(f\"\\nMasked tokens: {masked}/{total} ({100*masked/total:.1f}%)\")\n",
                                        "    print(f\"Loss computed on: {total-masked}/{total} ({100*(total-masked)/total:.1f}%)\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 5：使用 GPT-2 进行真实 SFT 训练\n",
                                        "\n",
                                        "现在开始实际对 GPT-2 做微调！\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Add special tokens to tokenizer\n",
                                        "print(\"Adding special tokens...\")\n",
                                        "\n",
                                        "from pathlib import Path\n",
                                        "\n",
                                        "if \"DATA_DIR\" in globals():\n",
                                        "    base_dir = Path(DATA_DIR).parent\n",
                                        "else:\n",
                                        "    base_dir = Path.cwd()\n",
                                        "    if not (base_dir / \"models\").exists() and (base_dir.parent / \"models\").exists():\n",
                                        "        base_dir = base_dir.parent\n",
                                        "\n",
                                        "SFT_SAVE_DIR = base_dir / \"models\" / \"ch8_sft_gpt2\"\n",
                                        "LOAD_SFT_MODEL = False  # Set True to load a saved SFT model\n",
                                        "\n",
                                        "if LOAD_SFT_MODEL and SFT_SAVE_DIR.exists():\n",
                                        "    sft_tokenizer = AutoTokenizer.from_pretrained(SFT_SAVE_DIR)\n",
                                        "    sft_model = AutoModelForCausalLM.from_pretrained(SFT_SAVE_DIR).to(device)\n",
                                        "    print(f\"Loaded SFT model from {SFT_SAVE_DIR}\")\n",
                                        "else:\n",
                                        "    # Create a fresh tokenizer and model for SFT\n",
                                        "    try:\n",
                                        "        sft_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
                                        "    except Exception:\n",
                                        "        sft_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
                                        "\n",
                                        "    # Add special tokens\n",
                                        "    special_tokens = {\n",
                                        "        'pad_token': '<|pad|>',\n",
                                        "        'additional_special_tokens': ['<|im_start|>', '<|im_end|>']\n",
                                        "    }\n",
                                        "    num_added = sft_tokenizer.add_special_tokens(special_tokens)\n",
                                        "\n",
                                        "    # Add ASCII tokens to avoid [UNK] for IDs/emails (WordPiece uses ## for subwords)\n",
                                        "    import string\n",
                                        "    ascii_chars = list(string.ascii_lowercase + string.ascii_uppercase + string.digits + \"@._-\")\n",
                                        "    json_tokens = [\"{\", \"}\", \"\\\"\", \":\", \",\"]\n",
                                        "    key_tokens = [\"name\", \"email\", \"order_id\", \"OD\", \"od\"]\n",
                                        "    extra_tokens = ascii_chars + [f\"##{ch}\" for ch in ascii_chars] + json_tokens + key_tokens\n",
                                        "    num_added += sft_tokenizer.add_tokens(extra_tokens)\n",
                                        "    print(f\"Added {num_added} total tokens (special + ASCII)\")\n",
                                        "\n",
                                        "    # Create model and resize embeddings\n",
                                        "    sft_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
                                        "    sft_model.resize_token_embeddings(len(sft_tokenizer))\n",
                                        "\n",
                                        "if sft_tokenizer.pad_token_id is None:\n",
                                        "    sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
                                        "sft_model.config.pad_token_id = sft_tokenizer.pad_token_id\n",
                                        "sft_model.generation_config.pad_token_id = sft_tokenizer.pad_token_id\n",
                                        "\n",
                                        "# Sanity check: ensure key patterns have no [UNK]\n",
                                        "sample_texts = [\n",
                                        "    '{\"name\":\"张强\",\"email\":\"zhang.qiang@example.com\",\"order_id\":\"OD20250425-2019\"}',\n",
                                        "    '{\"name\":\"孙悦\",\"email\":\"sun.yue@example.com\",\"order_id\":\"OD20250424-1269\"}'\n",
                                        "]\n",
                                        "has_unk = False\n",
                                        "for text in sample_texts:\n",
                                        "    tokens = sft_tokenizer.tokenize(text)\n",
                                        "    if \"[UNK]\" in tokens:\n",
                                        "        has_unk = True\n",
                                        "        print(\"Tokenizer UNK in sample:\", text)\n",
                                        "        print(tokens)\n",
                                        "if not has_unk:\n",
                                        "    print(\"Tokenizer OK: no [UNK] in JSON/email/order_id samples\")\n",
                                        "\n",
                                        "print(f\"Vocabulary size: {len(sft_tokenizer)}\")\n",
                                        "print(f\"Model parameters: {sum(p.numel() for p in sft_model.parameters()):,}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Create SFT dataset\n",
                                        "print(\"Creating SFT dataset...\")\n",
                                        "MASK_PROMPT = True  # Only compute loss on assistant responses\n",
                                        "if not MASK_PROMPT:\n",
                                        "    print(\"Warning: mask_prompt=False will train on user tokens; outputs may echo prompts and evaluation will be noisy.\")\n",
                                        "\n",
                                        "train_dataset = SFTDataset(\n",
                                        "    data=TRAIN_DATA,\n",
                                        "    tokenizer=sft_tokenizer,\n",
                                        "    max_length=256,\n",
                                        "    mask_prompt=MASK_PROMPT\n",
                                        ")\n",
                                        "\n",
                                        "val_dataset = SFTDataset(\n",
                                        "    data=VAL_DATA,\n",
                                        "    tokenizer=sft_tokenizer,\n",
                                        "    max_length=256,\n",
                                        "    mask_prompt=MASK_PROMPT\n",
                                        ")\n",
                                        "\n",
                                        "print(f\"Train dataset: {len(train_dataset)}\")\n",
                                        "print(f\"Val dataset: {len(val_dataset)}\")\n",
                                        "\n",
                                        "# Visualize an example\n",
                                        "visualize_loss_masking(train_dataset[0], sft_tokenizer)\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Training configuration\n",
                                        "BATCH_SIZE = 4\n",
                                        "NUM_EPOCHS = 3\n",
                                        "MAX_STEPS = 1000  # Set to None to disable early stop\n",
                                        "LEARNING_RATE = 5e-5\n",
                                        "WARMUP_STEPS = 50\n",
                                        "\n",
                                        "# Create data loaders\n",
                                        "train_dataloader = DataLoader(\n",
                                        "    train_dataset,\n",
                                        "    batch_size=BATCH_SIZE,\n",
                                        "    shuffle=True\n",
                                        ")\n",
                                        "\n",
                                        "val_dataloader = DataLoader(\n",
                                        "    val_dataset,\n",
                                        "    batch_size=BATCH_SIZE,\n",
                                        "    shuffle=False\n",
                                        ")\n",
                                        "\n",
                                        "# Optimizer and scheduler\n",
                                        "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
                                        "\n",
                                        "estimated_steps = len(train_dataloader) * NUM_EPOCHS\n",
                                        "if MAX_STEPS is None:\n",
                                        "    total_steps = estimated_steps\n",
                                        "else:\n",
                                        "    total_steps = min(estimated_steps, MAX_STEPS)\n",
                                        "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
                                        "    optimizer,\n",
                                        "    start_factor=0.1,\n",
                                        "    end_factor=1.0,\n",
                                        "    total_iters=WARMUP_STEPS\n",
                                        ")\n",
                                        "\n",
                                        "print(f\"Training configuration:\")\n",
                                        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
                                        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
                                        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
                                        "print(f\"  Total steps: {total_steps}\")\n",
                                        "print(f\"  Max steps: {MAX_STEPS}\")\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# SFT Training Loop\n",
                                        "print(\"\\n\" + \"=\"*60)\n",
                                        "print(\"Starting SFT Training\")\n",
                                        "print(\"=\"*60)\n",
                                        "\n",
                                        "SAVE_SFT_MODEL = True  # Set False to skip saving\n",
                                        "\n",
                                        "def evaluate_loss(dataloader):\n",
                                        "    sft_model.eval()\n",
                                        "    total_loss = 0\n",
                                        "    total_batches = 0\n",
                                        "    with torch.no_grad():\n",
                                        "        for batch in dataloader:\n",
                                        "            input_ids = batch['input_ids'].to(device)\n",
                                        "            attention_mask = batch['attention_mask'].to(device)\n",
                                        "            labels = batch['labels'].to(device)\n",
                                        "\n",
                                        "            outputs = sft_model(\n",
                                        "                input_ids=input_ids,\n",
                                        "                attention_mask=attention_mask,\n",
                                        "                labels=labels\n",
                                        "            )\n",
                                        "            total_loss += outputs.loss.item()\n",
                                        "            total_batches += 1\n",
                                        "\n",
                                        "    return total_loss / max(total_batches, 1)\n",
                                        "\n",
                                        "sft_model.train()\n",
                                        "training_losses = []\n",
                                        "step = 0\n",
                                        "stop_training = False\n",
                                        "\n",
                                        "for epoch in range(NUM_EPOCHS):\n",
                                        "    epoch_loss = 0\n",
                                        "    num_batches = 0\n",
                                        "\n",
                                        "    sft_model.train()\n",
                                        "    for batch in train_dataloader:\n",
                                        "        # Move to device\n",
                                        "        input_ids = batch['input_ids'].to(device)\n",
                                        "        attention_mask = batch['attention_mask'].to(device)\n",
                                        "        labels = batch['labels'].to(device)\n",
                                        "\n",
                                        "        # Forward pass\n",
                                        "        outputs = sft_model(\n",
                                        "            input_ids=input_ids,\n",
                                        "            attention_mask=attention_mask,\n",
                                        "            labels=labels\n",
                                        "        )\n",
                                        "        loss = outputs.loss\n",
                                        "\n",
                                        "        # Backward pass\n",
                                        "        optimizer.zero_grad()\n",
                                        "        loss.backward()\n",
                                        "        torch.nn.utils.clip_grad_norm_(sft_model.parameters(), 1.0)\n",
                                        "        optimizer.step()\n",
                                        "        scheduler.step()\n",
                                        "\n",
                                        "        epoch_loss += loss.item()\n",
                                        "        num_batches += 1\n",
                                        "        step += 1\n",
                                        "\n",
                                        "        if step % 20 == 0:\n",
                                        "            training_losses.append(loss.item())\n",
                                        "            print(f\"Step {step:4d} | Loss: {loss.item():.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
                                        "\n",
                                        "        if MAX_STEPS is not None and step >= MAX_STEPS:\n",
                                        "            stop_training = True\n",
                                        "            break\n",
                                        "\n",
                                        "    avg_epoch_loss = epoch_loss / max(num_batches, 1)\n",
                                        "    val_loss = evaluate_loss(val_dataloader)\n",
                                        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} completed\")\n",
                                        "    print(f\"  Train loss: {avg_epoch_loss:.4f}\")\n",
                                        "    print(f\"  Val loss:   {val_loss:.4f}\")\n",
                                        "\n",
                                        "    if stop_training:\n",
                                        "        print(f\"Reached MAX_STEPS={MAX_STEPS}, stopping early.\")\n",
                                        "        break\n",
                                        "\n",
                                        "if SAVE_SFT_MODEL:\n",
                                        "    SFT_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
                                        "    sft_model.save_pretrained(SFT_SAVE_DIR)\n",
                                        "    sft_tokenizer.save_pretrained(SFT_SAVE_DIR)\n",
                                        "\n",
                                        "    meta = {\n",
                                        "        \"task\": SFT_TASK,\n",
                                        "        \"use_plain_extract\": USE_PLAIN_EXTRACT,\n",
                                        "        \"labels\": TASK_LABELS,\n",
                                        "        \"model_name\": model_name,\n",
                                        "    }\n",
                                        "    with open(SFT_SAVE_DIR / \"sft_task.json\", \"w\", encoding=\"utf-8\") as f:\n",
                                        "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
                                        "\n",
                                        "    print(f\"Saved SFT model to {SFT_SAVE_DIR}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Visualize training progress\n",
                                        "plt.figure(figsize=(10, 4))\n",
                                        "plt.plot(training_losses, 'b-', linewidth=2)\n",
                                        "plt.xlabel('Step (x20)')\n",
                                        "plt.ylabel('Loss')\n",
                                        "plt.title('SFT Training Loss')\n",
                                        "plt.grid(True, alpha=0.3)\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 6：评估微调后的模型\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import re\n",
                                        "\n",
                                        "def generate_response(model, tokenizer, user_message: str, max_new_tokens: int = 100, temperature: float = 0.7, do_sample: bool = True, top_p: float = 0.9) -> str:\n",
                                        "    \"\"\"\n",
                                        "    Generate a response using the fine-tuned model.\n",
                                        "    \"\"\"\n",
                                        "    model.eval()\n",
                                        "\n",
                                        "    # Format as conversation\n",
                                        "    prompt = f\"<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                                        "\n",
                                        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                                        "\n",
                                        "    if tokenizer.pad_token_id is None:\n",
                                        "        tokenizer.pad_token = tokenizer.eos_token\n",
                                        "    if model.config.pad_token_id is None:\n",
                                        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "    if model.generation_config.pad_token_id is None:\n",
                                        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "    eos_id = tokenizer.convert_tokens_to_ids('<|im_end|>')\n",
                                        "    if eos_id is None or eos_id == tokenizer.unk_token_id:\n",
                                        "        eos_id = tokenizer.eos_token_id\n",
                                        "\n",
                                        "    gen_kwargs = {\n",
                                        "        \"max_new_tokens\": max_new_tokens,\n",
                                        "        \"do_sample\": do_sample,\n",
                                        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
                                        "        \"eos_token_id\": eos_id,\n",
                                        "        \"repetition_penalty\": 1.1,\n",
                                        "        \"no_repeat_ngram_size\": 3,\n",
                                        "    }\n",
                                        "    if do_sample:\n",
                                        "        gen_kwargs[\"temperature\"] = temperature\n",
                                        "        gen_kwargs[\"top_p\"] = top_p\n",
                                        "\n",
                                        "    input_len = inputs['input_ids'].shape[-1]\n",
                                        "    with torch.no_grad():\n",
                                        "        outputs = model.generate(\n",
                                        "            **inputs,\n",
                                        "            **gen_kwargs,\n",
                                        "        )\n",
                                        "\n",
                                        "    gen_ids = outputs[0][input_len:]\n",
                                        "    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=False)\n",
                                        "    gen_text = gen_text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
                                        "    if \"<|im_end|>\" in gen_text:\n",
                                        "        gen_text = gen_text.split(\"<|im_end|>\")[0]\n",
                                        "    return gen_text.strip()\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Quantitative evaluation on held-out test set\n",
                                        "import re\n",
                                        "import json\n",
                                        "\n",
                                        "def clean_generated(text):\n",
                                        "    if text is None:\n",
                                        "        return \"\"\n",
                                        "    cleaned = text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\")\n",
                                        "    for token in [\"<|im_start|>\", \"<|im_end|>\", \"<|assistant|>\", \"<|user|>\", \"<|system|>\"]:\n",
                                        "        cleaned = cleaned.replace(token, \"\")\n",
                                        "    return cleaned.strip()\n",
                                        "\n",
                                        "def normalize_text(text):\n",
                                        "    text = clean_generated(text)\n",
                                        "    return \" \".join(text.strip().lower().split())\n",
                                        "\n",
                                        "def extract_number(text):\n",
                                        "    match = re.search(r\"-?\\d+\", text)\n",
                                        "    return match.group(0) if match else None\n",
                                        "\n",
                                        "def extract_json(text):\n",
                                        "    text = clean_generated(text)\n",
                                        "    matches = re.findall(r\"\\{.*?\\}\", text, flags=re.DOTALL)\n",
                                        "    if not matches:\n",
                                        "        return None\n",
                                        "    for candidate in reversed(matches):\n",
                                        "        try:\n",
                                        "            return json.loads(candidate)\n",
                                        "        except Exception:\n",
                                        "            continue\n",
                                        "    return None\n",
                                        "\n",
                                        "def _to_ascii_digits(text: str) -> str:\n",
                                        "    return text.translate(str.maketrans(\"０１２３４５６７８９\", \"0123456789\"))\n",
                                        "\n",
                                        "def _normalize_json_key(key: str) -> str:\n",
                                        "    key = key.replace(\"##\", \"\")\n",
                                        "    key = re.sub(r\"\\s+\", \"\", key)\n",
                                        "    key = re.sub(r\"_+\", \"_\", key)\n",
                                        "    if key == \"orderid\":\n",
                                        "        key = \"order_id\"\n",
                                        "    return _to_ascii_digits(key)\n",
                                        "\n",
                                        "def _normalize_json_value(val):\n",
                                        "    if isinstance(val, str):\n",
                                        "        val = val.replace(\"##\", \"\")\n",
                                        "        val = re.sub(r\"\\s+\", \"\", val)\n",
                                        "        return _to_ascii_digits(val)\n",
                                        "    return val\n",
                                        "\n",
                                        "def normalize_json_obj(obj):\n",
                                        "    if not isinstance(obj, dict):\n",
                                        "        return obj\n",
                                        "    normalized = {}\n",
                                        "    for k, v in obj.items():\n",
                                        "        nk = _normalize_json_key(k)\n",
                                        "        nv = _normalize_json_value(v)\n",
                                        "        normalized[nk] = nv\n",
                                        "    return normalized\n",
                                        "\n",
                                        "USE_PLAIN_EVAL = USE_PLAIN_EXTRACT\n",
                                        "\n",
                                        "def _normalize_text_for_eval(text: str) -> str:\n",
                                        "    text = clean_generated(text)\n",
                                        "    text = text.replace(\"##\", \"\")\n",
                                        "    text = _to_ascii_digits(text)\n",
                                        "    return re.sub(r\"\\s+\", \"\", text)\n",
                                        "\n",
                                        "def parse_plain_fields(text: str):\n",
                                        "    text = _normalize_text_for_eval(text)\n",
                                        "    name = None\n",
                                        "    email = None\n",
                                        "    order_id = None\n",
                                        "\n",
                                        "    m = re.search(r\"姓名[:：]?([^；;，,。\\n]+)\", text)\n",
                                        "    if m:\n",
                                        "        name = m.group(1)\n",
                                        "\n",
                                        "    m = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)\n",
                                        "    if m:\n",
                                        "        email = m.group(0)\n",
                                        "\n",
                                        "    m = re.search(r\"OD\\d{8}-\\d{4}\", text)\n",
                                        "    if m:\n",
                                        "        order_id = m.group(0)\n",
                                        "\n",
                                        "    if not (name and email and order_id):\n",
                                        "        return None\n",
                                        "    return {\"name\": name, \"email\": email, \"order_id\": order_id}\n",
                                        "\n",
                                        "def score_prediction(pred, record):\n",
                                        "    pred = clean_generated(pred)\n",
                                        "    pred_norm = normalize_text(pred)\n",
                                        "    expected = record[\"expected\"]\n",
                                        "    expected_norm = normalize_text(expected)\n",
                                        "    metric = record[\"metric\"]\n",
                                        "\n",
                                        "    if metric == \"numeric\":\n",
                                        "        return extract_number(pred_norm) == expected\n",
                                        "\n",
                                        "    if metric == \"json\":\n",
                                        "        if USE_PLAIN_EVAL:\n",
                                        "            pred_obj = parse_plain_fields(pred)\n",
                                        "            try:\n",
                                        "                expected_obj = json.loads(expected)\n",
                                        "            except Exception:\n",
                                        "                expected_obj = None\n",
                                        "            if pred_obj is None or expected_obj is None:\n",
                                        "                return False\n",
                                        "            return normalize_json_obj(pred_obj) == normalize_json_obj(expected_obj)\n",
                                        "\n",
                                        "        pred_obj = extract_json(pred)\n",
                                        "        try:\n",
                                        "            expected_obj = json.loads(expected)\n",
                                        "        except Exception:\n",
                                        "            expected_obj = None\n",
                                        "        if pred_obj is None or expected_obj is None:\n",
                                        "            return False\n",
                                        "        return normalize_json_obj(pred_obj) == normalize_json_obj(expected_obj)\n",
                                        "\n",
                                        "    return expected_norm in pred_norm\n",
                                        "\n",
                                        "EVAL_MAX_SAMPLES = 200  # Set None for full test set\n",
                                        "EVAL_PROGRESS_EVERY = 50\n",
                                        "EVAL_MAX_NEW_TOKENS = 120\n",
                                        "EVAL_DO_SAMPLE = False\n",
                                        "EVAL_TEMPERATURE = 0.0\n",
                                        "EVAL_TOP_P = 1.0\n",
                                        "EVAL_RUN_BASE = True\n",
                                        "USE_LM_CLASSIFIER = TASK_LABELS is not None\n",
                                        "\n",
                                        "def _build_prompt(instruction, use_chatml):\n",
                                        "    if use_chatml:\n",
                                        "        return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                                        "    return instruction\n",
                                        "\n",
                                        "def _lm_classify(model, tok, prompt, labels):\n",
                                        "    prompt_ids = tok(prompt, add_special_tokens=False)[\"input_ids\"]\n",
                                        "    if not prompt_ids:\n",
                                        "        return labels[0]\n",
                                        "    best_label = labels[0]\n",
                                        "    best_score = float(\"-inf\")\n",
                                        "    for label in labels:\n",
                                        "        label_ids = tok(label, add_special_tokens=False)[\"input_ids\"]\n",
                                        "        input_ids = torch.tensor([prompt_ids + label_ids]).to(device)\n",
                                        "        with torch.no_grad():\n",
                                        "            logits = model(input_ids).logits\n",
                                        "        start = len(prompt_ids)\n",
                                        "        if start == 0 or len(label_ids) == 0:\n",
                                        "            continue\n",
                                        "        log_probs = torch.log_softmax(logits[0, start - 1:start - 1 + len(label_ids)], dim=-1)\n",
                                        "        target = input_ids[0, start:start + len(label_ids)]\n",
                                        "        token_logprobs = log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
                                        "        score = token_logprobs.mean().item()\n",
                                        "        if score > best_score:\n",
                                        "            best_score = score\n",
                                        "            best_label = label\n",
                                        "    return best_label\n",
                                        "\n",
                                        "def run_model(model, instruction, use_chatml):\n",
                                        "    if USE_LM_CLASSIFIER and TASK_LABELS:\n",
                                        "        prompt = _build_prompt(instruction, use_chatml)\n",
                                        "        tok = sft_tokenizer if use_chatml else tokenizer\n",
                                        "        return _lm_classify(model, tok, prompt, TASK_LABELS)\n",
                                        "    if use_chatml:\n",
                                        "        return generate_response(\n",
                                        "            model,\n",
                                        "            sft_tokenizer,\n",
                                        "            instruction,\n",
                                        "            max_new_tokens=EVAL_MAX_NEW_TOKENS,\n",
                                        "            temperature=EVAL_TEMPERATURE,\n",
                                        "            do_sample=EVAL_DO_SAMPLE,\n",
                                        "            top_p=EVAL_TOP_P\n",
                                        "        )\n",
                                        "    return generate_text(\n",
                                        "        model,\n",
                                        "        tokenizer,\n",
                                        "        instruction,\n",
                                        "        max_new_tokens=EVAL_MAX_NEW_TOKENS,\n",
                                        "        temperature=EVAL_TEMPERATURE,\n",
                                        "        do_sample=EVAL_DO_SAMPLE,\n",
                                        "        top_p=EVAL_TOP_P\n",
                                        "    )\n",
                                        "\n",
                                        "def evaluate_model(model, use_chatml):\n",
                                        "    results = {}\n",
                                        "    samples = []\n",
                                        "    records = TEST_RECORDS if EVAL_MAX_SAMPLES is None else TEST_RECORDS[:EVAL_MAX_SAMPLES]\n",
                                        "    total = len(records)\n",
                                        "    for idx, record in enumerate(records, 1):\n",
                                        "        pred = run_model(model, record[\"instruction\"], use_chatml)\n",
                                        "        correct = score_prediction(pred, record)\n",
                                        "        results.setdefault(record[\"category\"], []).append(correct)\n",
                                        "        if len(samples) < 3:\n",
                                        "            samples.append((record[\"instruction\"], record[\"expected\"], pred))\n",
                                        "        if EVAL_PROGRESS_EVERY and (idx % EVAL_PROGRESS_EVERY == 0 or idx == total):\n",
                                        "            print(f\"  evaluated {idx}/{total}\")\n",
                                        "\n",
                                        "    summary = {cat: sum(vals) / len(vals) for cat, vals in results.items()}\n",
                                        "    summary[\"overall\"] = sum(sum(v) for v in results.values()) / sum(len(v) for v in results.values())\n",
                                        "    return summary, samples\n",
                                        "\n",
                                        "print(\"Evaluation on held-out test set\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "base_summary = {}\n",
                                        "base_samples = []\n",
                                        "if EVAL_RUN_BASE:\n",
                                        "    base_summary, base_samples = evaluate_model(base_model, use_chatml=False)\n",
                                        "sft_summary, sft_samples = evaluate_model(sft_model, use_chatml=True)\n",
                                        "\n",
                                        "if EVAL_RUN_BASE:\n",
                                        "    print(\"\\nBase model:\")\n",
                                        "    for cat, acc in base_summary.items():\n",
                                        "        print(f\"  {cat}: {acc*100:.1f}%\" if cat != \"overall\" else f\"  overall: {acc*100:.1f}%\")\n",
                                        "\n",
                                        "print(\"\\nSFT model:\")\n",
                                        "for cat, acc in sft_summary.items():\n",
                                        "    print(f\"  {cat}: {acc*100:.1f}%\" if cat != \"overall\" else f\"  overall: {acc*100:.1f}%\")\n",
                                        "\n",
                                        "print(\"\\nSamples (SFT):\")\n",
                                        "for ins, exp, pred in sft_samples:\n",
                                        "    print(f\"  Q: {ins}\")\n",
                                        "    print(f\"  Expected: {exp}\")\n",
                                        "    print(f\"  Pred: {pred[:80]}\")\n",
                                        "    print(\"  ---\")\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Side-by-side comparison on a few test items\n",
                                        "print(\"\\nComparison: Base vs SFT\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "for record in TEST_RECORDS[:5]:\n",
                                        "    instruction = record[\"instruction\"]\n",
                                        "    expected = record[\"expected\"]\n",
                                        "    base_pred = run_model(base_model, instruction, use_chatml=False)\n",
                                        "    sft_pred = run_model(sft_model, instruction, use_chatml=True)\n",
                                        "\n",
                                        "    print(f\"\\nQ: {instruction}\")\n",
                                        "    print(f\"Expected: {expected}\")\n",
                                        "    print(f\"Base: {base_pred[:80]}\")\n",
                                        "    print(f\"SFT:  {sft_pred[:80]}\")\n",
                                        "    print(\"-\" * 40)\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 总结\n",
                                        "\n",
                                        "1. **SFT 的作用**：将只会文本续写的基础模型训练为能遵循指令、进行对话的模型\n",
                                        "\n",
                                        "2. **ChatML 格式**：用角色标记（`<|im_start|>`, `<|im_end|>`）表示对话的标准方式\n",
                                        "\n",
                                        "3. **Loss Masking**：关键技术——只在 assistant 回复上计算 loss（在 labels 中用 -100 屏蔽）\n",
                                        "\n",
                                        "4. **真实训练**：我们实际微调了 GPT-2，并看到它学会以对话形式回答问题\n",
                                        "\n",
                                        "### 关键要点\n",
                                        "\n",
                                        "| 维度 | 说明 |\n",
                                        "|--------|--------|\n",
                                        "| **数据** | 高质量的（instruction, response）样本对 |\n",
                                        "| **格式** | ChatML 或类似的结构化格式 |\n",
                                        "| **Loss** | 仅对 assistant tokens 计算（-100 mask） |\n",
                                        "| **结果** | 基础模型 → 对话模型 |\n",
                                        "\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 练习\n",
                                        "\n",
                                        "1. **扩充数据集**：加入更多样的指令类型（翻译、摘要等）\n",
                                        "2. **多轮训练**：确保模型能处理多轮对话\n",
                                        "3. **评估指标**：实现 ROUGE 或其他指标评估回复质量\n",
                                        "4. **超参数调优**：尝试学习率、batch size、epochs 等\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Exercise space\n",
                                        "\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 4
}
