{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "# Ch8 | SFT（Supervised Fine-Tuning）：让模型学会遵循指令\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "**目标：** 将基础模型转变为能够遵循指令的对话模型\n",
                                        "\n",
                                        "**本 notebook 使用 中文 GPT-2 和 HuggingFace Transformers 演示真实的 SFT 训练。**\n",
                                        "\n",
                                        "**设备建议：** GPU；CPU 仅建议阅读/演示。\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## 内容\n",
                                        "\n",
                                        "1. **SFT 原理**：从文本续写到对话\n",
                                        "2. **ChatML 格式**：标准对话格式\n",
                                        "3. **Loss Masking**：仅在 assistant 回复上计算损失\n",
                                        "4. **真实训练**：对 中文 GPT-2 进行指令微调\n",
                                        "\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 预备知识\n",
                                        "\n",
                                        "### 什么是 SFT？\n",
                                        "\n",
                                        "**SFT（Supervised Fine-Tuning）** = 教模型“如何进行对话”\n",
                                        "\n",
                                        "| | 基础模型 | 对话模型（SFT 后） |\n",
                                        "|:---|:---|:---|\n",
                                        "| 训练目标 | 预测下一个 token | 学习对话格式 |\n",
                                        "| 输入 | 任意文本 | 结构化对话 |\n",
                                        "| 输出 | 文本续写 | 有帮助的回复 |\n",
                                        "| 使用场景 | 文本补全 | 聊天、问答、指令 |\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 环境准备\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 1,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Using device: cuda\n",
                                                            "GPU: NVIDIA GeForce RTX 5080\n",
                                                            "Memory: 17.1 GB\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "from torch.utils.data import Dataset, DataLoader\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "from typing import Dict, List, Optional, Any\n",
                                        "import warnings\n",
                                        "warnings.filterwarnings('ignore')\n",
                                        "\n",
                                        "# Check for GPU\n",
                                        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                                        "print(f\"Using device: {device}\")\n",
                                        "\n",
                                        "# Check available memory\n",
                                        "if device == 'cuda':\n",
                                        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                                        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 2,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "HuggingFace Transformers loaded successfully!\n",
                                                            "TRL not available - will use standard Trainer\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Install required packages if needed\n",
                                        "try:\n",
                                        "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
                                        "    from datasets import Dataset as HFDataset\n",
                                        "    print(\"HuggingFace Transformers loaded successfully!\")\n",
                                        "except ImportError:\n",
                                        "    print(\"Installing required packages...\")\n",
                                        "    !pip install transformers datasets accelerate -q\n",
                                        "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
                                        "    from datasets import Dataset as HFDataset\n",
                                        "\n",
                                        "# Try to import TRL for SFTTrainer (optional but recommended)\n",
                                        "try:\n",
                                        "    from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
                                        "    USE_TRL = True\n",
                                        "    print(\"TRL library loaded - will use SFTTrainer\")\n",
                                        "except ImportError:\n",
                                        "    USE_TRL = False\n",
                                        "    print(\"TRL not available - will use standard Trainer\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 1：理解基础模型与对话模型\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 3,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Loading uer/gpt2-chinese-cluecorpussmall...\n",
                                                            "Model loaded: 102,068,736 parameters\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Load GPT-2 Chinese (base model)\n",
                                        "model_name = \"uer/gpt2-chinese-cluecorpussmall\"  # Chinese GPT-2 base\n",
                                        "\n",
                                        "print(f\"Loading {model_name}...\")\n",
                                        "try:\n",
                                        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
                                        "except Exception:\n",
                                        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
                                        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
                                        "\n",
                                        "# GPT-2 doesn't have a pad token by default\n",
                                        "if tokenizer.pad_token_id is None:\n",
                                        "    tokenizer.pad_token = tokenizer.eos_token\n",
                                        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "print(f\"Model loaded: {sum(p.numel() for p in base_model.parameters()):,} parameters\")\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 4,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "============================================================\n",
                                                            "BASE MODEL BEHAVIOR (before SFT)\n",
                                                            "============================================================\n",
                                                            "\n",
                                                            "Prompt: What is machine learning?\n",
                                                            "Output: ???? -?.??? ( 2014 年 )? ( 2013 年 )?? 2.\n",
                                                            "----------------------------------------\n",
                                                            "\n",
                                                            "Prompt: Question: What is 2+2?\n",
                                                            "Answer:\n",
                                                            "Output: ? in order ， and order. [ photo ] : indeal and communication ; indicated.\n",
                                                            "----------------------------------------\n",
                                                            "\n",
                                                            "Prompt: User: Hello!\n",
                                                            "Assistant:\n",
                                                            "Output: !!!! |!!!? | | |?! | | |!\n",
                                                            "----------------------------------------\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7, do_sample=True, top_p=0.9):\n",
                                        "    \"\"\"\n",
                                        "    Generate text from a prompt.\n",
                                        "    \"\"\"\n",
                                        "    model.eval()\n",
                                        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                                        "\n",
                                        "    if tokenizer.pad_token_id is None:\n",
                                        "        tokenizer.pad_token = tokenizer.eos_token\n",
                                        "    if model.config.pad_token_id is None:\n",
                                        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "    if model.generation_config.pad_token_id is None:\n",
                                        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "    gen_kwargs = {\n",
                                        "        \"max_new_tokens\": max_new_tokens,\n",
                                        "        \"do_sample\": do_sample,\n",
                                        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
                                        "        \"repetition_penalty\": 1.1,\n",
                                        "        \"no_repeat_ngram_size\": 3,\n",
                                        "    }\n",
                                        "    if do_sample:\n",
                                        "        gen_kwargs[\"temperature\"] = temperature\n",
                                        "        gen_kwargs[\"top_p\"] = top_p\n",
                                        "\n",
                                        "    input_len = inputs['input_ids'].shape[-1]\n",
                                        "    with torch.no_grad():\n",
                                        "        outputs = model.generate(\n",
                                        "            **inputs,\n",
                                        "            **gen_kwargs,\n",
                                        "        )\n",
                                        "\n",
                                        "    gen_ids = outputs[0][input_len:]\n",
                                        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
                                        "\n",
                                        "# Test base model behavior\n",
                                        "print(\"=\" * 60)\n",
                                        "print(\"BASE MODEL BEHAVIOR (before SFT)\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "test_prompts = [\n",
                                        "    \"What is machine learning?\",\n",
                                        "    \"Question: What is 2+2?\\nAnswer:\",\n",
                                        "    \"User: Hello!\\nAssistant:\",\n",
                                        "]\n",
                                        "\n",
                                        "for prompt in test_prompts:\n",
                                        "    print(f\"\\nPrompt: {prompt}\")\n",
                                        "    print(f\"Output: {generate_text(base_model, tokenizer, prompt, max_new_tokens=30)}\")\n",
                                        "    print(\"-\" * 40)\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "如你所见，基础模型并不理解指令格式——它只会沿着输入的风格继续生成文本。\n",
                                        "\n",
                                        "## Part 2：ChatML 格式\n",
                                        "\n",
                                        "ChatML 是一种用于表示对话的标准格式：\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 5,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "ChatML Format Example:\n",
                                                            "============================================================\n",
                                                            "<|im_start|>system\n",
                                                            "You are a helpful AI assistant.<|im_end|>\n",
                                                            "<|im_start|>user\n",
                                                            "What is Python?<|im_end|>\n",
                                                            "<|im_start|>assistant\n",
                                                            "Python is a high-level programming language known for its simplicity and readability.<|im_end|>\n",
                                                            "\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Define special tokens for chat format\n",
                                        "SPECIAL_TOKENS = {\n",
                                        "    \"bos_token\": \"<|startoftext|>\",\n",
                                        "    \"eos_token\": \"<|endoftext|>\",\n",
                                        "    \"pad_token\": \"<|pad|>\",\n",
                                        "    \"additional_special_tokens\": [\n",
                                        "        \"<|im_start|>\",\n",
                                        "        \"<|im_end|>\",\n",
                                        "        \"<|user|>\",\n",
                                        "        \"<|assistant|>\",\n",
                                        "        \"<|system|>\",\n",
                                        "    ]\n",
                                        "}\n",
                                        "\n",
                                        "def format_conversation(messages: List[Dict[str, str]], tokenizer=None) -> str:\n",
                                        "    \"\"\"\n",
                                        "    Convert a conversation to ChatML format.\n",
                                        "    \n",
                                        "    Args:\n",
                                        "        messages: List of {\"role\": str, \"content\": str} dicts\n",
                                        "        tokenizer: Optional tokenizer for encoding\n",
                                        "    \n",
                                        "    Returns:\n",
                                        "        Formatted conversation string\n",
                                        "    \"\"\"\n",
                                        "    formatted = \"\"\n",
                                        "    for msg in messages:\n",
                                        "        role = msg[\"role\"]\n",
                                        "        content = msg[\"content\"]\n",
                                        "        formatted += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
                                        "    return formatted\n",
                                        "\n",
                                        "# Example conversation\n",
                                        "example_conv = [\n",
                                        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
                                        "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
                                        "    {\"role\": \"assistant\", \"content\": \"Python is a high-level programming language known for its simplicity and readability.\"},\n",
                                        "]\n",
                                        "\n",
                                        "print(\"ChatML Format Example:\")\n",
                                        "print(\"=\" * 60)\n",
                                        "print(format_conversation(example_conv))"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 3：SFT 训练数据\n",
                                        "\n",
                                        "默认使用“诉求分类”数据集（gpt2_sft_*.jsonl），也可以切换到 JSON 抽取任务（custom_sft_*.jsonl）。\n",
                                        "JSON 抽取时字段：name / email / order_id（只输出 JSON）。\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 6,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Regenerated gpt2_sft_*.jsonl with classification labels\n",
                                                            "SFT task: classification\n",
                                                            "Train: 8000 | Val: 1000 | Test: 1000\n",
                                                            "Categories: ['诉求分类']\n",
                                                            "Example: [{'role': 'user', 'content': '请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：订单OD20240124-5506相关用户郭婷邮箱guo.ting@example.com，问题：退款申请。'}, {'role': 'assistant', 'content': '退款申请'}]\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Structured SFT training data from data/ (train/val/test splits)\n",
                                        "import json\n",
                                        "from pathlib import Path\n",
                                        "import os\n",
                                        "\n",
                                        "def resolve_data_dir():\n",
                                        "    candidates = [Path.cwd(), Path.cwd().parent]\n",
                                        "    for base in candidates:\n",
                                        "        data_dir = base / \"data\"\n",
                                        "        if data_dir.exists():\n",
                                        "            return str(data_dir)\n",
                                        "    return os.path.join(os.getcwd(), \"data\")\n",
                                        "\n",
                                        "DATA_DIR = resolve_data_dir()\n",
                                        "\n",
                                        "\n",
                                        "def load_jsonl(path):\n",
                                        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
                                        "        return [json.loads(line) for line in f if line.strip()]\n",
                                        "\n",
                                        "# Task selection: classification or JSON extraction\n",
                                        "SFT_TASK = \"classification\"  # \"classification\" or \"json_extraction\"\n",
                                        "USE_PLAIN_EXTRACT = False  # Only for json_extraction\n",
                                        "\n",
                                        "if SFT_TASK == \"classification\":\n",
                                        "    # Optional: regenerate a simpler, Chinese-only classification dataset\n",
                                        "    REGEN_SFT_DATA = True\n",
                                        "    ISSUE_LABELS = [\"延迟发货\", \"退款申请\", \"地址修改\", \"物流异常\", \"售后咨询\", \"发票问题\"]\n",
                                        "    TASK_LABELS = ISSUE_LABELS\n",
                                        "    TRAIN_FILE = \"gpt2_sft_train.jsonl\"\n",
                                        "    VAL_FILE = \"gpt2_sft_val.jsonl\"\n",
                                        "    TEST_FILE = \"gpt2_sft_test.jsonl\"\n",
                                        "else:\n",
                                        "    REGEN_SFT_DATA = False\n",
                                        "    ISSUE_LABELS = []\n",
                                        "    TASK_LABELS = None\n",
                                        "    TRAIN_FILE = \"custom_sft_train.jsonl\"\n",
                                        "    VAL_FILE = \"custom_sft_val.jsonl\"\n",
                                        "    TEST_FILE = \"custom_sft_test.jsonl\"\n",
                                        "\n",
                                        "if SFT_TASK != \"json_extraction\":\n",
                                        "    USE_PLAIN_EXTRACT = False\n",
                                        "\n",
                                        "\n",
                                        "def _rand_order_id(rng):\n",
                                        "    year = rng.choice([2024, 2025])\n",
                                        "    month = rng.randint(1, 12)\n",
                                        "    day = rng.randint(1, 28)\n",
                                        "    return f\"OD{year:04d}{month:02d}{day:02d}-{rng.randint(1000, 9999)}\"\n",
                                        "\n",
                                        "\n",
                                        "def _generate_records(count, seed):\n",
                                        "    import random\n",
                                        "    rng = random.Random(seed)\n",
                                        "    names = [\n",
                                        "        (\"陈洁\", \"chen.jie\"), (\"孙悦\", \"sun.yue\"), (\"张强\", \"zhang.qiang\"), (\"周凯\", \"zhou.kai\"),\n",
                                        "        (\"刘洋\", \"liu.yang\"), (\"王伟\", \"wang.wei\"), (\"吴磊\", \"wu.lei\"), (\"李娜\", \"li.na\"),\n",
                                        "        (\"赵敏\", \"zhao.min\"), (\"黄涛\", \"huang.tao\"), (\"郭婷\", \"guo.ting\"), (\"马超\", \"ma.chao\"),\n",
                                        "    ]\n",
                                        "    templates = [\n",
                                        "        \"客户{name}（{email}）反馈订单{order_id}{issue}。\",\n",
                                        "        \"订单{order_id}相关用户{name}邮箱{email}，问题：{issue}。\",\n",
                                        "        \"请处理：姓名{name}，邮箱{email}，订单号{order_id}，原因：{issue}。\",\n",
                                        "        \"用户{name}邮箱{email}，订单{order_id}需要处理：{issue}。\",\n",
                                        "        \"{name}反馈订单{order_id}{issue}，联系邮箱{email}。\",\n",
                                        "    ]\n",
                                        "\n",
                                        "    records = []\n",
                                        "    for _ in range(count):\n",
                                        "        name, email_user = rng.choice(names)\n",
                                        "        email = f\"{email_user}@example.com\"\n",
                                        "        order_id = _rand_order_id(rng)\n",
                                        "        issue = rng.choice(ISSUE_LABELS)\n",
                                        "        text = rng.choice(templates).format(\n",
                                        "            name=name,\n",
                                        "            email=email,\n",
                                        "            order_id=order_id,\n",
                                        "            issue=issue,\n",
                                        "        )\n",
                                        "        instruction = (\n",
                                        "            \"请判断用户诉求类别，只输出类别（\" + \"/\".join(ISSUE_LABELS) + \"）。\"\n",
                                        "            f\"文本：{text}\"\n",
                                        "        )\n",
                                        "        response = issue\n",
                                        "        records.append({\n",
                                        "            \"instruction\": instruction,\n",
                                        "            \"response\": response,\n",
                                        "            \"category\": \"诉求分类\",\n",
                                        "            \"metric\": \"text\",\n",
                                        "            \"expected\": response,\n",
                                        "        })\n",
                                        "    return records\n",
                                        "\n",
                                        "\n",
                                        "def _write_jsonl(path, records):\n",
                                        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
                                        "        for record in records:\n",
                                        "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
                                        "\n",
                                        "\n",
                                        "if REGEN_SFT_DATA and SFT_TASK == \"classification\":\n",
                                        "    train_records = _generate_records(8000, seed=42)\n",
                                        "    val_records = _generate_records(1000, seed=43)\n",
                                        "    test_records = _generate_records(1000, seed=44)\n",
                                        "    _write_jsonl(os.path.join(DATA_DIR, \"gpt2_sft_train.jsonl\"), train_records)\n",
                                        "    _write_jsonl(os.path.join(DATA_DIR, \"gpt2_sft_val.jsonl\"), val_records)\n",
                                        "    _write_jsonl(os.path.join(DATA_DIR, \"gpt2_sft_test.jsonl\"), test_records)\n",
                                        "    print(\"Regenerated gpt2_sft_*.jsonl with classification labels\")\n",
                                        "\n",
                                        "TRAIN_RECORDS = load_jsonl(os.path.join(DATA_DIR, TRAIN_FILE))\n",
                                        "VAL_RECORDS = load_jsonl(os.path.join(DATA_DIR, VAL_FILE))\n",
                                        "TEST_RECORDS = load_jsonl(os.path.join(DATA_DIR, TEST_FILE))\n",
                                        "\n",
                                        "\n",
                                        "def build_plain_instruction(record):\n",
                                        "    instruction = record[\"instruction\"]\n",
                                        "    text = instruction\n",
                                        "    if \"文本：\" in instruction:\n",
                                        "        text = instruction.split(\"文本：\", 1)[-1]\n",
                                        "    return (\n",
                                        "        \"从文本中抽取姓名、邮箱、订单号，按“姓名：…；邮箱：…；订单号：…”格式回答。\"\n",
                                        "        f\"文本：{text}\"\n",
                                        "    )\n",
                                        "\n",
                                        "\n",
                                        "def build_plain_response(record):\n",
                                        "    try:\n",
                                        "        obj = json.loads(record[\"response\"])\n",
                                        "    except Exception:\n",
                                        "        return record[\"response\"]\n",
                                        "    return f\"姓名：{obj.get('name','')}；邮箱：{obj.get('email','')}；订单号：{obj.get('order_id','')}\"\n",
                                        "\n",
                                        "\n",
                                        "def to_messages(record):\n",
                                        "    if USE_PLAIN_EXTRACT:\n",
                                        "        user_text = build_plain_instruction(record)\n",
                                        "        assistant_text = build_plain_response(record)\n",
                                        "    else:\n",
                                        "        user_text = record[\"instruction\"]\n",
                                        "        assistant_text = record[\"response\"]\n",
                                        "    return {\n",
                                        "        \"messages\": [\n",
                                        "            {\"role\": \"user\", \"content\": user_text},\n",
                                        "            {\"role\": \"assistant\", \"content\": assistant_text},\n",
                                        "        ]\n",
                                        "    }\n",
                                        "\n",
                                        "\n",
                                        "TRAIN_DATA = [to_messages(r) for r in TRAIN_RECORDS]\n",
                                        "VAL_DATA = [to_messages(r) for r in VAL_RECORDS]\n",
                                        "TEST_DATA = [to_messages(r) for r in TEST_RECORDS]\n",
                                        "\n",
                                        "print(f\"SFT task: {SFT_TASK}\")\n",
                                        "print(f\"Train: {len(TRAIN_DATA)} | Val: {len(VAL_DATA)} | Test: {len(TEST_DATA)}\")\n",
                                        "print(f\"Categories: {sorted(set(r['category'] for r in TRAIN_RECORDS))}\")\n",
                                        "print(f\"Example: {TRAIN_DATA[0]['messages']}\")\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 4：Loss Masking\n",
                                        "\n",
                                        "SFT 的关键点：**只在 assistant 回复上计算 loss**，而不是在 user 输入上计算。\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 7,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class SFTDataset(torch.utils.data.Dataset):\n",
                                        "    \"\"\"\n",
                                        "    Dataset for Supervised Fine-Tuning with proper loss masking.\n",
                                        "    \n",
                                        "    Key features:\n",
                                        "    - Formats conversations in ChatML style\n",
                                        "    - Creates labels with -100 for non-assistant tokens (loss masking)\n",
                                        "    - Handles variable length sequences\n",
                                        "    \"\"\"\n",
                                        "    \n",
                                        "    def __init__(\n",
                                        "        self,\n",
                                        "        data: List[Dict],\n",
                                        "        tokenizer,\n",
                                        "        max_length: int = 512,\n",
                                        "        mask_prompt: bool = True\n",
                                        "    ):\n",
                                        "        self.data = data\n",
                                        "        self.tokenizer = tokenizer\n",
                                        "        self.max_length = max_length\n",
                                        "        self.mask_prompt = mask_prompt\n",
                                        "        \n",
                                        "        # Pre-process all examples\n",
                                        "        self.processed = []\n",
                                        "        for item in data:\n",
                                        "            processed = self._process_example(item)\n",
                                        "            if processed is not None:\n",
                                        "                self.processed.append(processed)\n",
                                        "        \n",
                                        "        print(f\"Processed {len(self.processed)}/{len(data)} examples\")\n",
                                        "    \n",
                                        "    def _process_example(self, item: Dict) -> Optional[Dict]:\n",
                                        "        \"\"\"\n",
                                        "        Process a single conversation example.\n",
                                        "        \n",
                                        "        Returns:\n",
                                        "            Dict with input_ids, attention_mask, and labels\n",
                                        "        \"\"\"\n",
                                        "        messages = item['messages']\n",
                                        "        \n",
                                        "        # Build the full conversation text and track assistant positions\n",
                                        "        full_text = \"\"\n",
                                        "        assistant_ranges = []  # List of (start, end) character positions for assistant responses\n",
                                        "        \n",
                                        "        for msg in messages:\n",
                                        "            role = msg['role']\n",
                                        "            content = msg['content']\n",
                                        "            \n",
                                        "            msg_text = f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
                                        "            \n",
                                        "            if role == 'assistant':\n",
                                        "                # Track where assistant content starts and ends\n",
                                        "                content_start = len(full_text) + len(f\"<|im_start|>{role}\\n\")\n",
                                        "                content_end = content_start + len(content)\n",
                                        "                assistant_ranges.append((content_start, content_end))\n",
                                        "            \n",
                                        "            full_text += msg_text\n",
                                        "        \n",
                                        "        # Tokenize\n",
                                        "        encoding = self.tokenizer(\n",
                                        "            full_text,\n",
                                        "            truncation=True,\n",
                                        "            max_length=self.max_length,\n",
                                        "            padding='max_length',\n",
                                        "            return_tensors='pt',\n",
                                        "            return_offsets_mapping=getattr(self.tokenizer, \"is_fast\", False)\n",
                                        "        )\n",
                                        "        \n",
                                        "        input_ids = encoding['input_ids'].squeeze(0)\n",
                                        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
                                        "        \n",
                                        "        # Create labels: -100 for tokens we don't want to compute loss on\n",
                                        "        if self.mask_prompt:\n",
                                        "            labels = torch.full_like(input_ids, -100)\n",
                                        "            offsets = encoding.get('offset_mapping')\n",
                                        "            if offsets is not None:\n",
                                        "                offsets = offsets.squeeze(0).tolist()\n",
                                        "                for i, (start_char, end_char) in enumerate(offsets):\n",
                                        "                    if start_char == 0 and end_char == 0:\n",
                                        "                        continue\n",
                                        "                    for a_start, a_end in assistant_ranges:\n",
                                        "                        if start_char < a_end and end_char > a_start:\n",
                                        "                            labels[i] = input_ids[i]\n",
                                        "                            break\n",
                                        "            else:\n",
                                        "                # Fallback approximate mapping for non-fast tokenizers\n",
                                        "                for start_char, end_char in assistant_ranges:\n",
                                        "                    prefix = full_text[:start_char]\n",
                                        "                    prefix_tokens = self.tokenizer(prefix, add_special_tokens=False)['input_ids']\n",
                                        "                    start_token = len(prefix_tokens)\n",
                                        "\n",
                                        "                    content_text = full_text[:end_char]\n",
                                        "                    content_tokens = self.tokenizer(content_text, add_special_tokens=False)['input_ids']\n",
                                        "                    end_token = len(content_tokens)\n",
                                        "\n",
                                        "                    if start_token < self.max_length and end_token <= self.max_length:\n",
                                        "                        labels[start_token:end_token] = input_ids[start_token:end_token]\n",
                                        "        else:\n",
                                        "            labels = input_ids.clone()\n",
                                        "        \n",
                                        "        return {\n",
                                        "            'input_ids': input_ids,\n",
                                        "            'attention_mask': attention_mask,\n",
                                        "            'labels': labels\n",
                                        "        }\n",
                                        "    \n",
                                        "    def __len__(self):\n",
                                        "        return len(self.processed)\n",
                                        "    \n",
                                        "    def __getitem__(self, idx):\n",
                                        "        return self.processed[idx]"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 8,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Visualize loss masking\n",
                                        "def visualize_loss_masking(example: Dict, tokenizer):\n",
                                        "    \"\"\"\n",
                                        "    Visualize which tokens have loss computed vs masked.\n",
                                        "    \"\"\"\n",
                                        "    input_ids = example['input_ids']\n",
                                        "    labels = example['labels']\n",
                                        "    \n",
                                        "    # Only show non-padding tokens\n",
                                        "    valid_len = (input_ids != tokenizer.pad_token_id).sum().item()\n",
                                        "    input_ids = input_ids[:valid_len]\n",
                                        "    labels = labels[:valid_len]\n",
                                        "    \n",
                                        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
                                        "    \n",
                                        "    print(\"Loss Masking Visualization:\")\n",
                                        "    print(\"=\" * 60)\n",
                                        "    print(\"Green = Loss computed (assistant)\")\n",
                                        "    print(\"Red = Loss masked (user/system)\")\n",
                                        "    print(\"=\" * 60)\n",
                                        "    \n",
                                        "    # Print tokens with colors\n",
                                        "    for i, (token, label) in enumerate(zip(tokens[:50], labels[:50])):  # First 50 tokens\n",
                                        "        if label == -100:\n",
                                        "            print(f\"\\033[91m{token}\\033[0m\", end=\" \")  # Red\n",
                                        "        else:\n",
                                        "            print(f\"\\033[92m{token}\\033[0m\", end=\" \")  # Green\n",
                                        "    print(\"\\n\")\n",
                                        "    \n",
                                        "    # Statistics\n",
                                        "    masked = (labels == -100).sum().item()\n",
                                        "    total = len(labels)\n",
                                        "    print(f\"\\nMasked tokens: {masked}/{total} ({100*masked/total:.1f}%)\")\n",
                                        "    print(f\"Loss computed on: {total-masked}/{total} ({100*(total-masked)/total:.1f}%)\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 5：使用 GPT-2 进行真实 SFT 训练\n",
                                        "\n",
                                        "现在开始实际对 GPT-2 做微调！\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 9,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Adding special tokens...\n",
                                                            "Added 145 total tokens (special + ASCII)\n"
                                                  ]
                                        },
                                        {
                                                  "name": "stderr",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
                                                  ]
                                        },
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Tokenizer OK: no [UNK] in JSON/email/order_id samples\n",
                                                            "Vocabulary size: 21186\n",
                                                            "Model parameters: 102,113,280\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Add special tokens to tokenizer\n",
                                        "print(\"Adding special tokens...\")\n",
                                        "\n",
                                        "from pathlib import Path\n",
                                        "\n",
                                        "if \"DATA_DIR\" in globals():\n",
                                        "    base_dir = Path(DATA_DIR).parent\n",
                                        "else:\n",
                                        "    base_dir = Path.cwd()\n",
                                        "    if not (base_dir / \"models\").exists() and (base_dir.parent / \"models\").exists():\n",
                                        "        base_dir = base_dir.parent\n",
                                        "\n",
                                        "SFT_SAVE_DIR = base_dir / \"models\" / \"ch8_sft_gpt2\"\n",
                                        "LOAD_SFT_MODEL = False  # Set True to load a saved SFT model\n",
                                        "\n",
                                        "if LOAD_SFT_MODEL and SFT_SAVE_DIR.exists():\n",
                                        "    sft_tokenizer = AutoTokenizer.from_pretrained(SFT_SAVE_DIR)\n",
                                        "    sft_model = AutoModelForCausalLM.from_pretrained(SFT_SAVE_DIR).to(device)\n",
                                        "    print(f\"Loaded SFT model from {SFT_SAVE_DIR}\")\n",
                                        "else:\n",
                                        "    # Create a fresh tokenizer and model for SFT\n",
                                        "    try:\n",
                                        "        sft_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
                                        "    except Exception:\n",
                                        "        sft_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
                                        "\n",
                                        "    # Add special tokens\n",
                                        "    special_tokens = {\n",
                                        "        'pad_token': '<|pad|>',\n",
                                        "        'additional_special_tokens': ['<|im_start|>', '<|im_end|>']\n",
                                        "    }\n",
                                        "    num_added = sft_tokenizer.add_special_tokens(special_tokens)\n",
                                        "\n",
                                        "    # Add ASCII tokens to avoid [UNK] for IDs/emails (WordPiece uses ## for subwords)\n",
                                        "    import string\n",
                                        "    ascii_chars = list(string.ascii_lowercase + string.ascii_uppercase + string.digits + \"@._-\")\n",
                                        "    json_tokens = [\"{\", \"}\", \"\\\"\", \":\", \",\"]\n",
                                        "    key_tokens = [\"name\", \"email\", \"order_id\", \"OD\", \"od\"]\n",
                                        "    extra_tokens = ascii_chars + [f\"##{ch}\" for ch in ascii_chars] + json_tokens + key_tokens\n",
                                        "    num_added += sft_tokenizer.add_tokens(extra_tokens)\n",
                                        "    print(f\"Added {num_added} total tokens (special + ASCII)\")\n",
                                        "\n",
                                        "    # Create model and resize embeddings\n",
                                        "    sft_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
                                        "    sft_model.resize_token_embeddings(len(sft_tokenizer))\n",
                                        "\n",
                                        "if sft_tokenizer.pad_token_id is None:\n",
                                        "    sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
                                        "sft_model.config.pad_token_id = sft_tokenizer.pad_token_id\n",
                                        "sft_model.generation_config.pad_token_id = sft_tokenizer.pad_token_id\n",
                                        "\n",
                                        "# Sanity check: ensure key patterns have no [UNK]\n",
                                        "sample_texts = [\n",
                                        "    '{\"name\":\"张强\",\"email\":\"zhang.qiang@example.com\",\"order_id\":\"OD20250425-2019\"}',\n",
                                        "    '{\"name\":\"孙悦\",\"email\":\"sun.yue@example.com\",\"order_id\":\"OD20250424-1269\"}'\n",
                                        "]\n",
                                        "has_unk = False\n",
                                        "for text in sample_texts:\n",
                                        "    tokens = sft_tokenizer.tokenize(text)\n",
                                        "    if \"[UNK]\" in tokens:\n",
                                        "        has_unk = True\n",
                                        "        print(\"Tokenizer UNK in sample:\", text)\n",
                                        "        print(tokens)\n",
                                        "if not has_unk:\n",
                                        "    print(\"Tokenizer OK: no [UNK] in JSON/email/order_id samples\")\n",
                                        "\n",
                                        "print(f\"Vocabulary size: {len(sft_tokenizer)}\")\n",
                                        "print(f\"Model parameters: {sum(p.numel() for p in sft_model.parameters()):,}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 10,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Creating SFT dataset...\n",
                                                            "Processed 8000/8000 examples\n",
                                                            "Processed 1000/1000 examples\n",
                                                            "Train dataset: 8000\n",
                                                            "Val dataset: 1000\n",
                                                            "Loss Masking Visualization:\n",
                                                            "============================================================\n",
                                                            "Green = Loss computed (assistant)\n",
                                                            "Red = Loss masked (user/system)\n",
                                                            "============================================================\n",
                                                            "\u001b[91m[CLS]\u001b[0m \u001b[91m<|im_start|>\u001b[0m \u001b[91mu\u001b[0m \u001b[91ms\u001b[0m \u001b[91me\u001b[0m \u001b[91mr\u001b[0m \u001b[91m请\u001b[0m \u001b[91m判\u001b[0m \u001b[91m断\u001b[0m \u001b[91m用\u001b[0m \u001b[91m户\u001b[0m \u001b[91m诉\u001b[0m \u001b[91m求\u001b[0m \u001b[91m类\u001b[0m \u001b[91m别\u001b[0m \u001b[91m，\u001b[0m \u001b[91m只\u001b[0m \u001b[91m输\u001b[0m \u001b[91m出\u001b[0m \u001b[91m类\u001b[0m \u001b[91m别\u001b[0m \u001b[91m（\u001b[0m \u001b[91m延\u001b[0m \u001b[91m迟\u001b[0m \u001b[91m发\u001b[0m \u001b[91m货\u001b[0m \u001b[91m/\u001b[0m \u001b[91m退\u001b[0m \u001b[91m款\u001b[0m \u001b[91m申\u001b[0m \u001b[91m请\u001b[0m \u001b[91m/\u001b[0m \u001b[91m地\u001b[0m \u001b[91m址\u001b[0m \u001b[91m修\u001b[0m \u001b[91m改\u001b[0m \u001b[91m/\u001b[0m \u001b[91m物\u001b[0m \u001b[91m流\u001b[0m \u001b[91m异\u001b[0m \u001b[91m常\u001b[0m \u001b[91m/\u001b[0m \u001b[91m售\u001b[0m \u001b[91m后\u001b[0m \u001b[91m咨\u001b[0m \u001b[91m询\u001b[0m \u001b[91m/\u001b[0m \u001b[91m发\u001b[0m \u001b[91m票\u001b[0m \u001b[91m问\u001b[0m \n",
                                                            "\n",
                                                            "\n",
                                                            "Masked tokens: 122/126 (96.8%)\n",
                                                            "Loss computed on: 4/126 (3.2%)\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Create SFT dataset\n",
                                        "print(\"Creating SFT dataset...\")\n",
                                        "MASK_PROMPT = True  # Only compute loss on assistant responses\n",
                                        "if not MASK_PROMPT:\n",
                                        "    print(\"Warning: mask_prompt=False will train on user tokens; outputs may echo prompts and evaluation will be noisy.\")\n",
                                        "\n",
                                        "train_dataset = SFTDataset(\n",
                                        "    data=TRAIN_DATA,\n",
                                        "    tokenizer=sft_tokenizer,\n",
                                        "    max_length=256,\n",
                                        "    mask_prompt=MASK_PROMPT\n",
                                        ")\n",
                                        "\n",
                                        "val_dataset = SFTDataset(\n",
                                        "    data=VAL_DATA,\n",
                                        "    tokenizer=sft_tokenizer,\n",
                                        "    max_length=256,\n",
                                        "    mask_prompt=MASK_PROMPT\n",
                                        ")\n",
                                        "\n",
                                        "print(f\"Train dataset: {len(train_dataset)}\")\n",
                                        "print(f\"Val dataset: {len(val_dataset)}\")\n",
                                        "\n",
                                        "# Visualize an example\n",
                                        "visualize_loss_masking(train_dataset[0], sft_tokenizer)\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 11,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Training configuration:\n",
                                                            "  Batch size: 4\n",
                                                            "  Epochs: 3\n",
                                                            "  Learning rate: 5e-05\n",
                                                            "  Total steps: 1000\n",
                                                            "  Max steps: 1000\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Training configuration\n",
                                        "BATCH_SIZE = 4\n",
                                        "NUM_EPOCHS = 3\n",
                                        "MAX_STEPS = 1000  # Set to None to disable early stop\n",
                                        "LEARNING_RATE = 5e-5\n",
                                        "WARMUP_STEPS = 50\n",
                                        "\n",
                                        "# Create data loaders\n",
                                        "train_dataloader = DataLoader(\n",
                                        "    train_dataset,\n",
                                        "    batch_size=BATCH_SIZE,\n",
                                        "    shuffle=True\n",
                                        ")\n",
                                        "\n",
                                        "val_dataloader = DataLoader(\n",
                                        "    val_dataset,\n",
                                        "    batch_size=BATCH_SIZE,\n",
                                        "    shuffle=False\n",
                                        ")\n",
                                        "\n",
                                        "# Optimizer and scheduler\n",
                                        "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
                                        "\n",
                                        "estimated_steps = len(train_dataloader) * NUM_EPOCHS\n",
                                        "if MAX_STEPS is None:\n",
                                        "    total_steps = estimated_steps\n",
                                        "else:\n",
                                        "    total_steps = min(estimated_steps, MAX_STEPS)\n",
                                        "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
                                        "    optimizer,\n",
                                        "    start_factor=0.1,\n",
                                        "    end_factor=1.0,\n",
                                        "    total_iters=WARMUP_STEPS\n",
                                        ")\n",
                                        "\n",
                                        "print(f\"Training configuration:\")\n",
                                        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
                                        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
                                        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
                                        "print(f\"  Total steps: {total_steps}\")\n",
                                        "print(f\"  Max steps: {MAX_STEPS}\")\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 12,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stderr",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
                                                  ]
                                        },
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "\n",
                                                            "============================================================\n",
                                                            "Starting SFT Training\n",
                                                            "============================================================\n",
                                                            "Step   20 | Loss: 0.0055 | LR: 2.30e-05\n",
                                                            "Step   40 | Loss: 0.0001 | LR: 4.10e-05\n",
                                                            "Step   60 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step   80 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  100 | Loss: 0.0001 | LR: 5.00e-05\n",
                                                            "Step  120 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  140 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  160 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  180 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  200 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  220 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  240 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  260 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  280 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  300 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  320 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  340 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  360 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  380 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  400 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  420 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  440 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  460 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  480 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  500 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  520 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  540 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  560 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  580 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  600 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  620 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  640 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  660 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  680 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  700 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  720 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  740 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  760 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  780 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  800 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  820 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  840 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  860 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  880 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  900 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  920 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  940 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  960 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step  980 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "Step 1000 | Loss: 0.0000 | LR: 5.00e-05\n",
                                                            "\n",
                                                            "Epoch 1/3 completed\n",
                                                            "  Train loss: 0.0158\n",
                                                            "  Val loss:   0.0000\n",
                                                            "Reached MAX_STEPS=1000, stopping early.\n",
                                                            "Saved SFT model to c:\\Users\\lvbab\\Documents\\GitHub\\LLM-Agent-Core_Concept_Code\\models\\ch8_sft_gpt2\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# SFT Training Loop\n",
                                        "print(\"\\n\" + \"=\"*60)\n",
                                        "print(\"Starting SFT Training\")\n",
                                        "print(\"=\"*60)\n",
                                        "\n",
                                        "SAVE_SFT_MODEL = True  # Set False to skip saving\n",
                                        "\n",
                                        "def evaluate_loss(dataloader):\n",
                                        "    sft_model.eval()\n",
                                        "    total_loss = 0\n",
                                        "    total_batches = 0\n",
                                        "    with torch.no_grad():\n",
                                        "        for batch in dataloader:\n",
                                        "            input_ids = batch['input_ids'].to(device)\n",
                                        "            attention_mask = batch['attention_mask'].to(device)\n",
                                        "            labels = batch['labels'].to(device)\n",
                                        "\n",
                                        "            outputs = sft_model(\n",
                                        "                input_ids=input_ids,\n",
                                        "                attention_mask=attention_mask,\n",
                                        "                labels=labels\n",
                                        "            )\n",
                                        "            total_loss += outputs.loss.item()\n",
                                        "            total_batches += 1\n",
                                        "\n",
                                        "    return total_loss / max(total_batches, 1)\n",
                                        "\n",
                                        "sft_model.train()\n",
                                        "training_losses = []\n",
                                        "step = 0\n",
                                        "stop_training = False\n",
                                        "\n",
                                        "for epoch in range(NUM_EPOCHS):\n",
                                        "    epoch_loss = 0\n",
                                        "    num_batches = 0\n",
                                        "\n",
                                        "    sft_model.train()\n",
                                        "    for batch in train_dataloader:\n",
                                        "        # Move to device\n",
                                        "        input_ids = batch['input_ids'].to(device)\n",
                                        "        attention_mask = batch['attention_mask'].to(device)\n",
                                        "        labels = batch['labels'].to(device)\n",
                                        "\n",
                                        "        # Forward pass\n",
                                        "        outputs = sft_model(\n",
                                        "            input_ids=input_ids,\n",
                                        "            attention_mask=attention_mask,\n",
                                        "            labels=labels\n",
                                        "        )\n",
                                        "        loss = outputs.loss\n",
                                        "\n",
                                        "        # Backward pass\n",
                                        "        optimizer.zero_grad()\n",
                                        "        loss.backward()\n",
                                        "        torch.nn.utils.clip_grad_norm_(sft_model.parameters(), 1.0)\n",
                                        "        optimizer.step()\n",
                                        "        scheduler.step()\n",
                                        "\n",
                                        "        epoch_loss += loss.item()\n",
                                        "        num_batches += 1\n",
                                        "        step += 1\n",
                                        "\n",
                                        "        if step % 20 == 0:\n",
                                        "            training_losses.append(loss.item())\n",
                                        "            print(f\"Step {step:4d} | Loss: {loss.item():.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
                                        "\n",
                                        "        if MAX_STEPS is not None and step >= MAX_STEPS:\n",
                                        "            stop_training = True\n",
                                        "            break\n",
                                        "\n",
                                        "    avg_epoch_loss = epoch_loss / max(num_batches, 1)\n",
                                        "    val_loss = evaluate_loss(val_dataloader)\n",
                                        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} completed\")\n",
                                        "    print(f\"  Train loss: {avg_epoch_loss:.4f}\")\n",
                                        "    print(f\"  Val loss:   {val_loss:.4f}\")\n",
                                        "\n",
                                        "    if stop_training:\n",
                                        "        print(f\"Reached MAX_STEPS={MAX_STEPS}, stopping early.\")\n",
                                        "        break\n",
                                        "\n",
                                        "if SAVE_SFT_MODEL:\n",
                                        "    SFT_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
                                        "    sft_model.save_pretrained(SFT_SAVE_DIR)\n",
                                        "    sft_tokenizer.save_pretrained(SFT_SAVE_DIR)\n",
                                        "\n",
                                        "    meta = {\n",
                                        "        \"task\": SFT_TASK,\n",
                                        "        \"use_plain_extract\": USE_PLAIN_EXTRACT,\n",
                                        "        \"labels\": TASK_LABELS,\n",
                                        "        \"model_name\": model_name,\n",
                                        "    }\n",
                                        "    with open(SFT_SAVE_DIR / \"sft_task.json\", \"w\", encoding=\"utf-8\") as f:\n",
                                        "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
                                        "\n",
                                        "    print(f\"Saved SFT model to {SFT_SAVE_DIR}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 13,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "data": {
                                                            "image/png": "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",
                                                            "text/plain": [
                                                                      "<Figure size 1000x400 with 1 Axes>"
                                                            ]
                                                  },
                                                  "metadata": {},
                                                  "output_type": "display_data"
                                        }
                              ],
                              "source": [
                                        "# Visualize training progress\n",
                                        "plt.figure(figsize=(10, 4))\n",
                                        "plt.plot(training_losses, 'b-', linewidth=2)\n",
                                        "plt.xlabel('Step (x20)')\n",
                                        "plt.ylabel('Loss')\n",
                                        "plt.title('SFT Training Loss')\n",
                                        "plt.grid(True, alpha=0.3)\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## Part 6：评估微调后的模型\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 14,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import re\n",
                                        "\n",
                                        "def generate_response(model, tokenizer, user_message: str, max_new_tokens: int = 100, temperature: float = 0.7, do_sample: bool = True, top_p: float = 0.9) -> str:\n",
                                        "    \"\"\"\n",
                                        "    Generate a response using the fine-tuned model.\n",
                                        "    \"\"\"\n",
                                        "    model.eval()\n",
                                        "\n",
                                        "    # Format as conversation\n",
                                        "    prompt = f\"<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                                        "\n",
                                        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                                        "\n",
                                        "    if tokenizer.pad_token_id is None:\n",
                                        "        tokenizer.pad_token = tokenizer.eos_token\n",
                                        "    if model.config.pad_token_id is None:\n",
                                        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
                                        "    if model.generation_config.pad_token_id is None:\n",
                                        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
                                        "\n",
                                        "    eos_id = tokenizer.convert_tokens_to_ids('<|im_end|>')\n",
                                        "    if eos_id is None or eos_id == tokenizer.unk_token_id:\n",
                                        "        eos_id = tokenizer.eos_token_id\n",
                                        "\n",
                                        "    gen_kwargs = {\n",
                                        "        \"max_new_tokens\": max_new_tokens,\n",
                                        "        \"do_sample\": do_sample,\n",
                                        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
                                        "        \"eos_token_id\": eos_id,\n",
                                        "        \"repetition_penalty\": 1.1,\n",
                                        "        \"no_repeat_ngram_size\": 3,\n",
                                        "    }\n",
                                        "    if do_sample:\n",
                                        "        gen_kwargs[\"temperature\"] = temperature\n",
                                        "        gen_kwargs[\"top_p\"] = top_p\n",
                                        "\n",
                                        "    input_len = inputs['input_ids'].shape[-1]\n",
                                        "    with torch.no_grad():\n",
                                        "        outputs = model.generate(\n",
                                        "            **inputs,\n",
                                        "            **gen_kwargs,\n",
                                        "        )\n",
                                        "\n",
                                        "    gen_ids = outputs[0][input_len:]\n",
                                        "    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=False)\n",
                                        "    gen_text = gen_text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
                                        "    if \"<|im_end|>\" in gen_text:\n",
                                        "        gen_text = gen_text.split(\"<|im_end|>\")[0]\n",
                                        "    return gen_text.strip()\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 15,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Evaluation on held-out test set\n",
                                                            "============================================================\n",
                                                            "  evaluated 50/200\n",
                                                            "  evaluated 100/200\n",
                                                            "  evaluated 150/200\n",
                                                            "  evaluated 200/200\n",
                                                            "  evaluated 50/200\n",
                                                            "  evaluated 100/200\n",
                                                            "  evaluated 150/200\n",
                                                            "  evaluated 200/200\n",
                                                            "\n",
                                                            "Base model:\n",
                                                            "  诉求分类: 91.5%\n",
                                                            "  overall: 91.5%\n",
                                                            "\n",
                                                            "SFT model:\n",
                                                            "  诉求分类: 100.0%\n",
                                                            "  overall: 100.0%\n",
                                                            "\n",
                                                            "Samples (SFT):\n",
                                                            "  Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：客户吴磊（wu.lei@example.com）反馈订单OD20240313-4689地址修改。\n",
                                                            "  Expected: 地址修改\n",
                                                            "  Pred: 地址修改\n",
                                                            "  ---\n",
                                                            "  Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：周凯反馈订单OD20241001-2641退款申请，联系邮箱zhou.kai@example.com。\n",
                                                            "  Expected: 退款申请\n",
                                                            "  Pred: 退款申请\n",
                                                            "  ---\n",
                                                            "  Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：用户马超邮箱ma.chao@example.com，订单OD20251221-7200需要处理：售后咨询。\n",
                                                            "  Expected: 售后咨询\n",
                                                            "  Pred: 售后咨询\n",
                                                            "  ---\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Quantitative evaluation on held-out test set\n",
                                        "import re\n",
                                        "import json\n",
                                        "\n",
                                        "def clean_generated(text):\n",
                                        "    if text is None:\n",
                                        "        return \"\"\n",
                                        "    cleaned = text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\")\n",
                                        "    for token in [\"<|im_start|>\", \"<|im_end|>\", \"<|assistant|>\", \"<|user|>\", \"<|system|>\"]:\n",
                                        "        cleaned = cleaned.replace(token, \"\")\n",
                                        "    return cleaned.strip()\n",
                                        "\n",
                                        "def normalize_text(text):\n",
                                        "    text = clean_generated(text)\n",
                                        "    return \" \".join(text.strip().lower().split())\n",
                                        "\n",
                                        "def extract_number(text):\n",
                                        "    match = re.search(r\"-?\\d+\", text)\n",
                                        "    return match.group(0) if match else None\n",
                                        "\n",
                                        "def extract_json(text):\n",
                                        "    text = clean_generated(text)\n",
                                        "    matches = re.findall(r\"\\{.*?\\}\", text, flags=re.DOTALL)\n",
                                        "    if not matches:\n",
                                        "        return None\n",
                                        "    for candidate in reversed(matches):\n",
                                        "        try:\n",
                                        "            return json.loads(candidate)\n",
                                        "        except Exception:\n",
                                        "            continue\n",
                                        "    return None\n",
                                        "\n",
                                        "def _to_ascii_digits(text: str) -> str:\n",
                                        "    return text.translate(str.maketrans(\"０１２３４５６７８９\", \"0123456789\"))\n",
                                        "\n",
                                        "def _normalize_json_key(key: str) -> str:\n",
                                        "    key = key.replace(\"##\", \"\")\n",
                                        "    key = re.sub(r\"\\s+\", \"\", key)\n",
                                        "    key = re.sub(r\"_+\", \"_\", key)\n",
                                        "    if key == \"orderid\":\n",
                                        "        key = \"order_id\"\n",
                                        "    return _to_ascii_digits(key)\n",
                                        "\n",
                                        "def _normalize_json_value(val):\n",
                                        "    if isinstance(val, str):\n",
                                        "        val = val.replace(\"##\", \"\")\n",
                                        "        val = re.sub(r\"\\s+\", \"\", val)\n",
                                        "        return _to_ascii_digits(val)\n",
                                        "    return val\n",
                                        "\n",
                                        "def normalize_json_obj(obj):\n",
                                        "    if not isinstance(obj, dict):\n",
                                        "        return obj\n",
                                        "    normalized = {}\n",
                                        "    for k, v in obj.items():\n",
                                        "        nk = _normalize_json_key(k)\n",
                                        "        nv = _normalize_json_value(v)\n",
                                        "        normalized[nk] = nv\n",
                                        "    return normalized\n",
                                        "\n",
                                        "USE_PLAIN_EVAL = USE_PLAIN_EXTRACT\n",
                                        "\n",
                                        "def _normalize_text_for_eval(text: str) -> str:\n",
                                        "    text = clean_generated(text)\n",
                                        "    text = text.replace(\"##\", \"\")\n",
                                        "    text = _to_ascii_digits(text)\n",
                                        "    return re.sub(r\"\\s+\", \"\", text)\n",
                                        "\n",
                                        "def parse_plain_fields(text: str):\n",
                                        "    text = _normalize_text_for_eval(text)\n",
                                        "    name = None\n",
                                        "    email = None\n",
                                        "    order_id = None\n",
                                        "\n",
                                        "    m = re.search(r\"姓名[:：]?([^；;，,。\\n]+)\", text)\n",
                                        "    if m:\n",
                                        "        name = m.group(1)\n",
                                        "\n",
                                        "    m = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)\n",
                                        "    if m:\n",
                                        "        email = m.group(0)\n",
                                        "\n",
                                        "    m = re.search(r\"OD\\d{8}-\\d{4}\", text)\n",
                                        "    if m:\n",
                                        "        order_id = m.group(0)\n",
                                        "\n",
                                        "    if not (name and email and order_id):\n",
                                        "        return None\n",
                                        "    return {\"name\": name, \"email\": email, \"order_id\": order_id}\n",
                                        "\n",
                                        "def score_prediction(pred, record):\n",
                                        "    pred = clean_generated(pred)\n",
                                        "    pred_norm = normalize_text(pred)\n",
                                        "    expected = record[\"expected\"]\n",
                                        "    expected_norm = normalize_text(expected)\n",
                                        "    metric = record[\"metric\"]\n",
                                        "\n",
                                        "    if metric == \"numeric\":\n",
                                        "        return extract_number(pred_norm) == expected\n",
                                        "\n",
                                        "    if metric == \"json\":\n",
                                        "        if USE_PLAIN_EVAL:\n",
                                        "            pred_obj = parse_plain_fields(pred)\n",
                                        "            try:\n",
                                        "                expected_obj = json.loads(expected)\n",
                                        "            except Exception:\n",
                                        "                expected_obj = None\n",
                                        "            if pred_obj is None or expected_obj is None:\n",
                                        "                return False\n",
                                        "            return normalize_json_obj(pred_obj) == normalize_json_obj(expected_obj)\n",
                                        "\n",
                                        "        pred_obj = extract_json(pred)\n",
                                        "        try:\n",
                                        "            expected_obj = json.loads(expected)\n",
                                        "        except Exception:\n",
                                        "            expected_obj = None\n",
                                        "        if pred_obj is None or expected_obj is None:\n",
                                        "            return False\n",
                                        "        return normalize_json_obj(pred_obj) == normalize_json_obj(expected_obj)\n",
                                        "\n",
                                        "    return expected_norm in pred_norm\n",
                                        "\n",
                                        "EVAL_MAX_SAMPLES = 200  # Set None for full test set\n",
                                        "EVAL_PROGRESS_EVERY = 50\n",
                                        "EVAL_MAX_NEW_TOKENS = 120\n",
                                        "EVAL_DO_SAMPLE = False\n",
                                        "EVAL_TEMPERATURE = 0.0\n",
                                        "EVAL_TOP_P = 1.0\n",
                                        "EVAL_RUN_BASE = True\n",
                                        "USE_LM_CLASSIFIER = TASK_LABELS is not None\n",
                                        "\n",
                                        "def _build_prompt(instruction, use_chatml):\n",
                                        "    if use_chatml:\n",
                                        "        return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                                        "    return instruction\n",
                                        "\n",
                                        "def _lm_classify(model, tok, prompt, labels):\n",
                                        "    prompt_ids = tok(prompt, add_special_tokens=False)[\"input_ids\"]\n",
                                        "    if not prompt_ids:\n",
                                        "        return labels[0]\n",
                                        "    best_label = labels[0]\n",
                                        "    best_score = float(\"-inf\")\n",
                                        "    for label in labels:\n",
                                        "        label_ids = tok(label, add_special_tokens=False)[\"input_ids\"]\n",
                                        "        input_ids = torch.tensor([prompt_ids + label_ids]).to(device)\n",
                                        "        with torch.no_grad():\n",
                                        "            logits = model(input_ids).logits\n",
                                        "        start = len(prompt_ids)\n",
                                        "        if start == 0 or len(label_ids) == 0:\n",
                                        "            continue\n",
                                        "        log_probs = torch.log_softmax(logits[0, start - 1:start - 1 + len(label_ids)], dim=-1)\n",
                                        "        target = input_ids[0, start:start + len(label_ids)]\n",
                                        "        token_logprobs = log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
                                        "        score = token_logprobs.mean().item()\n",
                                        "        if score > best_score:\n",
                                        "            best_score = score\n",
                                        "            best_label = label\n",
                                        "    return best_label\n",
                                        "\n",
                                        "def run_model(model, instruction, use_chatml):\n",
                                        "    if USE_LM_CLASSIFIER and TASK_LABELS:\n",
                                        "        prompt = _build_prompt(instruction, use_chatml)\n",
                                        "        tok = sft_tokenizer if use_chatml else tokenizer\n",
                                        "        return _lm_classify(model, tok, prompt, TASK_LABELS)\n",
                                        "    if use_chatml:\n",
                                        "        return generate_response(\n",
                                        "            model,\n",
                                        "            sft_tokenizer,\n",
                                        "            instruction,\n",
                                        "            max_new_tokens=EVAL_MAX_NEW_TOKENS,\n",
                                        "            temperature=EVAL_TEMPERATURE,\n",
                                        "            do_sample=EVAL_DO_SAMPLE,\n",
                                        "            top_p=EVAL_TOP_P\n",
                                        "        )\n",
                                        "    return generate_text(\n",
                                        "        model,\n",
                                        "        tokenizer,\n",
                                        "        instruction,\n",
                                        "        max_new_tokens=EVAL_MAX_NEW_TOKENS,\n",
                                        "        temperature=EVAL_TEMPERATURE,\n",
                                        "        do_sample=EVAL_DO_SAMPLE,\n",
                                        "        top_p=EVAL_TOP_P\n",
                                        "    )\n",
                                        "\n",
                                        "def evaluate_model(model, use_chatml):\n",
                                        "    results = {}\n",
                                        "    samples = []\n",
                                        "    records = TEST_RECORDS if EVAL_MAX_SAMPLES is None else TEST_RECORDS[:EVAL_MAX_SAMPLES]\n",
                                        "    total = len(records)\n",
                                        "    for idx, record in enumerate(records, 1):\n",
                                        "        pred = run_model(model, record[\"instruction\"], use_chatml)\n",
                                        "        correct = score_prediction(pred, record)\n",
                                        "        results.setdefault(record[\"category\"], []).append(correct)\n",
                                        "        if len(samples) < 3:\n",
                                        "            samples.append((record[\"instruction\"], record[\"expected\"], pred))\n",
                                        "        if EVAL_PROGRESS_EVERY and (idx % EVAL_PROGRESS_EVERY == 0 or idx == total):\n",
                                        "            print(f\"  evaluated {idx}/{total}\")\n",
                                        "\n",
                                        "    summary = {cat: sum(vals) / len(vals) for cat, vals in results.items()}\n",
                                        "    summary[\"overall\"] = sum(sum(v) for v in results.values()) / sum(len(v) for v in results.values())\n",
                                        "    return summary, samples\n",
                                        "\n",
                                        "print(\"Evaluation on held-out test set\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "base_summary = {}\n",
                                        "base_samples = []\n",
                                        "if EVAL_RUN_BASE:\n",
                                        "    base_summary, base_samples = evaluate_model(base_model, use_chatml=False)\n",
                                        "sft_summary, sft_samples = evaluate_model(sft_model, use_chatml=True)\n",
                                        "\n",
                                        "if EVAL_RUN_BASE:\n",
                                        "    print(\"\\nBase model:\")\n",
                                        "    for cat, acc in base_summary.items():\n",
                                        "        print(f\"  {cat}: {acc*100:.1f}%\" if cat != \"overall\" else f\"  overall: {acc*100:.1f}%\")\n",
                                        "\n",
                                        "print(\"\\nSFT model:\")\n",
                                        "for cat, acc in sft_summary.items():\n",
                                        "    print(f\"  {cat}: {acc*100:.1f}%\" if cat != \"overall\" else f\"  overall: {acc*100:.1f}%\")\n",
                                        "\n",
                                        "print(\"\\nSamples (SFT):\")\n",
                                        "for ins, exp, pred in sft_samples:\n",
                                        "    print(f\"  Q: {ins}\")\n",
                                        "    print(f\"  Expected: {exp}\")\n",
                                        "    print(f\"  Pred: {pred[:80]}\")\n",
                                        "    print(\"  ---\")\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 16,
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "\n",
                                                            "Comparison: Base vs SFT\n",
                                                            "============================================================\n",
                                                            "\n",
                                                            "Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：客户吴磊（wu.lei@example.com）反馈订单OD20240313-4689地址修改。\n",
                                                            "Expected: 地址修改\n",
                                                            "Base: 地址修改\n",
                                                            "SFT:  地址修改\n",
                                                            "----------------------------------------\n",
                                                            "\n",
                                                            "Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：周凯反馈订单OD20241001-2641退款申请，联系邮箱zhou.kai@example.com。\n",
                                                            "Expected: 退款申请\n",
                                                            "Base: 退款申请\n",
                                                            "SFT:  退款申请\n",
                                                            "----------------------------------------\n",
                                                            "\n",
                                                            "Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：用户马超邮箱ma.chao@example.com，订单OD20251221-7200需要处理：售后咨询。\n",
                                                            "Expected: 售后咨询\n",
                                                            "Base: 售后咨询\n",
                                                            "SFT:  售后咨询\n",
                                                            "----------------------------------------\n",
                                                            "\n",
                                                            "Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：马超反馈订单OD20250603-6551地址修改，联系邮箱ma.chao@example.com。\n",
                                                            "Expected: 地址修改\n",
                                                            "Base: 延迟发货\n",
                                                            "SFT:  地址修改\n",
                                                            "----------------------------------------\n",
                                                            "\n",
                                                            "Q: 请判断用户诉求类别，只输出类别（延迟发货/退款申请/地址修改/物流异常/售后咨询/发票问题）。文本：请处理：姓名张强，邮箱zhang.qiang@example.com，订单号OD20240903-2550，原因：地址修改。\n",
                                                            "Expected: 地址修改\n",
                                                            "Base: 延迟发货\n",
                                                            "SFT:  地址修改\n",
                                                            "----------------------------------------\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# Side-by-side comparison on a few test items\n",
                                        "print(\"\\nComparison: Base vs SFT\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "for record in TEST_RECORDS[:5]:\n",
                                        "    instruction = record[\"instruction\"]\n",
                                        "    expected = record[\"expected\"]\n",
                                        "    base_pred = run_model(base_model, instruction, use_chatml=False)\n",
                                        "    sft_pred = run_model(sft_model, instruction, use_chatml=True)\n",
                                        "\n",
                                        "    print(f\"\\nQ: {instruction}\")\n",
                                        "    print(f\"Expected: {expected}\")\n",
                                        "    print(f\"Base: {base_pred[:80]}\")\n",
                                        "    print(f\"SFT:  {sft_pred[:80]}\")\n",
                                        "    print(\"-\" * 40)\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 总结\n",
                                        "\n",
                                        "1. **SFT 的作用**：将只会文本续写的基础模型训练为能遵循指令、进行对话的模型\n",
                                        "\n",
                                        "2. **ChatML 格式**：用角色标记（`<|im_start|>`, `<|im_end|>`）表示对话的标准方式\n",
                                        "\n",
                                        "3. **Loss Masking**：关键技术——只在 assistant 回复上计算 loss（在 labels 中用 -100 屏蔽）\n",
                                        "\n",
                                        "4. **真实训练**：我们实际微调了 GPT-2，并看到它学会以对话形式回答问题\n",
                                        "\n",
                                        "### 关键要点\n",
                                        "\n",
                                        "| 维度 | 说明 |\n",
                                        "|--------|--------|\n",
                                        "| **数据** | 高质量的（instruction, response）样本对 |\n",
                                        "| **格式** | ChatML 或类似的结构化格式 |\n",
                                        "| **Loss** | 仅对 assistant tokens 计算（-100 mask） |\n",
                                        "| **结果** | 基础模型 → 对话模型 |\n",
                                        "\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "metadata": {},
                              "source": [
                                        "## 练习\n",
                                        "\n",
                                        "1. **扩充数据集**：加入更多样的指令类型（翻译、摘要等）\n",
                                        "2. **多轮训练**：确保模型能处理多轮对话\n",
                                        "3. **评估指标**：实现 ROUGE 或其他指标评估回复质量\n",
                                        "4. **超参数调优**：尝试学习率、batch size、epochs 等\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 17,
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Exercise space\n",
                                        "\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 4
}
