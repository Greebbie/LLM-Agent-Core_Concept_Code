{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "027f20c8",
                              "metadata": {},
                              "source": [
                                        "# Bonus A | RLHF å…¨æ™¯ï¼šPPO ä¸ Reward Model\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "**ç›®æ ‡ï¼š** æ·±å…¥ç†è§£ RLHF çš„å®Œæ•´æµç¨‹\n",
                                        "\n",
                                        "**è¿™æ˜¯ ChatGPT çš„èµ·æºæŠ€æœ¯ï¼**\n",
                                        "\n",
                                        "---\n",
                                        "\n",
                                        "## æœ¬ç« å†…å®¹\n",
                                        "\n",
                                        "1. **RLHF ä¸‰é˜¶æ®µ**ï¼šSFT â†’ Reward Model â†’ PPO\n",
                                        "2. **Reward Model**ï¼šè®­ç»ƒä¸€ä¸ªæ‰“åˆ†æ¨¡å‹\n",
                                        "3. **PPO åŸºç¡€**ï¼šç­–ç•¥æ¢¯åº¦ä¼˜åŒ–\n",
                                        "4. **ä¸ºä»€ä¹ˆ DPO æ›´æµè¡Œ**ï¼šRLHF çš„é—®é¢˜"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "prereq_bonus_a_001",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## ğŸ“ å‰ç½®çŸ¥è¯†ï¼šæ·±å…¥ RLHF\n",
                                        "\n",
                                        "### ä»€ä¹ˆæ˜¯ RLHFï¼Ÿ\n",
                                        "\n",
                                        "**RLHF (Reinforcement Learning from Human Feedback)**\n",
                                        "\n",
                                        "= ç”¨äººç±»åé¦ˆæ¥è®­ç»ƒæ¨¡å‹\n",
                                        "\n",
                                        "è¿™æ˜¯ ChatGPT æˆåŠŸçš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ï¼\n",
                                        "\n",
                                        "### RLHF çš„ä¸‰ä¸ªé˜¶æ®µ\n",
                                        "\n",
                                        "```\n",
                                        "é˜¶æ®µ1: SFT (ç›‘ç£å¾®è°ƒ)\n",
                                        "       ç”¨å¯¹è¯æ•°æ®æ•™æ¨¡å‹\"æ€ä¹ˆå¯¹è¯\"\n",
                                        "           â†“\n",
                                        "é˜¶æ®µ2: è®­ç»ƒ Reward Model\n",
                                        "       ç”¨äººç±»åå¥½æ•°æ®è®­ç»ƒä¸€ä¸ª\"è¯„åˆ†å™¨\"\n",
                                        "           â†“\n",
                                        "é˜¶æ®µ3: PPO å¼ºåŒ–å­¦ä¹ \n",
                                        "       ç”¨ Reward Model çš„åˆ†æ•°æ¥ä¼˜åŒ–æ¨¡å‹\n",
                                        "```\n",
                                        "\n",
                                        "### ä¸ºä»€ä¹ˆéœ€è¦å¼ºåŒ–å­¦ä¹ ï¼Ÿ\n",
                                        "\n",
                                        "SFT çš„é—®é¢˜ï¼š\n",
                                        "- åªèƒ½æ¨¡ä»¿è®­ç»ƒæ•°æ®\n",
                                        "- ä¸èƒ½è¶…è¶Šè®­ç»ƒæ•°æ®çš„è´¨é‡\n",
                                        "\n",
                                        "å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ï¼š\n",
                                        "- å¯ä»¥æ¢ç´¢æ–°çš„å›ç­”æ–¹å¼\n",
                                        "- å¯ä»¥æ ¹æ®åé¦ˆæŒç»­æ”¹è¿›\n",
                                        "\n",
                                        "### ä¸ DPO çš„å…³ç³»\n",
                                        "\n",
                                        "- **RLHF**ï¼šå®Œæ•´æ–¹æ¡ˆï¼Œéœ€è¦è®­ç»ƒ Reward Model + PPO\n",
                                        "- **DPO**ï¼šç®€åŒ–æ–¹æ¡ˆï¼Œç›´æ¥ç”¨åå¥½æ•°æ®ä¼˜åŒ–\n",
                                        "\n",
                                        "æœ¬ç« æ·±å…¥è®²è§£ RLHF çš„æ•°å­¦åŸç†ï¼Œå¸®åŠ©ä½ æ›´å¥½ç†è§£ DPO ä¸ºä»€ä¹ˆæœ‰æ•ˆã€‚\n",
                                        "\n",
                                        "### æœ¬ç« ç›®æ ‡\n",
                                        "\n",
                                        "- ç†è§£ Reward Model çš„è®­ç»ƒæ–¹æ³•\n",
                                        "- å­¦ä¹  PPO ç®—æ³•çš„åŸºæœ¬åŸç†\n",
                                        "- äº†è§£ RLHF çš„å·¥ç¨‹æŒ‘æˆ˜\n",
                                        "- å¯¹æ¯” RLHF å’Œ DPO çš„ä¼˜ç¼ºç‚¹"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "d2355d06",
                              "metadata": {},
                              "source": [
                                        "## 0. ç¯å¢ƒå‡†å¤‡"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "f5515ab7",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "import torch\n",
                                        "import torch.nn as nn\n",
                                        "import torch.nn.functional as F\n",
                                        "import numpy as np\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "\n",
                                        "torch.manual_seed(42)\n",
                                        "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "334d31bb",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 1. RLHF ä¸‰é˜¶æ®µæ€»è§ˆ\n",
                                        "\n",
                                        "### å®Œæ•´æµç¨‹\n",
                                        "\n",
                                        "```\n",
                                        "é˜¶æ®µ 1: SFTï¼ˆSupervised Fine-Tuningï¼‰\n",
                                        "        â†“ å¾—åˆ°åˆå§‹ç­–ç•¥æ¨¡å‹\n",
                                        "é˜¶æ®µ 2: å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼ˆReward Model Trainingï¼‰\n",
                                        "        â†“ è®­ç»ƒæ‰“åˆ†æ¨¡å‹\n",
                                        "é˜¶æ®µ 3: PPOï¼ˆProximal Policy Optimizationï¼‰\n",
                                        "        â†“ ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥\n",
                                        "æœ€ç»ˆ: å¯¹é½åæ¨¡å‹ï¼ˆAligned Modelï¼‰\n",
                                        "```\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "f19d8b32",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ– RLHF æµç¨‹\n",
                                        "fig, ax = plt.subplots(figsize=(14, 8))\n",
                                        "ax.set_xlim(0, 14)\n",
                                        "ax.set_ylim(0, 10)\n",
                                        "ax.axis('off')\n",
                                        "\n",
                                        "import matplotlib.patches as patches\n",
                                        "\n",
                                        "# ä¸‰ä¸ªé˜¶æ®µ\n",
                                        "stages = [\n",
                                        "    (1, 7, 3, 2, \"Stage 1: SFT\\n\\nFine-tune on\\ndemonstration data\", \"lightblue\"),\n",
                                        "    (5.5, 7, 3, 2, \"Stage 2: RM\\n\\nTrain reward model\\non preferences\", \"lightyellow\"),\n",
                                        "    (10, 7, 3, 2, \"Stage 3: PPO\\n\\nOptimize policy\\nwith RL\", \"lightcoral\"),\n",
                                        "]\n",
                                        "\n",
                                        "for x, y, w, h, text, color in stages:\n",
                                        "    rect = patches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.1\",\n",
                                        "                                   facecolor=color, edgecolor='black', linewidth=2)\n",
                                        "    ax.add_patch(rect)\n",
                                        "    ax.text(x + w/2, y + h/2, text, ha='center', va='center', fontsize=9)\n",
                                        "\n",
                                        "# ç®­å¤´\n",
                                        "ax.annotate('', xy=(5.5, 8), xytext=(4, 8),\n",
                                        "            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
                                        "ax.annotate('', xy=(10, 8), xytext=(8.5, 8),\n",
                                        "            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
                                        "\n",
                                        "# è¾“å…¥æ•°æ®\n",
                                        "inputs = [\n",
                                        "    (1.5, 4.5, \"Demonstration\\nData\"),\n",
                                        "    (6.5, 4.5, \"Preference\\nData\"),\n",
                                        "    (11, 4.5, \"Prompts\"),\n",
                                        "]\n",
                                        "\n",
                                        "for x, y, text in inputs:\n",
                                        "    ax.text(x, y, text, ha='center', va='center', fontsize=9,\n",
                                        "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
                                        "    ax.annotate('', xy=(x, 6.8), xytext=(x, 5.2),\n",
                                        "                arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
                                        "\n",
                                        "# æœ€ç»ˆæ¨¡å‹\n",
                                        "rect = patches.FancyBboxPatch((5.5, 1.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
                                        "                               facecolor='lightgreen', edgecolor='black', linewidth=2)\n",
                                        "ax.add_patch(rect)\n",
                                        "ax.text(7, 2.25, \"Aligned Model\", ha='center', va='center', fontsize=11, fontweight='bold')\n",
                                        "ax.annotate('', xy=(7, 3), xytext=(11.5, 6.8),\n",
                                        "            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
                                        "\n",
                                        "plt.title('RLHF: Reinforcement Learning from Human Feedback', fontsize=14, fontweight='bold')\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "9b22b57e",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 2. Reward Modelï¼šè®­ç»ƒæ‰“åˆ†æ¨¡å‹\n",
                                        "\n",
                                        "### ä»€ä¹ˆæ˜¯ Reward Modelï¼Ÿ\n",
                                        "\n",
                                        "ç»™å®š (prompt, response)ï¼Œè¾“å‡ºä¸€ä¸ªåˆ†æ•°ï¼Œè¡¨ç¤ºå›ç­”çš„\"å¥½å\"ã€‚\n",
                                        "\n",
                                        "### è®­ç»ƒæ•°æ®\n",
                                        "\n",
                                        "```\n",
                                        "Prompt: \"å¦‚ä½•å­¦ä¹  Python?\"\n",
                                        "Response A: \"é˜…è¯»æ–‡æ¡£\" (è¯„åˆ†: 3)\n",
                                        "Response B: \"ä»åŸºç¡€å¼€å§‹ï¼Œæ¯å¤©ç»ƒä¹ ...\" (è¯„åˆ†: 8)\n",
                                        "```\n",
                                        "\n",
                                        "### è®­ç»ƒç›®æ ‡\n",
                                        "\n",
                                        "è®© RM(A) < RM(B) å½“ B æ¯” A æ›´å¥½æ—¶\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "d76097d6",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class RewardModel(nn.Module):\n",
                                        "    \"\"\"\n",
                                        "    ç®€å•çš„ Reward Model\n",
                                        "    \"\"\"\n",
                                        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
                                        "        super().__init__()\n",
                                        "        \n",
                                        "        # ç®€åŒ–ï¼šç”¨ä¸€ä¸ªå°å‹ Transformer\n",
                                        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
                                        "        self.transformer = nn.TransformerEncoder(\n",
                                        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, batch_first=True),\n",
                                        "            num_layers=2\n",
                                        "        )\n",
                                        "        \n",
                                        "        # è¾“å‡ºä¸€ä¸ªæ ‡é‡åˆ†æ•°\n",
                                        "        self.score_head = nn.Linear(embed_dim, 1)\n",
                                        "    \n",
                                        "    def forward(self, input_ids):\n",
                                        "        # input_ids: [batch, seq_len]\n",
                                        "        x = self.embedding(input_ids)  # [batch, seq_len, embed_dim]\n",
                                        "        x = self.transformer(x)         # [batch, seq_len, embed_dim]\n",
                                        "        \n",
                                        "        # å–æœ€åä¸€ä¸ª token çš„è¡¨ç¤º\n",
                                        "        x = x[:, -1, :]                 # [batch, embed_dim]\n",
                                        "        \n",
                                        "        # è¾“å‡ºåˆ†æ•°\n",
                                        "        score = self.score_head(x)      # [batch, 1]\n",
                                        "        return score.squeeze(-1)\n",
                                        "\n",
                                        "# åˆ›å»ºæ¨¡å‹\n",
                                        "rm = RewardModel(vocab_size=1000)\n",
                                        "print(f\"Reward Model å‚æ•°é‡: {sum(p.numel() for p in rm.parameters()):,}\")\n",
                                        "\n",
                                        "# æµ‹è¯•\n",
                                        "input_ids = torch.randint(0, 1000, (2, 50))\n",
                                        "scores = rm(input_ids)\n",
                                        "print(f\"è¾“å…¥å½¢çŠ¶: {input_ids.shape}\")\n",
                                        "print(f\"è¾“å‡ºåˆ†æ•°: {scores}\")"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "32260ff6",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def reward_model_loss(scores_chosen, scores_rejected):\n",
                                        "    \"\"\"\n",
                                        "    Reward Model çš„æŸå¤±å‡½æ•°\n",
                                        "    \n",
                                        "    ç›®æ ‡ï¼šè®© chosen çš„åˆ†æ•°é«˜äº rejected\n",
                                        "    \"\"\"\n",
                                        "    # Bradley-Terry loss\n",
                                        "    loss = -F.logsigmoid(scores_chosen - scores_rejected).mean()\n",
                                        "    return loss\n",
                                        "\n",
                                        "# æ¨¡æ‹Ÿè®­ç»ƒ\n",
                                        "rm = RewardModel(vocab_size=1000)\n",
                                        "optimizer = torch.optim.Adam(rm.parameters(), lr=1e-4)\n",
                                        "\n",
                                        "print(\"æ¨¡æ‹Ÿ Reward Model è®­ç»ƒ...\")\n",
                                        "losses = []\n",
                                        "\n",
                                        "for epoch in range(100):\n",
                                        "    # æ¨¡æ‹Ÿæ•°æ®\n",
                                        "    chosen = torch.randint(0, 1000, (8, 50))\n",
                                        "    rejected = torch.randint(0, 1000, (8, 50))\n",
                                        "    \n",
                                        "    scores_chosen = rm(chosen)\n",
                                        "    scores_rejected = rm(rejected)\n",
                                        "    \n",
                                        "    loss = reward_model_loss(scores_chosen, scores_rejected)\n",
                                        "    \n",
                                        "    optimizer.zero_grad()\n",
                                        "    loss.backward()\n",
                                        "    optimizer.step()\n",
                                        "    \n",
                                        "    losses.append(loss.item())\n",
                                        "\n",
                                        "plt.figure(figsize=(10, 4))\n",
                                        "plt.plot(losses)\n",
                                        "plt.xlabel('Epoch')\n",
                                        "plt.ylabel('Loss')\n",
                                        "plt.title('Reward Model Training')\n",
                                        "plt.grid(True, alpha=0.3)\n",
                                        "plt.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "7b251873",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 3. PPOï¼šç­–ç•¥æ¢¯åº¦ä¼˜åŒ–\n",
                                        "\n",
                                        "### PPO çš„æ ¸å¿ƒæ€æƒ³\n",
                                        "\n",
                                        "1. **ç­–ç•¥æ¢¯åº¦**ï¼šå¢åŠ é«˜å¥–åŠ±åŠ¨ä½œçš„æ¦‚ç‡\n",
                                        "2. **Clipping**ï¼šé™åˆ¶æ›´æ–°å¹…åº¦ï¼Œä¿æŒç¨³å®š\n",
                                        "3. **KL æƒ©ç½š**ï¼šä¸è¦åç¦»åŸå§‹æ¨¡å‹å¤ªè¿œ"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "752b6406",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def ppo_loss(log_probs, old_log_probs, advantages, clip_epsilon=0.2):\n",
                                        "    \"\"\"\n",
                                        "    PPO Clipped Loss\n",
                                        "    \n",
                                        "    å‚æ•°:\n",
                                        "        log_probs: å½“å‰ç­–ç•¥çš„ log æ¦‚ç‡\n",
                                        "        old_log_probs: æ—§ç­–ç•¥çš„ log æ¦‚ç‡\n",
                                        "        advantages: ä¼˜åŠ¿å‡½æ•°å€¼\n",
                                        "        clip_epsilon: è£å‰ªèŒƒå›´\n",
                                        "    \"\"\"\n",
                                        "    # è®¡ç®—æ¦‚ç‡æ¯”\n",
                                        "    ratio = torch.exp(log_probs - old_log_probs)\n",
                                        "    \n",
                                        "    # Clipped ratio\n",
                                        "    clipped_ratio = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)\n",
                                        "    \n",
                                        "    # PPO loss = min(ratio * A, clipped_ratio * A)\n",
                                        "    loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
                                        "    \n",
                                        "    return loss\n",
                                        "\n",
                                        "# å¯è§†åŒ– clipping\n",
                                        "ratios = np.linspace(0.5, 1.5, 100)\n",
                                        "advantages = np.array([1.0, -1.0])  # æ­£å‘å’Œè´Ÿå‘\n",
                                        "\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                                        "\n",
                                        "for i, adv in enumerate(advantages):\n",
                                        "    unclipped = ratios * adv\n",
                                        "    clipped = np.clip(ratios, 0.8, 1.2) * adv\n",
                                        "    objective = np.minimum(unclipped, clipped) if adv > 0 else np.maximum(unclipped, clipped)\n",
                                        "    \n",
                                        "    axes[i].plot(ratios, unclipped, 'b--', label='Unclipped', linewidth=2)\n",
                                        "    axes[i].plot(ratios, clipped, 'r--', label='Clipped', linewidth=2)\n",
                                        "    axes[i].plot(ratios, objective, 'g-', label='PPO Objective', linewidth=3)\n",
                                        "    axes[i].axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
                                        "    axes[i].set_xlabel('Probability Ratio')\n",
                                        "    axes[i].set_ylabel('Objective')\n",
                                        "    axes[i].set_title(f'Advantage = {adv}')\n",
                                        "    axes[i].legend()\n",
                                        "    axes[i].grid(True, alpha=0.3)\n",
                                        "\n",
                                        "plt.suptitle('PPO Clipped Objective', fontsize=14)\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"PPO é€šè¿‡ clipping é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œä¿æŒè®­ç»ƒç¨³å®š\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "46929c45",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## 4. RLHF çš„é—®é¢˜\n",
                                        "\n",
                                        "### ä¸ºä»€ä¹ˆ RLHF éš¾ä»¥è®­ç»ƒï¼Ÿ\n",
                                        "\n",
                                        "1. **Reward Hacking**ï¼šæ¨¡å‹æ‰¾åˆ°\"ä½œå¼Š\"æ–¹å¼è·å¾—é«˜åˆ†\n",
                                        "2. **ä¸ç¨³å®š**ï¼šPPO è¶…å‚æ•°æ•æ„Ÿ\n",
                                        "3. **å¤æ‚**ï¼šéœ€è¦åŒæ—¶ç»´æŠ¤å¤šä¸ªæ¨¡å‹\n",
                                        "4. **æˆæœ¬é«˜**ï¼šéœ€è¦å¤§é‡é‡‡æ ·å’Œè®¡ç®—\n",
                                        "\n",
                                        "**æ³¨ï¼š** ä¸‹æ–¹å¯¹æ¯”å›¾ä¸ºç¤ºæ„/æ¨¡æ‹Ÿæ•°æ®ï¼Œç”¨äºè¯´æ˜å¤æ‚åº¦ä¸æˆæœ¬å·®å¼‚ã€‚"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "8337c85e",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# å¯è§†åŒ– RLHF vs DPO çš„å¤æ‚åº¦ï¼ˆç¤ºæ„ï¼‰\n",
                                        "comparison = {\n",
                                        "    'Components': ['SFT Model', 'Reference Model', 'Reward Model', 'Policy Model', 'Value/Critic'],\n",
                                        "    'RLHF': [1, 1, 1, 1, 1],\n",
                                        "    'DPO': [1, 1, 0, 1, 0],\n",
                                        "}\n",
                                        "\n",
                                        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                                        "\n",
                                        "# ç»„ä»¶æ•°é‡\n",
                                        "x = np.arange(len(comparison['Components']))\n",
                                        "width = 0.35\n",
                                        "axes[0].bar(x - width/2, comparison['RLHF'], width, label='RLHF', color='coral')\n",
                                        "axes[0].bar(x + width/2, comparison['DPO'], width, label='DPO', color='green')\n",
                                        "axes[0].set_xticks(x)\n",
                                        "axes[0].set_xticklabels(comparison['Components'], rotation=15)\n",
                                        "axes[0].set_ylabel('Required (1=Yes, 0=No)')\n",
                                        "axes[0].set_title('Components Required (illustrative)')\n",
                                        "axes[0].legend()\n",
                                        "\n",
                                        "# è®­ç»ƒå¯¹æ¯”ï¼ˆæˆæœ¬ä¸ç¨³å®šæ€§ï¼Œè¶Šé«˜è¶Šå¤š/è¶Šç¨³å®šï¼‰\n",
                                        "metrics = ['Training Cost', 'Memory Cost', 'Tuning Cost', 'Stability']\n",
                                        "rlhf_scores = [90, 85, 80, 45]\n",
                                        "dpo_scores = [45, 50, 40, 80]\n",
                                        "\n",
                                        "x = np.arange(len(metrics))\n",
                                        "axes[1].bar(x - width/2, rlhf_scores, width, label='RLHF', color='coral')\n",
                                        "axes[1].bar(x + width/2, dpo_scores, width, label='DPO', color='green')\n",
                                        "axes[1].set_xticks(x)\n",
                                        "axes[1].set_xticklabels(metrics, rotation=15)\n",
                                        "axes[1].set_ylabel('Relative score (higher=more)')\n",
                                        "axes[1].set_title('Training Comparison (illustrative)')\n",
                                        "axes[1].legend()\n",
                                        "\n",
                                        "plt.tight_layout()\n",
                                        "plt.show()\n",
                                        "\n",
                                        "print(\"DPO æ›´ç®€å•ã€æ›´ç¨³å®šï¼Œä½† RLHF åœ¨æŸäº›åœºæ™¯ä»æœ‰ä¼˜åŠ¿ï¼ˆç¤ºæ„/æ¨¡æ‹Ÿï¼‰\")\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "5191e14b",
                              "metadata": {},
                              "source": [
                                        "---\n",
                                        "\n",
                                        "## æœ¬ç« æ€»ç»“\n",
                                        "\n",
                                        "1. **RLHF ä¸‰é˜¶æ®µ**\n",
                                        "   - SFT â†’ Reward Model â†’ PPO\n",
                                        "   - æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç‰¹å®šçš„è®­ç»ƒç›®æ ‡\n",
                                        "\n",
                                        "2. **Reward Model**\n",
                                        "   - å­¦ä¹ äººç±»åå¥½\n",
                                        "   - ç”¨ Bradley-Terry loss è®­ç»ƒ\n",
                                        "\n",
                                        "3. **PPO**\n",
                                        "   - ç­–ç•¥æ¢¯åº¦ + Clipping\n",
                                        "   - ä¿æŒè®­ç»ƒç¨³å®šæ€§\n",
                                        "\n",
                                        "4. **RLHF vs DPO**\n",
                                        "   - RLHF æ›´å¤æ‚ä½†æ›´çµæ´»\n",
                                        "   - DPO æ›´ç®€å•æ›´ç¨³å®š\n",
                                        "\n",
                                        "### ä½•æ—¶ä½¿ç”¨ RLHFï¼Ÿ\n",
                                        "\n",
                                        "- éœ€è¦åœ¨çº¿å­¦ä¹ æ—¶\n",
                                        "- å¥–åŠ±å‡½æ•°å¤æ‚æ—¶\n",
                                        "- éœ€è¦æ›´ç²¾ç»†æ§åˆ¶æ—¶"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "f73536af",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ç»ƒä¹ ç©ºé—´\n",
                                        "\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "llmc",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "name": "python",
                              "version": "3.9.25"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
